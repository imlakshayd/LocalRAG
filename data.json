[
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:1>\n",
    "text": "Artificial intelligence\nArtificial intelligence (AI) refers to the capability of computational systems to perform tasks typically\nassociated with human intelligence, such as learning, reasoning, problem-solving, perception, and\ndecision-making. It is a field of research in computer science that develops and studies methods and\nsoftware that enable machines to perceive their environment and use learning and intelligence to take\nactions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search);\nrecommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google\nAssistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g.,\nChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However,\nmany AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general\napplications, often without being called AI because once something becomes useful enough and common\nenough it's not labeled AI anymore.\"[2][3]\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The\ntraditional goals of AI research include learning, reasoning, knowledge representation, planning, natural\nlanguage processing, perception, and support for robotics.[a] General intelligence\u2014the ability to complete\nany task performed by a human on an at least equal level\u2014is among the field's long-term goals.[4] To\nreach these goals, AI researchers have adapted and integrated a wide range of techniques, including\nsearch and mathematical optimization, formal logic, artificial neural networks, and methods based on\nstatistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy,\nneuroscience, and other fields.[5]\nArtificial intelligence was founded as an academic discipline in 1956,[6] and the field went through\nmultiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of\nfunding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when deep learning\noutperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer\narchitecture,[12] and by the early 2020s many billions of dollars were being invested in AI and the field\nexperienced rapid ongoing progress in what has become known as the AI boom. The emergence of\nadvanced generative AI in the midst of the AI boom and its ability to create and modify content exposed\nseveral unintended consequences and harms in the present and raised concerns about the risks of AI and\nits long-term effects in the future, prompting discussions about regulatory policies to ensure the safety\nand benefits of the technology.\nGoals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These\nconsist of particular traits or capabilities that researchers expect an intelligent system to display. The traits\ndescribed below have received the most attention and cover the scope of AI research.[a]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:2>\n",
    "text": "Reasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they\nsolve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for\ndealing with uncertain or incomplete information, employing concepts from probability and\neconomics.[14]\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a\n\"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans\nrarely use the step-by-step deduction that early AI research could model. They solve most of their\nproblems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\nKnowledge representation\nKnowledge representation and knowledge\nengineering[17] allow AI programs to answer\nquestions intelligently and make deductions about\nreal-world facts. Formal knowledge representations\nare used in content-based indexing and retrieval,[18]\nscene interpretation,[19] clinical decision support,[20]\nknowledge discovery (mining \"interesting\" and\nactionable inferences from large databases),[21] and\nother areas.[22]\nA knowledge base is a body of knowledge represented\nin a form that can be used by a program. An ontology\nis the set of objects, relations, concepts, and\nproperties used by a particular domain of\nknowledge.[23] Knowledge bases need to represent\nAn ontology represents knowledge as a set of\nthings such as objects, properties, categories, and\nconcepts within a domain and the relationships\nrelations between objects;[24] situations, events,\nbetween those concepts.\nstates, and time;[25] causes and effects;[26] knowledge\nabout knowledge (what we know about what other\npeople know);[27] default reasoning (things that humans assume are true until they are told differently and\nwill remain true even when other facts are changing);[28] and many other aspects and domains of\nknowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense\nknowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic\nform of most commonsense knowledge (much of what people know is not represented as \"facts\" or\n\"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition,\nthe problem of obtaining knowledge for AI applications.[c]\nPlanning and decision-making\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or\npreferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific\ngoal.[33] In automated decision-making, the agent has preferences\u2014there are some situations it would\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:3>\n",
    "text": "prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to\neach situation (called the \"utility\") that measures how much the agent prefers it. For each possible action,\nit can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the\nprobability that the outcome will occur. It can then choose the action with the maximum expected\nutility.[34]\nIn classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world\nproblems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or\n\"unobservable\") and it may not know for certain what will happen after each possible action (it is not\n\"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation\nto see if the action worked.[36]\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or\nhumans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek\ninformation to improve its preferences.[37] Information value theory can be used to weigh the value of\nexploratory or experimental actions.[38] The space of possible future actions and situations is typically\nintractably large, so the agents must take actions and evaluate situations while being uncertain of what the\noutcome will be.\nA Markov decision process has a transition model that describes the probability that a particular action\nwill change the state in a particular way and a reward function that supplies the utility of each state and\nthe cost of each action. A policy associates a decision with each possible state. The policy could be\ncalculated (e.g., by iteration), be heuristic, or it can be learned.[39]\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that\nmake decisions that involve other agents.[40]\nLearning\nMachine learning is the study of programs that can improve their performance on a given task\nautomatically.[41] It has been a part of AI from the beginning.[e]\nThere are several kinds of machine learning.\nUnsupervised learning analyzes a stream of data\nand finds patterns and makes predictions without\nany other guidance.[44] Supervised learning\nrequires labeling the training data with the\nexpected answers, and comes in two main\nvarieties: classification (where the program must\nlearn to predict what category the input belongs\nin) and regression (where the program must deduce a numeric function based on numeric input).[45]\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent\nlearns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge\ngained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning\nthat runs inputs through biologically inspired artificial neural networks for all of these types of\nlearning.[48]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:4>\n",
    "text": "Computational learning theory can assess learners by computational complexity, by sample complexity\n(how much data is required), or by other notions of optimization.[49]\nNatural language processing\nNatural language processing (NLP)[50] allows programs to read, write and communicate in human\nlanguages such as English. Specific problems include speech recognition, speech synthesis, machine\ntranslation, information extraction, information retrieval and question answering.[51]\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with\nword-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the\ncommon sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not\ngrammar that was the key to understanding languages, and that thesauri and not dictionaries should be the\nbasis of computational language structure.\nModern deep learning techniques for NLP include word embedding (representing words, typically as\nvectors encoding their meaning),[52] transformers (a deep learning architecture using an attention\nmechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models\nbegan to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on\nthe bar exam, SAT test, GRE test, and many other real-world applications.[57]\nPerception\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless\nsignals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is\nthe ability to analyze visual input.[58]\nThe field includes speech recognition,[59] image classification,[60] facial recognition, object\nrecognition,[61] object tracking,[62] and robotic perception.[63]\nSocial intelligence\nAffective computing is a field that comprises systems that\nrecognize, interpret, process, or simulate human feeling,\nemotion, and mood.[65] For example, some virtual assistants\nare programmed to speak conversationally or even to banter\nhumorously; it makes them appear more sensitive to the\nemotional dynamics of human interaction, or to otherwise\nfacilitate human\u2013computer interaction.\nHowever, this tends to give na\u00efve users an unrealistic\nKismet, a robot head which was made in\nconception of the intelligence of existing computer agents.[66]\nthe 1990s; it is a machine that can\nModerate successes related to affective computing include\nrecognize and simulate emotions.[64]\ntextual sentiment analysis and, more recently, multimodal\nsentiment analysis, wherein AI classifies the effects displayed\nby a videotaped subject.[67]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:5>\n",
    "text": "General intelligence\nA machine with artificial general intelligence should be able to solve a wide variety of problems with\nbreadth and versatility similar to human intelligence.[4]\nTechniques\nAI research uses a wide variety of techniques to accomplish the goals above.[b]\nSearch and optimization\nAI can solve many problems by intelligently searching through many possible solutions.[68] There are\ntwo very different kinds of search used in AI: state space search and local search.\nState space search\nState space search searches through a tree of possible states to try to find a goal state.[69] For example,\nplanning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal,\na process called means-ends analysis.[70]\nSimple exhaustive searches[71] are rarely sufficient for most real-world problems: the search space (the\nnumber of places to search) quickly grows to astronomical numbers. The result is a search that is too slow\nor never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to\nreach a goal.[72]\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of\npossible moves and countermoves, looking for a winning position.[73]\nLocal search\nLocal search uses mathematical optimization to find a\nsolution to a problem. It begins with some form of guess and\nrefines it incrementally.[74]\nGradient descent is a type of local search that optimizes a set\nof numerical parameters by incrementally adjusting them to\nminimize a loss function. Variants of gradient descent are\ncommonly used to train neural networks,[75] through the\nbackpropagation algorithm.\nAnother type of local search is evolutionary computation,\nIllustration of gradient descent for 3\nwhich aims to iteratively improve a set of candidate solutions\ndifferent starting points; two parameters\nby \"mutating\" and \"recombining\" them, selecting only the\n(represented by the plan coordinates) are\nfittest to survive each generation.[76] adjusted in order to minimize the loss\nfunction (the height)\nDistributed search processes can coordinate via swarm\nintelligence algorithms. Two popular swarm algorithms used\nin search are particle swarm optimization (inspired by bird flocking) and ant colony optimization\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:6>\n",
    "text": "(inspired by ant trails).[77]\nLogic\nFormal logic is used for reasoning and knowledge representation.[78] Formal logic comes in two main\nforms: propositional logic (which operates on statements that are true or false and uses logical\nconnectives such as \"and\", \"or\", \"not\" and \"implies\")[79] and predicate logic (which also operates on\nobjects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that\nare Ys\").[80]\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other\nstatements that are given and assumed to be true (the premises).[81] Proofs can be structured as proof\ntrees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by\ninference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root\nnode is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In\nthe case of Horn clauses, problem-solving search can be performed by reasoning forwards from the\npremises or backwards from the problem.[82] In the more general case of the clausal form of first-order\nlogic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a\ncontradiction from premises that include the negation of the problem to be solved.[83]\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable.\nHowever, backward reasoning with Horn clauses, which underpins computation in the logic\nprogramming language Prolog, is Turing complete. Moreover, its efficiency is competitive with\ncomputation in other symbolic programming languages.[84]\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are\nvague and partially true.[85]\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle\ndefault reasoning.[28] Other specialized versions of logic have been developed to describe many complex\ndomains.\nProbabilistic methods for uncertain reasoning\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the\nagent to operate with incomplete or uncertain information. AI researchers have devised a number of tools\nto solve these problems using methods from probability theory and economics.[86] Precise mathematical\ntools have been developed that analyze how an agent can make choices and plan, using decision theory,\ndecision analysis,[87] and information value theory.[88] These tools include models such as Markov\ndecision processes,[89] dynamic decision networks,[90] game theory and mechanism design.[91]\nBayesian networks[92] are a tool that can be used for reasoning (using the Bayesian inference\nalgorithm),[g][94] learning (using the expectation\u2013maximization algorithm),[h][96] planning (using decision\nnetworks)[97] and perception (using dynamic Bayesian networks).[90]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:7>\n",
    "text": "Probabilistic algorithms can also\nbe used for filtering, prediction,\nsmoothing, and finding\nexplanations for streams of data,\nthus helping perception systems\nanalyze processes that occur over\ntime (e.g., hidden Markov models\nor Kalman filters).[90]\nClassifiers and\nstatistical learning\nmethods A simple Bayesian network, with the associated conditional probability\ntables\nThe simplest AI applications can\nbe divided into two types:\nclassifiers (e.g., \"if shiny then diamond\"), on one\nhand, and controllers (e.g., \"if diamond then pick\nup\"), on the other hand. Classifiers [98] are functions\nthat use pattern matching to determine the closest\nmatch. They can be fine-tuned based on chosen\nexamples using supervised learning. Each pattern\n(also called an \"observation\") is labeled with a certain\npredefined class. All the observations combined with\ntheir class labels are known as a data set. When a new\nobservation is received, that observation is classified\nbased on previous experience.[45]\nThere are many kinds of classifiers in use.[99] The\nExpectation\u2013maximization clustering of Old\ndecision tree is the simplest and most widely used\nFaithful eruption data starts from a random guess\nsymbolic machine learning algorithm.[100] K-nearest\nbut then successfully converges on an accurate\nneighbor algorithm was the most widely used\nclustering of the two physically distinct modes of\nanalogical AI until the mid-1990s, and Kernel\neruption.\nmethods such as the support vector machine (SVM)\ndisplaced k-nearest neighbor in the 1990s.[101] The\nnaive Bayes classifier is reportedly the \"most widely used learner\"[102] at Google, due in part to its\nscalability.[103] Neural networks are also used as classifiers.[104]\nArtificial neural networks\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which\nloosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can\nrecognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output.\nEach node applies a function and once the weight crosses its specified threshold, the data is transmitted to\nthe next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[104]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:8>\n",
    "text": "Learning algorithms for neural networks use local search to\nchoose the weights that will get the right output for each input\nduring training. The most common training technique is the\nbackpropagation algorithm.[105] Neural networks learn to\nmodel complex relationships between inputs and outputs and\nfind patterns in data. In theory, a neural network can learn any\nfunction.[106]\nIn feedforward neural networks the signal passes in only one\ndirection.[107] Recurrent neural networks feed the output\nsignal back into the input, which allows short-term memories\nof previous input events. Long short term memory is the most\nA neural network is an interconnected\nsuccessful network architecture for recurrent networks.[108]\ngroup of nodes, akin to the vast network\nPerceptrons [109] use only a single layer of neurons; deep of neurons in the human brain.\nlearning[110] uses multiple layers. Convolutional neural\nnetworks strengthen the connection between neurons that are\n\"close\" to each other\u2014this is especially important in image processing, where a local set of neurons must\nidentify an \"edge\" before the network can identify an object.[111]\nDeep learning\nDeep learning[110] uses several layers of neurons between the\nnetwork's inputs and outputs. The multiple layers can progressively\nextract higher-level features from the raw input. For example, in image\nprocessing, lower layers may identify edges, while higher layers may\nidentify the concepts relevant to a human such as digits, letters, or\nfaces.[112]\nDeep learning has profoundly improved the performance of programs\nin many important subfields of artificial intelligence, including\ncomputer vision, speech recognition, natural language processing,\nimage classification,[113] and others. The reason that deep learning\nperforms so well in so many applications is not known as of 2021.[114] The sudden success of deep\nlearning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep\nneural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but\nbecause of two factors: the incredible increase in computer power (including the hundred-fold increase in\nspeed by switching to GPUs) and the availability of vast amounts of training data, especially the giant\ncurated datasets used for benchmark testing, such as ImageNet.[j]\nGPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on\nthe semantic relationships between words in sentences. Text-based GPT models are pretrained on a large\ncorpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token\nbeing usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate\nknowledge about the world and can then generate human-like text by repeatedly predicting the next\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:9>\n",
    "text": "token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless,\nusually with a technique called reinforcement learning from human feedback (RLHF). Current GPT\nmodels are prone to generating falsehoods called \"hallucinations\", although this can be reduced with\nRLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task\nin simple text.[122][123]\nCurrent models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and\nLLaMA.[124] Multimodal GPT models can process different types of data (modalities) such as images,\nvideos, sound, and text.[125]\nHardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific\nenhancements and used with specialized TensorFlow software had replaced previously used central\nprocessing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine\nlearning models' training.[126] Specialized programming languages such as Prolog were used in early AI\nresearch,[127] but general-purpose programming languages like Python have become predominant.[128]\nThe transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a\ntrend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it.\nImprovements in GPUs have been even faster,[129] a trend sometimes called Huang's law,[130] named\nafter Nvidia co-founder and CEO Jensen Huang.\nApplications\nAI and machine learning technology is used in most of the essential applications of the 2020s, including:\nsearch engines (such as Google Search), targeting online advertisements, recommendation systems\n(offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense,\nFacebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and\nself-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial\nrecognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used\nby Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation\nofficer (CAO).\nHealth and medicine\nThe application of AI in medicine and medical research has the potential to increase patient care and\nquality of life.[131] Through the lens of the Hippocratic Oath, medical professionals are ethically\ncompelled to use AI, if applications can more accurately diagnose and treat patients.[132][133]\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly\nimportant for organoid and tissue engineering development which use microscopy imaging as a key\ntechnique in fabrication.[134] It has been suggested that AI can overcome discrepancies in funding\nallocated to different fields of research.[134][135] New AI tools can deepen the understanding of\nbiomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to\napproximate, in hours rather than months, the 3D structure of a protein.[136] In 2023, it was reported that\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:10>\n",
    "text": "AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-\nresistant bacteria.[137] In 2024, researchers used machine learning to accelerate the search for Parkinson's\ndisease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of\nalpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial\nscreening process ten-fold and reduce the cost by a thousand-fold.[138][139]\nGames\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced\ntechniques.[140] Deep Blue became the first computer chess-playing system to beat a reigning world chess\nchampion, Garry Kasparov, on 11 May 1997.[141] In 2011, in a Jeopardy! quiz show exhibition match,\nIBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter\nand Ken Jennings, by a significant margin.[142] In March 2016, AlphaGo won 4 out of 5 games of Go in a\nmatch with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a\nprofessional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player\nin the world.[143] Other programs handle imperfect-information games, such as the poker-playing\nprogram Pluribus.[144] DeepMind developed increasingly generalistic reinforcement learning models,\nsuch as with MuZero, which could be trained to play chess, Go, or Atari games.[145] In 2019, DeepMind's\nAlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game\nthat involves incomplete knowledge of what happens on the map.[146] In 2021, an AI agent competed in a\nPlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers\nusing deep reinforcement learning.[147] In 2024, Google DeepMind introduced SIMA, a type of AI\ncapable of autonomously playing nine previously unseen open-world video games by observing screen\noutput, as well as executing short, specific tasks in response to natural language instructions.[148]\nMathematics\nLarge language models, such as GPT-4, Gemini, Claude, LLaMa or Mistral, are increasingly used in\nmathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of\nhallucinations. They sometimes need a large database of mathematical problems to learn from, but also\nmethods such as supervised fine-tuning[149] or trained classifiers with human-annotated data to improve\nanswers for new problems and learn from corrections.[150] A February 2024 study showed that the\nperformance of some language models for reasoning capabilities in solving math problems not included\nin their training data was low, even for problems with only minor deviations from trained data.[151] One\ntechnique to improve their performance involves training the models to produce correct reasoning steps,\nrather than just the correct result.[152] The Alibaba Group developed a version of its Qwen models called\nQwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including\n84% accuracy on the MATH dataset of competition mathematics problems.[153] In January 2025,\nMicrosoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step\nreasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and\n90% of the MATH benchmark problems.[154]\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome\nincluding proof of theorems have been developed such as AlphaTensor, AlphaGeometry and AlphaProof\nall from Google DeepMind,[155] Llemma from EleutherAI[156] or Julius.[157]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:11>\n",
    "text": "When natural language is used to describe mathematical problems, converters can transform such\nprompts into a formal language such as Lean to define mathematical tasks.\nSome models have been developed to solve challenging problems and reach good results in benchmark\ntests, others to serve as educational tools in mathematics.[158]\nTopological deep learning integrates various topological approaches.\nFinance\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail\nonline banking to investment advice and insurance, where automated \"robot advisers\" have been in use\nfor some years.[159]\nAccording to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to\nsee the emergence of highly innovative AI-informed financial products and services. He argues that \"the\ndeployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in\nbanking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new\nwave of [e.g., sophisticated] pension innovation.\"[160]\nMilitary\nVarious countries are deploying AI military applications.[161] The main applications enhance command\nand control, communications, sensors, integration and interoperability.[162] Research is targeting\nintelligence collection and analysis, logistics, cyber operations, information operations, and\nsemiautonomous and autonomous vehicles.[161] AI technologies enable coordination of sensors and\neffectors, threat detection and identification, marking of enemy positions, target acquisition, coordination\nand deconfliction of distributed Joint Fires between networked combat vehicles, both human operated and\nautonomous.[162]\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.[161][163][164][165]\nGenerative AI\nGenerative artificial intelligence (Generative AI, GenAI,[166] or GAI) is a subfield of artificial\nintelligence that uses generative models to produce text, images, videos, or other forms of\ndata.[167][168][169] These models learn the underlying patterns and structures of their training data and use\nthem to produce new data[170][171] based on the input, which often comes in the form of natural language\nprompts.[172][173]\nGenerative AI tools have become more common since an \"AI boom\" in the 2020s. This boom was made\npossible by improvements in transformer-based deep neural networks, particularly large language models\n(LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image\nartificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and\ntext-to-video AI generators such as Sora.[174][175][176][177] Technology companies developing generative\nAI include OpenAI, Anthropic, Microsoft, Google, and Baidu.[178][179][180]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:12>\n",
    "text": "Generative AI has raised many ethical questions. It can be\nused for cybercrime, or to deceive or manipulate people\nthrough fake news or deepfakes.[181] Even if used ethically, it\nmay lead to the mass replacement of human jobs.[182] The\ntools themselves have been criticized as violating intellectual\nproperty laws, since they are trained on and emulate\ncopyrighted works of art.[183]\nAgents\nArtificial intelligent (AI) agents are software entities designed\nto perceive their environment, make decisions, and take\nactions autonomously to achieve specific goals. These agents\ncan interact with users, their environment, or other agents. AI\nVincent van Gogh in watercolour created\nagents are used in various applications, including virtual\nby generative AI software\nassistants, chatbots, autonomous vehicles, game-playing\nsystems, and industrial robotics. AI agents operate within the\nconstraints of their programming, available computational resources, and hardware limitations. This\nmeans they are restricted to performing tasks within their defined scope and have finite memory and\nprocessing capabilities. In real-world applications, AI agents often face time constraints for decision-\nmaking and action execution. Many AI agents incorporate learning algorithms, enabling them to improve\ntheir performance over time through experience or training. Using machine learning, AI agents can adapt\nto new situations and optimise their behaviour for their designated tasks.[184][185][186]\nSexuality\nApplications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user\ndata to offer prediction,[187] AI-integrated sex toys (e.g., teledildonics),[188] AI-generated sexual\neducation content,[189] and AI agents that simulate sexual and romantic partners (e.g., Replika).[190] AI is\nalso used for the production of non-consensual deepfake pornography, raising significant ethical and legal\nconcerns.[191]\nAI technologies have also been used to attempt to identify online gender-based violence and online\nsexual grooming of minors.[192][193]\nOther industry-specific tasks\nThere are also thousands of successful AI applications used to solve specific problems for specific\nindustries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in\nsome offerings or processes.[194] A few examples are energy storage, medical diagnosis, military\nlogistics, applications that predict the result of judicial decisions, foreign policy, or supply chain\nmanagement.\nAI applications for evacuation and disaster management are growing. AI has been used to investigate if\nand how people evacuated in large scale and small scale evacuations using historical data from GPS,\nvideos or social media. Further, AI can provide real time information on the real time evacuation\nconditions.[195][196][197]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:13>\n",
    "text": "In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments\nor increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict\nthe ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct\npredictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests,\nand save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and\napplications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the\ndevelopment of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting\nsolar activity, and distinguishing between signals and instrumental effects in gravitational wave\nastronomy. Additionally, it could be used for activities in space, such as space exploration, including the\nanalysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,\nand more autonomous operation.\nDuring the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably\nby creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters,\nand by translating speeches to various local languages.[198]\nEthics\nAI has potential benefits and potential risks.[199] AI may be able to advance science and find solutions for\nserious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve\neverything else\".[200] However, as the use of AI has become widespread, several unintended\nconsequences and risks have been identified.[201] In-production systems can sometimes not factor ethics\nand bias into their AI training processes, especially when the AI algorithms are inherently unexplainable\nin deep learning.[202]\nRisks and harm\nPrivacy and copyright\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have\nraised concerns about privacy, surveillance and copyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect\npersonal information, raising concerns about intrusive data gathering and unauthorized access by third\nparties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of\ndata, potentially leading to a surveillance society where individual activities are constantly monitored and\nanalyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video, or audio.[203]\nFor example, in order to build speech recognition algorithms, Amazon has recorded millions of private\nconversations and allowed temporary workers to listen to and transcribe some of them.[204] Opinions\nabout this widespread surveillance range from those who see it as a necessary evil to those for whom it is\nclearly unethical and a violation of the right to privacy.[205]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:14>\n",
    "text": "AI developers argue that this is the only way to deliver valuable applications and have developed several\ntechniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-\nidentification and differential privacy.[206] Since 2016, some privacy experts, such as Cynthia Dwork,\nhave begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from\nthe question of 'what they know' to the question of 'what they're doing with it'.\"[207]\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or\ncomputer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well\nand under what circumstances this rationale will hold up in courts of law; relevant factors may include\n\"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market\nfor the copyrighted work\".[208][209] Website owners who do not wish to have their content scraped can\nindicate it in a \"robots.txt\" file.[210] In 2023, leading authors (including John Grisham and Jonathan\nFranzen) sued AI companies for using their work to train generative AI.[211][212] Another discussed\napproach is to envision a separate sui generis system of protection for creations generated by AI to ensure\nfair attribution and compensation for human authors.[213]\nDominance by tech giants\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple\nInc., Meta Platforms, and Microsoft.[214][215][216] Some of these players already own the vast majority of\nexisting cloud infrastructure and computing power from data centers, allowing them to entrench further in\nthe marketplace.[217][218]\nPower needs and environmental impacts\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast\nto 2026, forecasting electric power use.[219] This is the first IEA report to make projections for data\ncenters and power consumption for artificial intelligence and cryptocurrency. The report states that power\ndemand for these uses might double by 2026, with additional electric power usage equal to electricity\nused by the whole Japanese nation.[220]\nProdigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay\nclosings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of\ndata centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon)\ninto voracious consumers of electric power. Projected electric consumption is so immense that there is\nconcern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the\nelectrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear\nenergy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually\nkinder to the environment, but they need the energy now. AI makes the power grid more efficient and\n\"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to\ntechnology firms.[221]\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge,\nfound \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that,\nby 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for\nthe electrical power generation industry by a variety of means.[222] Data centers' need for more and more\nelectrical power is such that they might max out the electrical grid. The Big Tech companies counter that\nAI can be used to maximize the utilization of the grid by all.[223]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:15>\n",
    "text": "In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US\nnuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a\nPennsylvania nuclear-powered data center for $650 Million (US).[224] Nvidia CEO Jen-Hsun Huang said\nnuclear power is a good option for the data centers.[225]\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three\nMile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the\nplant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in\n1979, will require Constellation to get through strict regulatory processes which will include extensive\nsafety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US\nre-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of\nenergy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is\ndependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[226] The\nUS government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades\nNuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October\n2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a\nnuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of\nConstellation.[227]\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of\nTaoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[228] Taiwan aims to\nphase out nuclear power by 2025.[228] On the other hand, Singapore imposed a ban on the opening of\ndata centers in 2019 due to electric power, but in 2022, lifted this ban.[228]\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident,\naccording to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in\nwhich Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for\ngenerative AI.[229] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and\nstable power for AI.[229]\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application\nsubmitted by Talen Energy for approval to supply some electricity from the nuclear power station\nSusquehanna to Amazon's data center.[230] According to the Commission Chairman Willie L. Phillips, it\nis a burden on the electricity grid as well as a significant cost shifting concern to households and other\nbusiness sectors.[230]\nIn 2025 a report prepared by the International Energy Agency estimated the greenhouse gas emissions\nfrom the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300-500\nmillion tonnes depending on what measures will be taken. This is below 1.5% of the energy sector\nemissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions,\nbut rebound effects (for example if people will pass from public transport to autonomous cars) can reduce\nit.[231]\nMisinformation\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI\nprograms were given the goal of maximizing user engagement (that is, the only goal was to keep people\nwatching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:16>\n",
    "text": "partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch\nmore content on the same subject, so the AI led people into filter bubbles where they received multiple\nversions of the same misinformation.[232] This convinced many users that the misinformation was true,\nand ultimately undermined trust in institutions, the media and the government.[233] The AI program had\ncorrectly learned to maximize its goal, but the result was harmful to society. After the U.S. election in\n2016, major technology companies took some steps to mitigate the problem.[234]\nIn 2022, generative AI began to create images, audio, video and text that are indistinguishable from real\nphotographs, recordings, films, or human writing. It is possible for bad actors to use this technology to\ncreate massive amounts of misinformation or propaganda.[235] One such potential malicious use is\ndeepfakes for computational propaganda [236]. AI pioneer Geoffrey Hinton expressed concern about AI\nenabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[237]\nAlgorithmic bias and fairness\nMachine learning applications will be biased[k] if they learn from biased data.[239] The developers may\nnot be aware that the bias exists.[240] Bias can be introduced by the way training data is selected and by\nthe way a model is deployed.[241][239] If a biased algorithm is used to make decisions that can seriously\nharm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may\ncause discrimination.[242] The field of fairness studies how to prevent harms from algorithmic biases.\nOn June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a\nfriend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few\nimages of black people,[243] a problem called \"sample size disparity\".[244] Google \"fixed\" this problem by\npreventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos\nstill could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and\nAmazon.[245]\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant\nbecoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial\nbias, despite the fact that the program was not told the races of the defendants. Although the error rate for\nboth whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the\nsystem consistently overestimated the chance that a black person would re-offend and would\nunderestimate the chance that a white person would not re-offend.[246] In 2017, several researchers[l]\nshowed that it was mathematically impossible for COMPAS to accommodate all possible measures of\nfairness when the base rates of re-offense were different for whites and blacks in the data.[248]\nA program can make biased decisions even if the data does not explicitly mention a problematic feature\n(such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping\nhistory\" or \"first name\"), and the program will make the same decisions based on these features as it\nwould on \"race\" or \"gender\".[249] Moritz Hardt said \"the most robust fact in this research area is that\nfairness through blindness doesn't work.\"[250]\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that\nare only valid if we assume that the future will resemble the past. If they are trained on data that includes\nthe results of racist decisions in the past, machine learning models must predict that racist decisions will\nbe made in the future. If an application then uses these predictions as recommendations, some of these\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:17>\n",
    "text": "\"recommendations\" will likely be racist.[251] Thus, machine learning is not well suited to help make\ndecisions in areas where there is hope that the future will be better than the past. It is descriptive rather\nthan prescriptive.[m]\nBias and unfairness may go undetected because the developers are overwhelmingly white and male:\namong AI engineers, about 4% are black and 20% are women.[244]\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on\nethical assumptions, and are influenced by beliefs about society. One broad category is distributive\nfairness, which focuses on the outcomes, often identifying groups and seeking to compensate for\nstatistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative\nstereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather\nthan the outcome. The most relevant notions of fairness may depend on the context, notably the type of\nAI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult\nfor companies to operationalize them. Having access to sensitive attributes such as race or gender is also\nconsidered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict\nwith anti-discrimination laws.[238]\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the\nAssociation for Computing Machinery, in Seoul, South Korea, presented and published findings that\nrecommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are\nunsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed\ninternet data should be curtailed.[253]\nLack of transparency\nMany AI systems are so complex that their designers cannot explain how they reach their decisions.[254]\nParticularly with deep neural networks, in which there are a large amount of non-linear relationships\nbetween inputs and outputs. But some popular explainability techniques exist.[255]\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works.\nThere have been many cases where a machine learning program passed rigorous tests, but nevertheless\nlearned something different than what the programmers intended. For example, a system that could\nidentify skin diseases better than medical professionals was found to actually have a strong tendency to\nclassify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to\nshow the scale.[256] Another machine learning system designed to help effectively allocate medical\nresources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia.\nHaving asthma is actually a severe risk factor, but since the patients having asthma would usually get\nmuch more medical care, they were relatively unlikely to die according to the training data. The\ncorrelation between asthma and low risk of dying from pneumonia was real, but misleading.[257]\nPeople who have been harmed by an algorithm's decision have a right to an explanation.[258] Doctors, for\nexample, are expected to clearly and completely explain to their colleagues the reasoning behind any\ndecision they make. Early drafts of the European Union's General Data Protection Regulation in 2016\nincluded an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved\nproblem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has\nno solution, the tools should not be used.[259]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:18>\n",
    "text": "DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these\nproblems.[260]\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution\nof each feature to the output.[261] LIME can locally approximate a model's outputs with a simpler,\ninterpretable model.[262] Multitask learning provides a large number of outputs in addition to the target\nclassification. These other outputs can help developers deduce what the network has learned.[263]\nDeconvolution, DeepDream and other generative methods can allow developers to see what different\nlayers of a deep network for computer vision have learned, and produce output that can suggest what the\nnetwork is learning.[264] For generative pre-trained transformers, Anthropic developed a technique based\non dictionary learning that associates patterns of neuron activations with human-understandable\nconcepts.[265]\nBad actors and weaponized AI\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian\ngovernments, terrorists, criminals or rogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human\nsupervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous\nweapons and, if produced at scale, they are potentially weapons of mass destruction.[267] Even when used\nin conventional warfare, they currently cannot reliably choose targets and could potentially kill an\ninnocent person.[267] In 2014, 30 nations (including China) supported a ban on autonomous weapons\nunder the United Nations' Convention on Certain Conventional Weapons, however the United States and\nothers disagreed.[268] By 2015, over fifty countries were reported to be researching battlefield robots.[269]\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways.\nFace and voice recognition allow widespread surveillance. Machine learning, operating this data, can\nclassify potential enemies of the state and prevent them from hiding. Recommendation systems can\nprecisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in\nproducing misinformation. Advanced AI can make authoritarian centralized decision making more\ncompetitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of\ndigital warfare and advanced spyware.[270] All these technologies have been available since 2020 or\nearlier\u2014AI facial recognition systems are already being used for mass surveillance in China.[271][272]\nThere many other ways that AI is expected to help bad actors, some of which can not be foreseen. For\nexample, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of\nhours.[273]\nTechnological unemployment\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about\nunemployment if there is no adequate social policy for full employment.[274]\nIn the past, technology has tended to increase rather than reduce total employment, but economists\nacknowledge that \"we're in uncharted territory\" with AI.[275] A survey of economists showed\ndisagreement about whether the increasing use of robots and AI will cause a substantial increase in long-\nterm unemployment, but they generally agree that it could be a net benefit if productivity gains are\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:19>\n",
    "text": "redistributed.[276] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt\nFrey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report\nclassified only 9% of U.S. jobs as \"high risk\".[p][278] The methodology of speculating about future\nemployment levels has been criticised as lacking evidential foundation, and for implying that technology,\nrather than social policy, creates unemployment, as opposed to redundancies.[274] In April 2023, it was\nreported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative\nartificial intelligence.[279][280]\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial\nintelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what\nsteam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[281]\nJobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for\ncare-related professions ranging from personal healthcare to the clergy.[282]\nFrom the early days of the development of artificial intelligence, there have been arguments, for example,\nthose put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually\nshould be done by them, given the difference between computers and humans, and between quantitative\ncalculation and qualitative, value-based judgement.[283]\nExistential risk\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This\ncould, as physicist Stephen Hawking stated, \"spell the end of the human race\".[284] This scenario has been\ncommon in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\"\n(or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are\nmisleading in several ways.\nFirst, AI does not require human-like sentience to be an existential risk. Modern AI programs are given\nspecific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if\none gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it\n(he used the example of a paperclip factory manager).[286] Stuart Russell gives the example of household\nrobot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you\ncan't fetch the coffee if you're dead.\"[287] In order to be safe for humanity, a superintelligence would have\nto be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[288]\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an\nexistential risk. The essential parts of civilization are not physical. Things like ideologies, law,\ngovernment, money and the economy are built on language; they exist because there are stories that\nbillions of people believe. The current prevalence of misinformation suggests that an AI could use\nlanguage to convince people to believe anything, even to take actions that are destructive.[289]\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and\nunconcerned by risk from eventual superintelligent AI.[290] Personalities such as Stephen Hawking, Bill\nGates, and Elon Musk,[291] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis\nHassabis, and Sam Altman, have expressed concerns about existential risk from AI.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:20>\n",
    "text": "In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak\nout about the risks of AI\" without \"considering how this impacts Google\".[292] He notably mentioned\nrisks of an AI takeover,[293] and stressed that in order to avoid the worst outcomes, establishing safety\nguidelines will require cooperation among those competing in use of AI.[294]\nIn 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from\nAI should be a global priority alongside other societal-scale risks such as pandemics and nuclear\nwar\".[295]\nSome other researchers were more optimistic. AI pioneer J\u00fcrgen Schmidhuber did not sign the joint\nstatement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and\nhealthier and easier.\"[296] While the tools that are now being used to improve lives can also be used by\nbad actors, \"they can also be used against the bad actors.\"[297][298] Andrew Ng also argued that \"it's a\nmistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested\ninterests.\"[299] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and\neven, eventually, human extinction.\"[300] In the early 2010s, experts argued that the risks are too distant in\nthe future to warrant research or that humans will be valuable from the perspective of a superintelligent\nmachine.[301] However, after 2016, the study of current and future risks and possible solutions became a\nserious area of research.[302]\nEthical machines and alignment\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make\nchoices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI\nshould be a higher research priority: it may require a large investment and it must be completed before AI\nbecomes an existential risk.[303]\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field\nof machine ethics provides machines with ethical principles and procedures for resolving ethical\ndilemmas.[304] The field of machine ethics is also called computational morality,[304] and was founded at\nan AAAI symposium in 2005.[305]\nOther approaches include Wendell Wallach's \"artificial moral agents\"[306] and Stuart J. Russell's three\nprinciples for developing provably beneficial machines.[307]\nOpen source\nActive organizations in the AI open-source community include Hugging Face,[308] Google,[309]\nEleutherAI and Meta.[310] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been\nmade open-weight,[311][312] meaning that their architecture and trained parameters (the \"weights\") are\npublicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize\nthem with their own data and for their own use-case.[313] Open-weight models are useful for research and\ninnovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as\nobjecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn\nthat future AI models may develop dangerous capabilities (such as the potential to drastically facilitate\nbioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They\nrecommend pre-release audits and cost-benefit analyses.[314]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:21>\n",
    "text": "Frameworks\nArtificial Intelligence projects can be guided by ethical considerations during the design, development,\nand implementation of an AI system. An AI framework such as the Care and Act Framework, developed\nby the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined\nas follows:[315][316]\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference,\nthe Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative,\namong others;[317] however, these principles are not without criticism, especially regards to the people\nchosen to contribute to these frameworks.[318]\nPromotion of the wellbeing of the people and communities that these technologies affect requires\nconsideration of the social and ethical implications at all stages of AI system design, development and\nimplementation, and collaboration between job roles such as data scientists, product managers, data\nengineers, domain experts, and delivery managers.[319]\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations\navailable under a MIT open-source licence which is freely available on GitHub and can be improved with\nthird-party packages. It can be used to evaluate AI models in a range of areas including core knowledge,\nability to reason, and autonomous capabilities.[320]\nRegulation\nThe regulation of artificial intelligence is the\ndevelopment of public sector policies and laws for\npromoting and regulating AI; it is therefore related to\nthe broader regulation of algorithms.[321] The\nregulatory and policy landscape for AI is an emerging\nissue in jurisdictions globally.[322] According to AI\nIndex at Stanford, the annual number of AI-related\nlaws passed in the 127 survey countries jumped from\none passed in 2016 to 37 passed in 2022\nalone.[323][324] Between 2016 and 2020, more than 30\ncountries adopted dedicated strategies for AI.[325] The first global AI Safety Summit was held in the\nUnited Kingdom in November 2023 with a\nMost EU member states had released national AI\ndeclaration calling for international cooperation.\nstrategies, as had Canada, China, India, Japan,\nMauritius, the Russian Federation, Saudi Arabia,\nUnited Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy,\nincluding Bangladesh, Malaysia and Tunisia.[325] The Global Partnership on Artificial Intelligence was\nlaunched in June 2020, stating a need for AI to be developed in accordance with human rights and\ndemocratic values, to ensure public confidence and trust in the technology.[325] Henry Kissinger, Eric\nSchmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:22>\n",
    "text": "government commission to regulate AI.[326] In 2023, OpenAI leaders published recommendations for the\ngovernance of superintelligence, which they believe may happen in less than 10 years.[327] In 2023, the\nUnited Nations also launched an advisory body to provide recommendations on AI governance; the body\ncomprises technology company executives, governments officials and academics.[328] In 2024, the\nCouncil of Europe created the first international legally binding treaty on AI, called the \"Framework\nConvention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was\nadopted by the European Union, the United States, the United Kingdom, and other signatories.[329]\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only\n35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[323]\nA 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to\nhumanity.[330] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional\n41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding\n\"not very important\" and 8% responding \"not at all important\".[331][332]\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the\nnear and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[333]\n28 countries including the United States, China, and the European Union issued a declaration at the start\nof the summit, calling for international co-operation to manage the challenges and risks of artificial\nintelligence.[334][335] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety\ncommitments on the development of AI.[336][337]\nHistory\nThe study of mechanical or \"formal\" reasoning began with\nphilosophers and mathematicians in antiquity. The study of\nlogic led directly to Alan Turing's theory of computation,\nwhich suggested that a machine, by shuffling symbols as\nsimple as \"0\" and \"1\", could simulate any conceivable form\nof mathematical reasoning.[339][340] This, along with\nconcurrent discoveries in cybernetics, information theory and\nneurobiology, led researchers to consider the possibility of\nbuilding an \"electronic brain\".[r] They developed several In 2024, AI patents in China and the US\nareas of research that would become part of AI,[342] such as numbered more than three-fourths of AI\npatents worldwide.[338] Though China\nMcCullouch and Pitts design for \"artificial neurons\" in\n1943,[115] and Turing's influential 1950 paper 'Computing had more AI patents, the US had 35%\nmore patents per AI patent-applicant\nMachinery and Intelligence', which introduced the Turing test\ncompany than China.[338]\nand showed that \"machine intelligence\" was\nplausible.[343][340]\nThe field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees\nbecame the leaders of AI research in the 1960s.[t] They and their students produced programs that the\npress described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems\nin algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were\nset up at a number of British and U.S. universities in the latter 1950s and early 1960s.[340]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:23>\n",
    "text": "Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in\ncreating a machine with general intelligence and considered this the goal of their field.[347] In 1965\nHerbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can\ndo\".[348] In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating\n'artificial intelligence' will substantially be solved\".[349] They had, however, underestimated the difficulty\nof the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in\nresponse to the criticism of Sir James Lighthill [351] and ongoing pressure from the U.S. Congress to fund\nmore productive projects.[352] Minsky's and Papert's book Perceptrons was understood as proving that\nartificial neural networks would never be useful for solving real-world tasks, thus discrediting the\napproach altogether.[353] The \"AI winter\", a period when obtaining funding for AI projects was difficult,\nfollowed.[9]\nIn the early 1980s, AI research was revived by the commercial success of expert systems,[354] a form of\nAI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for\nAI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired\nthe U.S. and British governments to restore funding for academic research.[8] However, beginning with\nthe collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-\nlasting winter began.[10]\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent\nmental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt\nthat this approach would be able to imitate all the processes of human cognition, especially perception,\nrobotics, learning and pattern recognition,[355] and began to look into \"sub-symbolic\" approaches.[356]\nRodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that\nmove and survive.[x] Judea Pearl, Lofti Zadeh, and others developed methods that handled incomplete\nand uncertain information by making reasonable guesses rather than precise logic.[86][361] But the most\nimportant development was the revival of \"connectionism\", including neural network research, by\nGeoffrey Hinton and others.[362] In 1990, Yann LeCun successfully showed that convolutional neural\nnetworks can recognize handwritten digits, the first of many successful applications of neural\nnetworks.[363]\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal\nmathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\"\nfocus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics,\neconomics and mathematics).[364] By 2000, solutions developed by AI researchers were being widely\nused, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as\nthe AI effect).[365] However, several academic researchers became concerned that AI was no longer\npursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they\nfounded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded\ninstitutions by the 2010s.[4]\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y] Deep learning's success was based on both\nhardware improvements (faster computers,[367] graphics processing units, cloud computing[368]) and\naccess to large amounts of data [369] (including curated datasets,[368] such as ImageNet). Deep learning's\nsuccess led to an enormous increase in interest and funding in AI.[z] The amount of machine learning\nresearch (measured by total publications) increased by 50% in the years 2015\u20132019.[325]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:24>\n",
    "text": "In 2016, issues of fairness and the misuse of technology were\ncatapulted into center stage at machine learning conferences,\npublications vastly increased, funding became available, and\nmany researchers re-focussed their careers on these issues.\nThe alignment problem became a serious field of academic\nstudy.[302]\nIn the late 2010s and early 2020s, AGI companies began to\ndeliver programs that created enormous interest. In 2015,\nAlphaGo, developed by DeepMind, beat the world champion\nThe number of Google searches for the\nGo player. The program taught only the game's rules and\nterm \"AI\" accelerated in 2022.\ndeveloped a strategy by itself. GPT-3 is a large language\nmodel that was released in 2020 by OpenAI and is capable of\ngenerating high-quality human-like text.[370] ChatGPT, launched on November 30, 2022, became the\nfastest-growing consumer software application in history, gaining over 100 million users in two\nmonths.[371] It marked what is widely regarded as AI's breakout year, bringing it into the public\nconsciousness.[372] These programs, and others, inspired an aggressive AI boom, where large companies\nbegan investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually\nwas invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science\nPhD graduates have specialized in \"AI\".[373] About 800,000 \"AI\"-related U.S. job openings existed in\n2022.[374] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI\ncompanies.[375]\nPhilosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make\nintelligent machines.[376] Another major focus has been whether machines can be conscious, and the\nassociated ethical implications.[377] Many other topics in philosophy are relevant to AI, such as\nepistemology and free will.[378] Rapid advancements have intensified public discussions on the\nphilosophy and ethics of AI.[377]\nDefining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[379] He advised\nchanging the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to\nshow intelligent behaviour\".[379] He devised the Turing test, which measures the ability of a machine to\nsimulate human conversation.[343] Since we can only observe the behavior of the machine, it does not\nmatter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these\nthings about other people but \"it is usual to have a polite convention that everyone thinks.\"[380]\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not\ninternal structure.[1] However, they are critical that the test requires the machine to imitate humans.\n\"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines\nthat fly so exactly like pigeons that they can fool other pigeons.'\"[382] AI founder John McCarthy agreed,\nwriting that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[383]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:25>\n",
    "text": "McCarthy defines intelligence as \"the computational part of\nthe ability to achieve goals in the world\".[384] Another AI\nfounder, Marvin Minsky, similarly describes it as \"the ability\nto solve hard problems\".[385] The leading AI textbook defines\nit as the study of agents that perceive their environment and\ntake actions that maximize their chances of achieving defined\ngoals.[1] These definitions view intelligence in terms of well-\ndefined problems with well-defined solutions, where both the\ndifficulty of the problem and the performance of the program\nare direct measures of the \"intelligence\" of the machine\u2014and\nThe Turing test can provide some\nno other philosophical discussion is required, or may not even evidence of intelligence, but it penalizes\nbe possible. non-human intelligent behavior.[381]\nAnother definition has been adopted by Google,[386] a major\npractitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as\nthe manifestation of intelligence, similar to the way it is defined in biological intelligence.\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with\ncontention as to whether classical algorithms should be categorised as AI,[387] with many companies\nduring the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not\nactually use AI in a material way\".[388]\nEvaluating approaches to AI\nNo established unifying theory or paradigm has guided AI research for most of its history.[aa] The\nunprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much\nso that some sources, especially in the business world, use the term \"artificial intelligence\" to mean\n\"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics\nargue that these questions may have to be revisited by future generations of AI researchers.\nSymbolic AI and its limits\nSymbolic AI (or \"GOFAI\")[390] simulated the high-level conscious reasoning that people use when they\nsolve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\"\ntasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems\nhypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent\naction.\"[391]\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning,\nrecognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level\n\"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[392]\nPhilosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious\ninstinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than\nexplicit symbolic knowledge.[393] Although his arguments had been ridiculed and ignored when they\nwere first presented, eventually, AI research came to agree with him.[ab][16]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:26>\n",
    "text": "The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that\nhuman intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research\ninto symbolic AI will still be necessary to attain general intelligence,[395][396] in part because sub-\nsymbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a\nmodern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial\nintelligence attempts to bridge the two approaches.\nNeat vs. scruffy\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic,\noptimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of\nunrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on\nincremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[397] but\neventually was seen as irrelevant. Modern AI has elements of both.\nSoft vs. hard computing\nFinding a provably correct or optimal solution is intractable for many important problems.[15] Soft\ncomputing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are\ntolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in\nthe late 1980s and most successful AI programs in the 21st century are examples of soft computing with\nneural networks.\nNarrow vs. general AI\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and\nsuperintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these\nsolutions will lead indirectly to the field's long-term goals.[398][399] General intelligence is difficult to\ndefine and difficult to measure, and modern AI has had more verifiable successes by focusing on specific\nproblems with specific solutions. The sub-field of artificial general intelligence studies this area\nexclusively.\nMachine consciousness, sentience, and mind\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental\nstates, in the same sense that human beings do. This issue considers the internal experiences of the\nmachine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because\nit does not affect the goals of the field: to build machines that can solve problems using intelligence.\nRussell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way\nhumans are is not one that we are equipped to take on.\"[400] However, the question has become central to\nthe philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\nConsciousness\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and\n\"easy\" problems of consciousness.[401] The easy problem is understanding how the brain processes\nsignals, makes plans and controls behavior. The hard problem is explaining how this feels or why it\nshould feel like anything at all, assuming we are right in thinking that it truly does feel like something\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:27>\n",
    "text": "(Dennett's consciousness illusionism says this is an illusion). While human information processing is easy\nto explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-\nblind person who has learned to identify which objects in their field of view are red, but it is not clear\nwhat would be required for the person to know what red looks like.[402]\nComputationalism and functionalism\nComputationalism is the position in the philosophy of mind that the human mind is an information\nprocessing system and that thinking is a form of computing. Computationalism argues that the\nrelationship between mind and body is similar or identical to the relationship between software and\nhardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired\nby the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by\nphilosophers Jerry Fodor and Hilary Putnam.[403]\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed\ncomputer with the right inputs and outputs would thereby have a mind in exactly the same sense human\nbeings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to\nshow that even a computer capable of perfectly simulating human behavior would not have a mind.[407]\nAI welfare and rights\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel),\nand if so, to what degree.[408] But if there is a significant chance that a given machine can feel and suffer,\nthen it may be entitled to certain rights or welfare protection measures, similarly to animals.[409][410]\nSapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may\nprovide another moral basis for AI rights.[409] Robot rights are also sometimes proposed as a practical\nway to integrate autonomous agents into society.[411]\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI\nsystems. Similarly to the legal status of companies, it would have conferred rights but also\nresponsibilities.[412] Critics argued in 2018 that granting rights to AI systems would downplay the\nimportance of human rights, and that legislation should focus on user needs rather than speculative\nfuturistic scenarios. They also noted that robots lacked the autonomy to take part to society on their\nown.[413][414]\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI\nsentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot\nanalogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created\nand carelessly exploited.[410][409]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:28>\n",
    "text": "Future\nSuperintelligence and the singularity\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the\nbrightest and most gifted human mind.[399] If research into artificial general intelligence produced\nsufficiently intelligent software, it might be able to reprogram and improve itself. The improved software\nwould be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and\nVernor Vinge called a \"singularity\".[415]\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped\ncurve, slowing when they reach the physical limits of what the technology can do.[416]\nTranshumanism\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted\nthat humans and machines may merge in the future into cyborgs that are more capable and powerful than\neither. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert\nEttinger.[417]\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by\nSamuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George\nDyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[418]\nDecomputing\nArguments for decomputing have been raised by Dan McQuillan (Resisting AI: An Anti-fascist Approach\nto Artificial Intelligence, 2022), meaning an opposition to the sweeping application and expansion of\nartificial intelligence. Similar to degrowth, the approach criticizes AI as an outgrowth of the systemic\nissues and capitalist world we live in. It argues that a different future is possible, in which distance\nbetween people is reduced rather than increased through AI intermediaries.[419]\nIn fiction\nThought-capable artificial beings have appeared as storytelling devices since antiquity,[420] and have been\na persistent theme in science fiction.[421]\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation\nbecomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's\n2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery\nOne spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots\nsuch as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent\nin popular culture.[422]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:29>\n",
    "text": "Isaac Asimov introduced the Three Laws of Robotics\nin many stories, most notably with the \"Multivac\"\nsuper-intelligent computer. Asimov's laws are often\nbrought up during lay discussions of machine\nethics;[423] while almost all artificial intelligence\nresearchers are familiar with Asimov's laws through\npopular culture, they generally consider the laws\nuseless for many reasons, one of which is their\nambiguity.[424]\nThe word \"robot\" itself was coined by Karel \u010capek\nin his 1921 play R.U.R., the title standing for\nSeveral works use AI to force us to confront the\n\"Rossum's Universal Robots\".\nfundamental question of what makes us human,\nshowing us artificial beings that have the ability to\nfeel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex\nMachina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers\nthe idea that our understanding of human subjectivity is altered by technology created with artificial\nintelligence.[425]\nSee also\nArtificial consciousness \u2013 Field in cognitive science\nArtificial intelligence and elections \u2013 Use and impact of AI on political elections\nArtificial intelligence content detection \u2013 Software to detect AI-generated content\nBehavior selection algorithm \u2013 Algorithm that selects actions for intelligent agents\nBusiness process automation \u2013 Automation of business processes\nCase-based reasoning \u2013 Process of solving new problems based on the solutions of similar\npast problems\nComputational intelligence \u2013 Ability of a computer to learn a specific task from data or\nexperimental observation\nDigital immortality \u2013 Hypothetical concept of storing a personality in digital form\nEmergent algorithm \u2013 Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies \u2013 Gender biases in digital technology\nGlossary of artificial intelligence \u2013 List of definitions of terms and concepts commonly used\nin the study of artificial intelligence\nIntelligence amplification \u2013 Use of information technology to augment human intelligence\nIntelligent agent \u2013 Software agent which acts autonomously\nMind uploading \u2013 Hypothetical process of digitally emulating a brain\nOrganoid intelligence \u2013 Use of brain cells and brain organoids for intelligent computing\nRobotic process automation \u2013 Form of business process automation technology\nThe Last Day \u2013 1967 Welsh science fiction novel\nWetware computer \u2013 Computer composed of organic material\nExplanatory notes\na. This list of intelligent traits is based on the topics covered by the major AI textbooks,\nincluding: Russell & Norvig (2021), Luger & Stubblefield (2004), Poole, Mackworth & Goebel\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:30>\n",
    "text": "(1998) and Nilsson (1998)\nb. This list of tools is based on the topics covered by the major AI textbooks, including: Russell\n& Norvig (2021), Luger & Stubblefield (2004), Poole, Mackworth & Goebel (1998) and\nNilsson (1998)\nc. It is among the reasons that expert systems proved to be inefficient for capturing\nknowledge.[30][31]\nd. \"Rational agent\" is general term used in economics, philosophy and theoretical artificial\nintelligence. It can refer to anything that directs its behavior to accomplish goals, such as a\nperson, an animal, a corporation, a nation, or in the case of AI, a computer program.\ne. Alan Turing discussed the centrality of learning as early as 1950, in his classic paper\n[42]\n\"Computing Machinery and Intelligence\". In 1956, at the original Dartmouth AI summer\nconference, Ray Solomonoff wrote a report on unsupervised probabilistic machine learning:\n\"An Inductive Inference Machine\".[43]\nf. See AI winter \u00a7 Machine translation and the ALPAC report of 1966\ng. Compared with symbolic logic, formal Bayesian inference is computationally expensive. For\ninference to be tractable, most observations must be conditionally independent of one\nanother. AdSense uses a Bayesian network with over 300 million edges to learn which ads\nto serve.[93]\nh. Expectation\u2013maximization, one of the most popular algorithms in machine learning, allows\n[95]\nclustering in the presence of unknown latent variables.\ni. Some form of deep neural networks (without a specific learning algorithm) were described\nby: Warren S. McCulloch and Walter Pitts (1943)[115] Alan Turing (1948);[116] Karl Steinbuch\nand Roger David Joseph (1961).[117] Deep or recurrent networks that learned (or used\n[116]\ngradient descent) were developed by: Frank Rosenblatt(1957); Oliver Selfridge\n(1959);[117] Alexey Ivakhnenko and Valentin Lapa (1965);[118] Kaoru Nakano (1971);[119]\nShun-Ichi Amari (1972);[119] John Joseph Hopfield (1982).[119] Precursors to\n[116]\nbackpropagation were developed by: Henry J. Kelley (1960); Arthur E. Bryson\n(1962);[116] Stuart Dreyfus (1962);[116] Arthur E. Bryson and Yu-Chi Ho (1969);[116]\nBackpropagation was independently developed by: Seppo Linnainmaa (1970);[120] Paul\n[116]\nWerbos (1974).\nj. Geoffrey Hinton said, of his work on neural networks in the 1990s, \"our labeled datasets\nwere thousands of times too small. [And] our computers were millions of times too slow.\"[121]\nk. In statistics, a bias is a systematic error or deviation from the correct value. But in the\ncontext of fairness, it refers to a tendency in favor or against a certain group or individual\ncharacteristic, usually in a way that is considered unfair or harmful. A statistically unbiased\nAI system that produces disparate outcomes for different demographic groups may thus be\nviewed as biased in the ethical sense.[238]\nl. Including Jon Kleinberg (Cornell University), Sendhil Mullainathan (University of Chicago),\n[247]\nCynthia Chouldechova (Carnegie Mellon) and Sam Corbett-Davis (Stanford)\nm. Moritz Hardt (a director at the Max Planck Institute for Intelligent Systems) argues that\nmachine learning \"is fundamentally the wrong tool for a lot of domains, where you're trying\nto design interventions and mechanisms that change the world.\"[252]\nn. When the law was passed in 2018, it still contained a form of this provision.\no. This is the United Nations' definition, and includes things like land mines as well.[266]\n[277]\np. See table 4; 9% is both the OECD average and the U.S. average.\nq. Sometimes called a \"robopocalypse\"[285]\nr. \"Electronic brain\" was the term used by the press around this time.[339][341]\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:31>\n",
    "text": "s. Daniel Crevier wrote, \"the conference is generally recognized as the official birthdate of the\nnew science.\"[344] Russell and Norvig called the conference \"the inception of artificial\n[115]\nintelligence.\"\nt. Russell and Norvig wrote \"for the next 20 years the field would be dominated by these\npeople and their students.\"[345]\nu. Russell and Norvig wrote, \"it was astonishing whenever a computer did anything kind of\nsmartish\".[346]\nv. The programs described are Arthur Samuel's checkers program for the IBM 701, Daniel\nBobrow's STUDENT, Newell and Simon's Logic Theorist and Terry Winograd's SHRDLU.\nw. Russell and Norvig write: \"in almost all cases, these early systems failed on more difficult\n[350]\nproblems\"\nx. Embodied approaches to AI[357] were championed by Hans Moravec[358] and Rodney\nBrooks[359] and went by many names: Nouvelle AI.[359] Developmental robotics.[360]\ny. Matteo Wong wrote in The Atlantic: \"Whereas for decades, computer-science fields such as\nnatural-language processing, computer vision, and robotics used extremely different\nmethods, now they all use a programming method called \"deep learning\". As a result, their\ncode and approaches have become more similar, and their models are easier to integrate\n[366]\ninto one another.\"\nz. Jack Clark wrote in Bloomberg: \"After a half-decade of quiet breakthroughs in artificial\nintelligence, 2015 has been a landmark year. Computers are smarter and learning faster\nthan ever\", and noted that the number of software projects that use machine learning at\nGoogle increased from a \"sporadic usage\" in 2012 to more than 2,700 projects in 2015.[368]\naa. Nils Nilsson wrote in 1983: \"Simply put, there is wide disagreement in the field about what AI\nis all about.\"[389]\nab. Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of\nDreyfus's comments. Had he formulated them less aggressively, constructive actions they\n[394]\nsuggested might have been taken much earlier.\"\nac. Searle presented this definition of \"Strong AI\" in 1999.[404] Searle's original formulation was\n\"The appropriately programmed computer really is a mind, in the sense that computers\ngiven the right programs can be literally said to understand and have other cognitive\nstates.\"[405] Strong AI is defined similarly by Russell and Norvig: \"Stong AI \u2013 the assertion\n[406]\nthat machines that do so are actually thinking (as opposed to simulating thinking).\"\nReferences\n1. Russell & Norvig (2021), pp. 1\u20134.\n2. AI set to exceed human brain power (http://www.cnn.com/2006/TECH/science/07/24/ai.bostr\nom/) Archived (https://web.archive.org/web/20080219001624/http://www.cnn.com/2006/TEC\nH/science/07/24/ai.bostrom/) 2008-02-19 at the Wayback Machine CNN.com (July 26, 2006)\n3. Kaplan, Andreas; Haenlein, Michael (2019). \"Siri, Siri, in my hand: Who's the fairest in the\nland? On the interpretations, illustrations, and implications of artificial intelligence\". Business\nHorizons. 62: 15\u201325. doi:10.1016/j.bushor.2018.08.004 (https://doi.org/10.1016%2Fj.bushor.\n2018.08.004). ISSN 0007-6813 (https://search.worldcat.org/issn/0007-6813).\nS2CID 158433736 (https://api.semanticscholar.org/CorpusID:158433736).\n4. Artificial general intelligence: Russell & Norvig (2021, pp. 32\u201333, 1020\u20131021)\nProposal for the modern version: Pennachin & Goertzel (2007)\nWarnings of overspecialization in AI from leading researchers: Nilsson (1995), McCarthy\n(2007), Beal & Winston (2009)\n5. Russell & Norvig (2021, \u00a71.2).\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:32>\n",
    "text": "6. Dartmouth workshop: Russell & Norvig (2021, p. 18), McCorduck (2004, pp. 111\u2013136), NRC\n(1999, pp. 200\u2013201)\nThe proposal: McCarthy et al. (1955)\n7. Successful programs of the 1960s: McCorduck (2004, pp. 243\u2013252), Crevier (1993, pp. 52\u2013\n107), Moravec (1988, p. 9), Russell & Norvig (2021, pp. 19\u201321)\n8. Funding initiatives in the early 1980s: Fifth Generation Project (Japan), Alvey (UK),\nMicroelectronics and Computer Technology Corporation (US), Strategic Computing Initiative\n(US): McCorduck (2004, pp. 426\u2013441), Crevier (1993, pp. 161\u2013162, 197\u2013203, 211, 240),\nRussell & Norvig (2021, p. 23), NRC (1999, pp. 210\u2013211), Newquist (1994, pp. 235\u2013248)\n9. First AI Winter, Lighthill report, Mansfield Amendment: Crevier (1993, pp. 115\u2013117), Russell\n& Norvig (2021, pp. 21\u201322), NRC (1999, pp. 212\u2013213), Howe (1994), Newquist (1994,\npp. 189\u2013201)\n10. Second AI Winter: Russell & Norvig (2021, p. 24), McCorduck (2004, pp. 430\u2013435), Crevier\n(1993, pp. 209\u2013210), NRC (1999, pp. 214\u2013216), Newquist (1994, pp. 301\u2013318)\n11. Deep learning revolution, AlexNet: Goldman (2022), Russell & Norvig (2021, p. 26),\nMcKinsey (2018)\n12. Toews (2023).\n13. Problem-solving, puzzle solving, game playing, and deduction: Russell & Norvig (2021,\nchpt. 3\u20135), Russell & Norvig (2021, chpt. 6) (constraint satisfaction), Poole, Mackworth &\nGoebel (1998, chpt. 2, 3, 7, 9), Luger & Stubblefield (2004, chpt. 3, 4, 6, 8), Nilsson (1998,\nchpt. 7\u201312)\n14. Uncertain reasoning: Russell & Norvig (2021, chpt. 12\u201318), Poole, Mackworth & Goebel\n(1998, pp. 345\u2013395), Luger & Stubblefield (2004, pp. 333\u2013381), Nilsson (1998, chpt. 7\u201312)\n15. Intractability and efficiency and the combinatorial explosion: Russell & Norvig (2021, p. 21)\n16. Psychological evidence of the prevalence of sub-symbolic reasoning and knowledge:\nKahneman (2011), Dreyfus & Dreyfus (1986), Wason & Shapiro (1966), Kahneman, Slovic\n& Tversky (1982)\n17. Knowledge representation and knowledge engineering: Russell & Norvig (2021, chpt. 10),\nPoole, Mackworth & Goebel (1998, pp. 23\u201346, 69\u201381, 169\u2013233, 235\u2013277, 281\u2013298, 319\u2013\n345), Luger & Stubblefield (2004, pp. 227\u2013243), Nilsson (1998, chpt. 17.1\u201317.4, 18)\n18. Smoliar & Zhang (1994).\n19. Neumann & M\u00f6ller (2008).\n20. Kuperman, Reichley & Bailey (2006).\n21. McGarry (2005).\n22. Bertini, Del Bimbo & Torniai (2006).\n23. Russell & Norvig (2021), pp. 272.\n24. Representing categories and relations: Semantic networks, description logics, inheritance\n(including frames, and scripts): Russell & Norvig (2021, \u00a710.2 & 10.5), Poole, Mackworth &\nGoebel (1998, pp. 174\u2013177), Luger & Stubblefield (2004, pp. 248\u2013258), Nilsson (1998, chpt.\n18.3)\n25. Representing events and time:Situation calculus, event calculus, fluent calculus (including\nsolving the frame problem): Russell & Norvig (2021, \u00a710.3), Poole, Mackworth & Goebel\n(1998, pp. 281\u2013298), Nilsson (1998, chpt. 18.2)\n26. Causal calculus: Poole, Mackworth & Goebel (1998, pp. 335\u2013337)\n27. Representing knowledge about knowledge: Belief calculus, modal logics: Russell & Norvig\n(2021, \u00a710.4), Poole, Mackworth & Goebel (1998, pp. 275\u2013277)\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:33>\n",
    "text": "28. Default reasoning, Frame problem, default logic, non-monotonic logics, circumscription,\nclosed world assumption, abduction: Russell & Norvig (2021, \u00a710.6), Poole, Mackworth &\nGoebel (1998, pp. 248\u2013256, 323\u2013335), Luger & Stubblefield (2004, pp. 335\u2013363), Nilsson\n(1998, ~18.3.3) (Poole et al. places abduction under \"default reasoning\". Luger et al. places\nthis under \"uncertain reasoning\").\n29. Breadth of commonsense knowledge: Lenat & Guha (1989, Introduction), Crevier (1993,\npp. 113\u2013114), Moravec (1988, p. 13), Russell & Norvig (2021, pp. 241, 385, 982)\n(qualification problem)\n30. Newquist (1994), p. 296.\n31. Crevier (1993), pp. 204\u2013208.\n32. Russell & Norvig (2021), p. 528.\n33. Automated planning: Russell & Norvig (2021, chpt. 11).\n34. Automated decision making, Decision theory: Russell & Norvig (2021, chpt. 16\u201318).\n35. Classical planning: Russell & Norvig (2021, Section 11.2).\n36. Sensorless or \"conformant\" planning, contingent planning, replanning (a.k.a online\nplanning): Russell & Norvig (2021, Section 11.5).\n37. Uncertain preferences: Russell & Norvig (2021, Section 16.7) Inverse reinforcement\nlearning: Russell & Norvig (2021, Section 22.6)\n38. Information value theory: Russell & Norvig (2021, Section 16.6).\n39. Markov decision process: Russell & Norvig (2021, chpt. 17).\n40. Game theory and multi-agent decision theory: Russell & Norvig (2021, chpt. 18).\n41. Learning: Russell & Norvig (2021, chpt. 19\u201322), Poole, Mackworth & Goebel (1998,\npp. 397\u2013438), Luger & Stubblefield (2004, pp. 385\u2013542), Nilsson (1998, chpt. 3.3, 10.3,\n17.5, 20)\n42. Turing (1950).\n43. Solomonoff (1956).\n44. Unsupervised learning: Russell & Norvig (2021, pp. 653) (definition), Russell & Norvig\n(2021, pp. 738\u2013740) (cluster analysis), Russell & Norvig (2021, pp. 846\u2013860) (word\nembedding)\n45. Supervised learning: Russell & Norvig (2021, \u00a719.2) (Definition), Russell & Norvig (2021,\nChpt. 19\u201320) (Techniques)\n46. Reinforcement learning: Russell & Norvig (2021, chpt. 22), Luger & Stubblefield (2004,\npp. 442\u2013449)\n47. Transfer learning: Russell & Norvig (2021, pp. 281), The Economist (2016)\n48. \"Artificial Intelligence (AI): What Is AI and How Does It Work? | Built In\" (https://builtin.com/ar\ntificial-intelligence). builtin.com. Retrieved 30 October 2023.\n49. Computational learning theory: Russell & Norvig (2021, pp. 672\u2013674), Jordan & Mitchell\n(2015)\n50. Natural language processing (NLP): Russell & Norvig (2021, chpt. 23\u201324), Poole,\nMackworth & Goebel (1998, pp. 91\u2013104), Luger & Stubblefield (2004, pp. 591\u2013632)\n51. Subproblems of NLP: Russell & Norvig (2021, pp. 849\u2013850)\n52. Russell & Norvig (2021), pp. 856\u2013858.\n53. Dickson (2022).\n54. Modern statistical and deep learning approaches to NLP: Russell & Norvig (2021, chpt. 24),\nCambria & White (2014)\n55. Vincent (2019).\n56. Russell & Norvig (2021), pp. 875\u2013878.\n57. Bushwick (2023).\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:34>\n",
    "text": "58. Computer vision: Russell & Norvig (2021, chpt. 25), Nilsson (1998, chpt. 6)\n59. Russell & Norvig (2021), pp. 849\u2013850.\n60. Russell & Norvig (2021), pp. 895\u2013899.\n61. Russell & Norvig (2021), pp. 899\u2013901.\n62. Challa et al. (2011).\n63. Russell & Norvig (2021), pp. 931\u2013938.\n64. MIT AIL (2014).\n65. Affective computing: Thro (1993), Edelson (1991), Tao & Tan (2005), Scassellati (2002)\n66. Waddell (2018).\n67. Poria et al. (2017).\n68. Search algorithms: Russell & Norvig (2021, chpts. 3\u20135), Poole, Mackworth & Goebel (1998,\npp. 113\u2013163), Luger & Stubblefield (2004, pp. 79\u2013164, 193\u2013219), Nilsson (1998, chpts. 7\u2013\n12)\n69. State space search: Russell & Norvig (2021, chpt. 3)\n70. Russell & Norvig (2021), sect. 11.2.\n71. Uninformed searches (breadth first search, depth-first search and general state space\nsearch): Russell & Norvig (2021, sect. 3.4), Poole, Mackworth & Goebel (1998, pp. 113\u2013\n132), Luger & Stubblefield (2004, pp. 79\u2013121), Nilsson (1998, chpt. 8)\n72. Heuristic or informed searches (e.g., greedy best first and A*): Russell & Norvig (2021, sect.\n3.5), Poole, Mackworth & Goebel (1998, pp. 132\u2013147), Poole & Mackworth (2017, sect.\n3.6), Luger & Stubblefield (2004, pp. 133\u2013150)\n73. Adversarial search: Russell & Norvig (2021, chpt. 5)\n74. Local or \"optimization\" search: Russell & Norvig (2021, chpt. 4)\n75. Singh Chauhan, Nagesh (18 December 2020). \"Optimization Algorithms in Neural\nNetworks\" (https://www.kdnuggets.com/optimization-algorithms-in-neural-networks).\nKDnuggets. Retrieved 13 January 2024.\n76. Evolutionary computation: Russell & Norvig (2021, sect. 4.1.2)\n77. Merkle & Middendorf (2013).\n78. Logic: Russell & Norvig (2021, chpts. 6\u20139), Luger & Stubblefield (2004, pp. 35\u201377), Nilsson\n(1998, chpt. 13\u201316)\n79. Propositional logic: Russell & Norvig (2021, chpt. 6), Luger & Stubblefield (2004, pp. 45\u201350),\nNilsson (1998, chpt. 13)\n80. First-order logic and features such as equality: Russell & Norvig (2021, chpt. 7), Poole,\nMackworth & Goebel (1998, pp. 268\u2013275), Luger & Stubblefield (2004, pp. 50\u201362), Nilsson\n(1998, chpt. 15)\n81. Logical inference: Russell & Norvig (2021, chpt. 10)\n82. logical deduction as search: Russell & Norvig (2021, sects. 9.3, 9.4), Poole, Mackworth &\nGoebel (1998, pp. ~46\u201352), Luger & Stubblefield (2004, pp. 62\u201373), Nilsson (1998, chpt.\n4.2, 7.2)\n83. Resolution and unification: Russell & Norvig (2021, sections 7.5.2, 9.2, 9.5)\n84. Warren, D.H.; Pereira, L.M.; Pereira, F. (1977). \"Prolog-the language and its implementation\ncompared with Lisp\". ACM SIGPLAN Notices. 12 (8): 109\u2013115. doi:10.1145/872734.806939\n(https://doi.org/10.1145%2F872734.806939).\n85. Fuzzy logic: Russell & Norvig (2021, pp. 214, 255, 459), Scientific American (1999)\n86. Stochastic methods for uncertain reasoning: Russell & Norvig (2021, chpt. 12\u201318, 20),\nPoole, Mackworth & Goebel (1998, pp. 345\u2013395), Luger & Stubblefield (2004, pp. 165\u2013191,\n333\u2013381), Nilsson (1998, chpt. 19)\n87. decision theory and decision analysis: Russell & Norvig (2021, chpt. 16\u201318), Poole,\nMackworth & Goebel (1998, pp. 381\u2013394)\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:35>\n",
    "text": "88. Information value theory: Russell & Norvig (2021, sect. 16.6)\n89. Markov decision processes and dynamic decision networks: Russell & Norvig (2021, chpt.\n17)\n90. Stochastic temporal models: Russell & Norvig (2021, chpt. 14) Hidden Markov model:\nRussell & Norvig (2021, sect. 14.3) Kalman filters: Russell & Norvig (2021, sect. 14.4)\nDynamic Bayesian networks: Russell & Norvig (2021, sect. 14.5)\n91. Game theory and mechanism design: Russell & Norvig (2021, chpt. 18)\n92. Bayesian networks: Russell & Norvig (2021, sects. 12.5\u201312.6, 13.4\u201313.5, 14.3\u201314.5, 16.5,\n20.2\u201320.3), Poole, Mackworth & Goebel (1998, pp. 361\u2013381), Luger & Stubblefield (2004,\npp. ~182\u2013190, \u2248363\u2013379), Nilsson (1998, chpt. 19.3\u201319.4)\n93. Domingos (2015), chpt. 6.\n94. Bayesian inference algorithm: Russell & Norvig (2021, sect. 13.3\u201313.5), Poole, Mackworth\n& Goebel (1998, pp. 361\u2013381), Luger & Stubblefield (2004, pp. ~363\u2013379), Nilsson (1998,\nchpt. 19.4 & 7)\n95. Domingos (2015), p. 210.\n96. Bayesian learning and the expectation\u2013maximization algorithm: Russell & Norvig (2021,\nchpt. 20), Poole, Mackworth & Goebel (1998, pp. 424\u2013433), Nilsson (1998, chpt. 20),\nDomingos (2015, p. 210)\n97. Bayesian decision theory and Bayesian decision networks: Russell & Norvig (2021, sect.\n16.5)\n98. Statistical learning methods and classifiers: Russell & Norvig (2021, chpt. 20),\n99. Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from\ndata analysis to generative AI. Intellisemantic Editions. ISBN 978-8-8947-8760-3.\n100. Decision trees: Russell & Norvig (2021, sect. 19.3), Domingos (2015, p. 88)\n101. Non-parameteric learning models such as K-nearest neighbor and support vector machines:\nRussell & Norvig (2021, sect. 19.7), Domingos (2015, p. 187) (k-nearest neighbor)\nDomingos (2015, p. 88) (kernel methods)\n102. Domingos (2015), p. 152.\n103. Naive Bayes classifier: Russell & Norvig (2021, sect. 12.6), Domingos (2015, p. 152)\n104. Neural networks: Russell & Norvig (2021, chpt. 21), Domingos (2015, Chapter 4)\n105. Gradient calculation in computational graphs, backpropagation, automatic differentiation:\nRussell & Norvig (2021, sect. 21.2), Luger & Stubblefield (2004, pp. 467\u2013474), Nilsson\n(1998, chpt. 3.3)\n106. Universal approximation theorem: Russell & Norvig (2021, p. 752) The theorem: Cybenko\n(1988), Hornik, Stinchcombe & White (1989)\n107. Feedforward neural networks: Russell & Norvig (2021, sect. 21.1)\n108. Recurrent neural networks: Russell & Norvig (2021, sect. 21.6)\n109. Perceptrons: Russell & Norvig (2021, pp. 21, 22, 683, 22)\n110. Deep learning: Russell & Norvig (2021, chpt. 21), Goodfellow, Bengio & Courville (2016),\nHinton et al. (2016), Schmidhuber (2015)\n111. Convolutional neural networks: Russell & Norvig (2021, sect. 21.3)\n112. Deng & Yu (2014), pp. 199\u2013200.\n113. Ciresan, Meier & Schmidhuber (2012).\n114. Russell & Norvig (2021), p. 750.\n115. Russell & Norvig (2021), p. 17.\n116. Russell & Norvig (2021), p. 785.\n117. Schmidhuber (2022), sect. 5.\n118. Schmidhuber (2022), sect. 6.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:36>\n",
    "text": "119. Schmidhuber (2022), sect. 7.\n120. Schmidhuber (2022), sect. 8.\n121. Quoted in Christian (2020, p. 22)\n122. Smith (2023).\n123. \"Explained: Generative AI\" (https://news.mit.edu/2023/explained-generative-ai-1109). 9\nNovember 2023.\n124. \"AI Writing and Content Creation Tools\" (https://mitsloanedtech.mit.edu/ai/tools/writing). MIT\nSloan Teaching & Learning Technologies. Archived (https://web.archive.org/web/202312252\n32503/https://mitsloanedtech.mit.edu/ai/tools/writing/) from the original on 25 December\n2023. Retrieved 25 December 2023.\n125. Marmouyet (2023).\n126. Kobielus (2019).\n127. Thomason, James (21 May 2024). \"Mojo Rising: The resurgence of AI-first programming\nlanguages\" (https://venturebeat.com/ai/mojo-rising-the-resurgence-of-ai-first-programming-l\nanguages). VentureBeat. Archived (https://web.archive.org/web/20240627143853/https://ve\nnturebeat.com/ai/mojo-rising-the-resurgence-of-ai-first-programming-languages/) from the\noriginal on 27 June 2024. Retrieved 26 May 2024.\n128. Wodecki, Ben (5 May 2023). \"7 AI Programming Languages You Need to Know\" (https://aibu\nsiness.com/verticals/7-ai-programming-languages-you-need-to-know). AI Business.\nArchived (https://web.archive.org/web/20240725164443/https://aibusiness.com/verticals/7-ai\n-programming-languages-you-need-to-know) from the original on 25 July 2024. Retrieved\n5 October 2024.\n129. Plumb, Taryn (18 September 2024). \"Why Jensen Huang and Marc Benioff see 'gigantic'\nopportunity for agentic AI\" (https://venturebeat.com/ai/why-jensen-huang-and-marc-benioff-s\nee-gigantic-opportunity-for-agentic-ai/). VentureBeat. Archived (https://web.archive.org/web/\n20241005165649/https://venturebeat.com/ai/why-jensen-huang-and-marc-benioff-see-gigan\ntic-opportunity-for-agentic-ai/) from the original on 5 October 2024. Retrieved 4 October\n2024.\n130. Mims, Christopher (19 September 2020). \"Huang's Law Is the New Moore's Law, and\nExplains Why Nvidia Wants Arm\" (https://www.wsj.com/articles/huangs-law-is-the-new-moor\nes-law-and-explains-why-nvidia-wants-arm-11600488001). Wall Street Journal. ISSN 0099-\n9660 (https://search.worldcat.org/issn/0099-9660). Archived (https://web.archive.org/web/20\n231002080608/https://www.wsj.com/articles/huangs-law-is-the-new-moores-law-and-explain\ns-why-nvidia-wants-arm-11600488001) from the original on 2 October 2023. Retrieved\n19 January 2025.\n131. Davenport, T; Kalakota, R (June 2019). \"The potential for artificial intelligence in healthcare\"\n(https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6616181). Future Healthc J. 6 (2): 94\u201398.\ndoi:10.7861/futurehosp.6-2-94 (https://doi.org/10.7861%2Ffuturehosp.6-2-94).\nPMC 6616181 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6616181). PMID 31363513\n(https://pubmed.ncbi.nlm.nih.gov/31363513).\n132. Lyakhova, U.A.; Lyakhov, P.A. (2024). \"Systematic review of approaches to detection and\nclassification of skin cancer using artificial intelligence: Development and prospects\" (https://\nlinkinghub.elsevier.com/retrieve/pii/S0010482524008278). Computers in Biology and\nMedicine. 178: 108742. doi:10.1016/j.compbiomed.2024.108742 (https://doi.org/10.1016%2\nFj.compbiomed.2024.108742). PMID 38875908 (https://pubmed.ncbi.nlm.nih.gov/3887590\n8). Archived (https://web.archive.org/web/20241203172502/https://linkinghub.elsevier.com/r\netrieve/pii/S0010482524008278) from the original on 3 December 2024. Retrieved\n10 October 2024.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:37>\n",
    "text": "133. Alqudaihi, Kawther S.; Aslam, Nida; Khan, Irfan Ullah; Almuhaideb, Abdullah M.; Alsunaidi,\nShikah J.; Ibrahim, Nehad M. Abdel Rahman; Alhaidari, Fahd A.; Shaikh, Fatema S.;\nAlsenbel, Yasmine M.; Alalharith, Dima M.; Alharthi, Hajar M.; Alghamdi, Wejdan M.;\nAlshahrani, Mohammed S. (2021). \"Cough Sound Detection and Diagnosis Using Artificial\nIntelligence Techniques: Challenges and Opportunities\" (https://www.ncbi.nlm.nih.gov/pmc/a\nrticles/PMC8545201). IEEE Access. 9: 102327\u2013102344. Bibcode:2021IEEEA...9j2327A (htt\nps://ui.adsabs.harvard.edu/abs/2021IEEEA...9j2327A). doi:10.1109/ACCESS.2021.3097559\n(https://doi.org/10.1109%2FACCESS.2021.3097559). ISSN 2169-3536 (https://search.world\ncat.org/issn/2169-3536). PMC 8545201 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC854\n5201). PMID 34786317 (https://pubmed.ncbi.nlm.nih.gov/34786317).\n134. Bax, Monique; Thorpe, Jordan; Romanov, Valentin (December 2023). \"The future of\npersonalized cardiovascular medicine demands 3D and 4D printing, stem cells, and artificial\nintelligence\" (https://doi.org/10.3389%2Ffsens.2023.1294721). Frontiers in Sensors. 4.\ndoi:10.3389/fsens.2023.1294721 (https://doi.org/10.3389%2Ffsens.2023.1294721).\nISSN 2673-5067 (https://search.worldcat.org/issn/2673-5067).\n135. Dankwa-Mullan, Irene (2024). \"Health Equity and Ethical Considerations in Using Artificial\nIntelligence in Public Health and Medicine\" (https://www.cdc.gov/pcd/issues/2024/24_0245.h\ntm). Preventing Chronic Disease. 21: E64. doi:10.5888/pcd21.240245 (https://doi.org/10.588\n8%2Fpcd21.240245). ISSN 1545-1151 (https://search.worldcat.org/issn/1545-1151).\nPMC 11364282 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11364282).\nPMID 39173183 (https://pubmed.ncbi.nlm.nih.gov/39173183).\n136. Jumper, J; Evans, R; Pritzel, A (2021). \"Highly accurate protein structure prediction with\nAlphaFold\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605). Nature. 596 (7873):\n583\u2013589. Bibcode:2021Natur.596..583J (https://ui.adsabs.harvard.edu/abs/2021Natur.596..\n583J). doi:10.1038/s41586-021-03819-2 (https://doi.org/10.1038%2Fs41586-021-03819-2).\nPMC 8371605 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605). PMID 34265844\n(https://pubmed.ncbi.nlm.nih.gov/34265844).\n137. \"AI discovers new class of antibiotics to kill drug-resistant bacteria\" (https://www.newscientis\nt.com/article/2409706-ai-discovers-new-class-of-antibiotics-to-kill-drug-resistant-bacteria/).\n20 December 2023. Archived (https://web.archive.org/web/20240916014421/https://www.ne\nwscientist.com/article/2409706-ai-discovers-new-class-of-antibiotics-to-kill-drug-resistant-ba\ncteria/) from the original on 16 September 2024. Retrieved 5 October 2024.\n138. \"AI speeds up drug design for Parkinson's ten-fold\" (https://www.cam.ac.uk/research/news/a\ni-speeds-up-drug-design-for-parkinsons-ten-fold). Cambridge University. 17 April 2024.\nArchived (https://web.archive.org/web/20241005165755/https://www.cam.ac.uk/research/ne\nws/ai-speeds-up-drug-design-for-parkinsons-ten-fold) from the original on 5 October 2024.\nRetrieved 5 October 2024.\n139. Horne, Robert I.; Andrzejewska, Ewa A.; Alam, Parvez; Brotzakis, Z. Faidon; Srivastava,\nAnkit; Aubert, Alice; Nowinska, Magdalena; Gregory, Rebecca C.; Staats, Roxine; Possenti,\nAndrea; Chia, Sean; Sormanni, Pietro; Ghetti, Bernardino; Caughey, Byron; Knowles,\nTuomas P. J.; Vendruscolo, Michele (17 April 2024). \"Discovery of potent inhibitors of \u03b1-\nsynuclein aggregation using structure-based iterative learning\" (https://www.ncbi.nlm.nih.go\nv/pmc/articles/PMC11062903). Nature Chemical Biology. 20 (5). Nature: 634\u2013645.\ndoi:10.1038/s41589-024-01580-x (https://doi.org/10.1038%2Fs41589-024-01580-x).\nPMC 11062903 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11062903).\nPMID 38632492 (https://pubmed.ncbi.nlm.nih.gov/38632492).\n140. Grant, Eugene F.; Lardner, Rex (25 July 1952). \"The Talk of the Town \u2013 It\" (https://www.new\nyorker.com/magazine/1952/08/02/it). The New Yorker. ISSN 0028-792X (https://search.worl\ndcat.org/issn/0028-792X). Archived (https://web.archive.org/web/20200216034025/https://w\nww.newyorker.com/magazine/1952/08/02/it) from the original on 16 February 2020.\nRetrieved 28 January 2024.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:38>\n",
    "text": "141. Anderson, Mark Robert (11 May 2017). \"Twenty years on from Deep Blue vs Kasparov: how\na chess match started the big data revolution\" (https://theconversation.com/twenty-years-on-\nfrom-deep-blue-vs-kasparov-how-a-chess-match-started-the-big-data-revolution-76882).\nThe Conversation. Archived (https://web.archive.org/web/20240917000827/https://theconver\nsation.com/twenty-years-on-from-deep-blue-vs-kasparov-how-a-chess-match-started-the-bi\ng-data-revolution-76882) from the original on 17 September 2024. Retrieved 28 January\n2024.\n142. Markoff, John (16 February 2011). \"Computer Wins on 'Jeopardy!': Trivial, It's Not\" (https://w\nww.nytimes.com/2011/02/17/science/17jeopardy-watson.html). The New York Times.\nISSN 0362-4331 (https://search.worldcat.org/issn/0362-4331). Archived (https://web.archive.\norg/web/20141022023202/http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.\nhtml) from the original on 22 October 2014. Retrieved 28 January 2024.\n143. Byford, Sam (27 May 2017). \"AlphaGo retires from competitive Go after defeating world\nnumber one 3\u20130\" (https://www.theverge.com/2017/5/27/15704088/alphago-ke-jie-game-3-re\nsult-retires-future). The Verge. Archived (https://web.archive.org/web/20170607184301/http\ns://www.theverge.com/2017/5/27/15704088/alphago-ke-jie-game-3-result-retires-future)\nfrom the original on 7 June 2017. Retrieved 28 January 2024.\n144. Brown, Noam; Sandholm, Tuomas (30 August 2019). \"Superhuman AI for multiplayer poker\"\n(https://www.science.org/doi/10.1126/science.aay2400). Science. 365 (6456): 885\u2013890.\nBibcode:2019Sci...365..885B (https://ui.adsabs.harvard.edu/abs/2019Sci...365..885B).\ndoi:10.1126/science.aay2400 (https://doi.org/10.1126%2Fscience.aay2400). ISSN 0036-\n8075 (https://search.worldcat.org/issn/0036-8075). PMID 31296650 (https://pubmed.ncbi.nl\nm.nih.gov/31296650).\n145. \"MuZero: Mastering Go, chess, shogi and Atari without rules\" (https://deepmind.google/disc\nover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules). Google DeepMind. 23\nDecember 2020. Retrieved 28 January 2024.\n146. Sample, Ian (30 October 2019). \"AI becomes grandmaster in 'fiendishly complex' StarCraft\nII\" (https://www.theguardian.com/technology/2019/oct/30/ai-becomes-grandmaster-in-fiendis\nhly-complex-starcraft-ii). The Guardian. ISSN 0261-3077 (https://search.worldcat.org/issn/02\n61-3077). Archived (https://web.archive.org/web/20201229185547/https://www.theguardian.\ncom/technology/2019/oct/30/ai-becomes-grandmaster-in-fiendishly-complex-starcraft-ii)\nfrom the original on 29 December 2020. Retrieved 28 January 2024.\n147. Wurman, P. R.; Barrett, S.; Kawamoto, K. (2022). \"Outracing champion Gran Turismo\ndrivers with deep reinforcement learning\" (https://www.researchsquare.com/article/rs-79595\n4/latest.pdf) (PDF). Nature. 602 (7896): 223\u2013228. Bibcode:2022Natur.602..223W (https://ui.\nadsabs.harvard.edu/abs/2022Natur.602..223W). doi:10.1038/s41586-021-04357-7 (https://d\noi.org/10.1038%2Fs41586-021-04357-7). PMID 35140384 (https://pubmed.ncbi.nlm.nih.go\nv/35140384).\n148. Wilkins, Alex (13 March 2024). \"Google AI learns to play open-world video games by\nwatching them\" (https://www.newscientist.com/article/2422101-google-ai-learns-to-play-ope\nn-world-video-games-by-watching-them). New Scientist. Archived (https://web.archive.org/w\neb/20240726182946/https://www.newscientist.com/article/2422101-google-ai-learns-to-play-\nopen-world-video-games-by-watching-them/) from the original on 26 July 2024. Retrieved\n21 July 2024.\n149. Wu, Zhengxuan; Arora, Aryaman; Wang, Zheng; Geiger, Atticus; Jurafsky, Dan; Manning,\nChristopher D.; Potts, Christopher (2024). \"ReFT: Representation Finetuning for Language\nModels\". NeurIPS. arXiv:2404.03592 (https://arxiv.org/abs/2404.03592).\n150. \"Improving mathematical reasoning with process supervision\" (https://openai.com/index/imp\nroving-mathematical-reasoning-with-process-supervision/). OpenAI. 31 May 2023. Retrieved\n26 January 2025.\n151. Srivastava, Saurabh (29 February 2024). \"Functional Benchmarks for Robust Evaluation of\nReasoning Performance, and the Reasoning Gap\". arXiv:2402.19450 (https://arxiv.org/abs/2\n402.19450) [cs.AI (https://arxiv.org/archive/cs.AI)].\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:39>\n",
    "text": "152. Lightman, Hunter; Kosaraju, Vineet; Burda, Yura; Edwards, Harri; Baker, Bowen; Lee,\nTeddy; Leike, Jan; Schulman, John; Sutskever, Ilya; Cobbe, Karl (2023). \"Let's Verify Step\nby Step\". arXiv:2305.20050v1 (https://arxiv.org/abs/2305.20050v1) [cs.LG (https://arxiv.org/a\nrchive/cs.LG)].\n153. Franzen, Carl (8 August 2024). \"Alibaba claims no. 1 spot in AI math models with Qwen2-\nMath\" (https://venturebeat.com/ai/alibaba-claims-no-1-spot-in-ai-math-models-with-qwen2-m\nath/). VentureBeat. Retrieved 16 February 2025.\n154. Franzen, Carl (9 January 2025). \"Microsoft's new rStar-Math technique upgrades small\nmodels to outperform OpenAI's o1-preview at math problems\" (https://venturebeat.com/ai/mi\ncrosofts-new-rstar-math-technique-upgrades-small-models-to-outperform-openais-o1-previe\nw-at-math-problems/). VentureBeat. Retrieved 26 January 2025.\n155. Roberts, Siobhan (25 July 2024). \"AI achieves silver-medal standard solving International\nMathematical Olympiad problems\" (https://www.nytimes.com/2024/07/25/science/ai-math-al\nphaproof-deepmind.html). The New York Times. Archived (https://web.archive.org/web/2024\n0926131402/https://www.nytimes.com/2024/07/25/science/ai-math-alphaproof-deepmind.ht\nml) from the original on 26 September 2024. Retrieved 7 August 2024.\n156. Azerbayev, Zhangir; Schoelkopf, Hailey; Paster, Keiran; Santos, Marco Dos; McAleer',\nStephen; Jiang, Albert Q.; Deng, Jia; Biderman, Stella; Welleck, Sean (16 October 2023).\n\"Llemma: An Open Language Model For Mathematics\" (https://blog.eleuther.ai/llemma/).\nEleutherAI Blog. Retrieved 26 January 2025.\n157. \"Julius AI\" (https://julius.ai/home/ai-math). julius.ai.\n158. McFarland, Alex (12 July 2024). \"8 Best AI for Math Tools (January 2025)\" (https://www.unit\ne.ai/best-ai-for-math-tools/). Unite.AI. Retrieved 26 January 2025.\n159. Matthew Finio & Amanda Downie: IBM Think 2024 Primer, \"What is Artificial Intelligence (AI)\nin Finance?\" 8 Dec. 2023\n160. M. Nicolas, J. Firzli: Pensions Age / European Pensions magazine, \"Artificial Intelligence:\nAsk the Industry\", May\u2013June 2024. https://videovoice.org/ai-in-finance-innovation-\nentrepreneurship-vs-over-regulation-with-the-eus-artificial-intelligence-act-wont-work-as-\nintended/ Archived (https://web.archive.org/web/20240911125502/https://videovoice.org/ai-i\nn-finance-innovation-entrepreneurship-vs-over-regulation-with-the-eus-artificial-intelligence-\nact-wont-work-as-intended/) 11 September 2024 at the Wayback Machine.\n161. Congressional Research Service (2019). Artificial Intelligence and National Security (https://f\nas.org/sgp/crs/natsec/R45178.pdf) (PDF). Washington, DC: Congressional Research\nService. Archived (https://web.archive.org/web/20200508062631/https://fas.org/sgp/crs/nats\nec/R45178.pdf) (PDF) from the original on 8 May 2020. Retrieved 25 February 2024.PD-\nnotice\n162. Slyusar, Vadym (2019). Artificial intelligence as the basis of future control networks\n(Preprint). doi:10.13140/RG.2.2.30247.50087 (https://doi.org/10.13140%2FRG.2.2.30247.5\n0087).\n163. Iraqi, Amjad (3 April 2024). \"'Lavender': The AI machine directing Israel's bombing spree in\nGaza\" (https://www.972mag.com/lavender-ai-israeli-army-gaza/). +972 Magazine. Archived\n(https://web.archive.org/web/20241010022042/https://www.972mag.com/lavender-ai-israeli-\narmy-gaza/) from the original on 10 October 2024. Retrieved 6 April 2024.\n164. Davies, Harry; McKernan, Bethan; Sabbagh, Dan (1 December 2023). \"'The Gospel': how\nIsrael uses AI to select bombing targets in Gaza\" (https://www.theguardian.com/world/2023/\ndec/01/the-gospel-how-israel-uses-ai-to-select-bombing-targets). The Guardian. Archived (h\nttps://web.archive.org/web/20231206213901/https://www.theguardian.com/world/2023/dec/0\n1/the-gospel-how-israel-uses-ai-to-select-bombing-targets) from the original on 6 December\n2023. Retrieved 4 December 2023.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:40>\n",
    "text": "165. Marti, J Werner (10 August 2024). \"Drohnen haben den Krieg in der Ukraine revolutioniert,\ndoch sie sind empfindlich auf St\u00f6rsender \u2013 deshalb sollen sie jetzt autonom operieren\" (http\ns://www.nzz.ch/international/die-ukraine-setzt-auf-drohnen-die-autonom-navigieren-und-toet\nen-koennen-ld.1838731). Neue Z\u00fcrcher Zeitung (in German). Archived (https://web.archive.\norg/web/20240810054043/https://www.nzz.ch/international/die-ukraine-setzt-auf-drohnen-di\ne-autonom-navigieren-und-toeten-koennen-ld.1838731) from the original on 10 August\n2024. Retrieved 10 August 2024.\n166. Newsom, Gavin; Weber, Shirley N. (5 September 2023). \"Executive Order N-12-23\" (https://\nwww.gov.ca.gov/wp-content/uploads/2023/09/AI-EO-No.12-_-GGN-Signed.pdf) (PDF).\nExecutive Department, State of California. Archived (https://web.archive.org/web/202402212\n22035/https://www.gov.ca.gov/wp-content/uploads/2023/09/AI-EO-No.12-_-GGN-Signed.pd\nf) (PDF) from the original on 21 February 2024. Retrieved 7 September 2023.\n167. Pinaya, Walter H. L.; Graham, Mark S.; Kerfoot, Eric; Tudosiu, Petru-Daniel; Dafflon,\nJessica; Fernandez, Virginia; Sanchez, Pedro; Wolleb, Julia; da Costa, Pedro F.; Patel,\nAshay (2023). \"Generative AI for Medical Imaging: extending the MONAI Framework\".\narXiv:2307.15208 (https://arxiv.org/abs/2307.15208) [eess.IV (https://arxiv.org/archive/eess.I\nV)].\n168. \"What is ChatGPT, DALL-E, and generative AI?\" (https://www.mckinsey.com/featured-insight\ns/mckinsey-explainers/what-is-generative-ai). McKinsey. Archived (https://web.archive.org/w\neb/20230423114030/https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-\nis-generative-ai) from the original on 23 April 2023. Retrieved 14 December 2024.\n169. \"What is generative AI?\" (https://www.ibm.com/topics/generative-ai). IBM. 22 March 2024.\nArchived (https://web.archive.org/web/20241213143644/https://www.ibm.com/topics/generat\nive-ai) from the original on 13 December 2024. Retrieved 13 December 2024.\n170. Pasick, Adam (27 March 2023). \"Artificial Intelligence Glossary: Neural Networks and Other\nTerms Explained\" (https://www.nytimes.com/article/ai-artificial-intelligence-glossary.html).\nThe New York Times. ISSN 0362-4331 (https://search.worldcat.org/issn/0362-4331).\nArchived (https://web.archive.org/web/20230901183440/https://www.nytimes.com/article/ai-\nartificial-intelligence-glossary.html) from the original on 1 September 2023. Retrieved\n22 April 2023.\n171. Karpathy, Andrej; Abbeel, Pieter; Brockman, Greg; Chen, Peter; Cheung, Vicki; Duan, Yan;\nGoodfellow, Ian; Kingma, Durk; Ho, Jonathan; Rein Houthooft; Tim Salimans; John\nSchulman; Ilya Sutskever; Wojciech Zaremba (16 June 2016). \"Generative models\" (https://\nopenai.com/research/generative-models). OpenAI. Archived (https://web.archive.org/web/20\n231117151617/https://openai.com/research/generative-models) from the original on 17\nNovember 2023. Retrieved 15 March 2023.\n172. Griffith, Erin; Metz, Cade (27 January 2023). \"Anthropic Said to Be Closing In on $300\nMillion in New A.I. Funding\" (https://www.nytimes.com/2023/01/27/technology/anthropic-ai-fu\nnding.html). The New York Times. Archived (https://web.archive.org/web/20231209074235/h\nttps://www.nytimes.com/2023/01/27/technology/anthropic-ai-funding.html) from the original\non 9 December 2023. Retrieved 14 March 2023.\n173. Lanxon, Nate; Bass, Dina; Davalos, Jackie (10 March 2023). \"A Cheat Sheet to AI\nBuzzwords and Their Meanings\" (https://news.bloomberglaw.com/tech-and-telecom-law/a-c\nheat-sheet-to-ai-buzzwords-and-their-meanings-quicktake). Bloomberg News. Archived (http\ns://web.archive.org/web/20231117140835/https://news.bloomberglaw.com/tech-and-telecom\n-law/a-cheat-sheet-to-ai-buzzwords-and-their-meanings-quicktake) from the original on 17\nNovember 2023. Retrieved 14 March 2023.\n174. Metz, Cade (14 March 2023). \"OpenAI Plans to Up the Ante in Tech's A.I. Race\" (https://ww\nw.nytimes.com/2023/03/14/technology/openai-gpt4-chatgpt.html). The New York Times.\nISSN 0362-4331 (https://search.worldcat.org/issn/0362-4331). Archived (https://web.archive.\norg/web/20230331011258/https://www.nytimes.com/2023/03/14/technology/openai-gpt4-cha\ntgpt.html) from the original on 31 March 2023. Retrieved 31 March 2023.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:41>\n",
    "text": "175. Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv\n(20 January 2022). \"LaMDA: Language Models for Dialog Applications\". arXiv:2201.08239\n(https://arxiv.org/abs/2201.08239) [cs.CL (https://arxiv.org/archive/cs.CL)].\n176. Roose, Kevin (21 October 2022). \"A Coming-Out Party for Generative A.I., Silicon Valley's\nNew Craze\" (https://www.nytimes.com/2022/10/21/technology/generative-ai.html). The New\nYork Times. Archived (https://web.archive.org/web/20230215010524/https://www.nytimes.co\nm/2022/10/21/technology/generative-ai.html) from the original on 15 February 2023.\nRetrieved 14 March 2023.\n177. Metz, Cade (15 February 2024). \"OpenAI Unveils A.I. That Instantly Generates Eye-Popping\nVideos\" (https://www.nytimes.com/2024/02/15/technology/openai-sora-videos.html). The\nNew York Times. ISSN 0362-4331 (https://search.worldcat.org/issn/0362-4331). Archived (ht\ntps://web.archive.org/web/20240215220626/https://www.nytimes.com/2024/02/15/technolog\ny/openai-sora-videos.html) from the original on 15 February 2024. Retrieved 16 February\n2024.\n178. Griffith, Erin; Metz, Cade (27 January 2023). \"Anthropic Said to Be Closing In on $300\nMillion in New A.I. Funding\" (https://www.nytimes.com/2023/01/27/technology/anthropic-ai-fu\nnding.html). The New York Times. Archived (https://web.archive.org/web/20231209074235/h\nttps://www.nytimes.com/2023/01/27/technology/anthropic-ai-funding.html) from the original\non 9 December 2023. Retrieved 14 March 2023.\n179. \"The race of the AI labs heats up\" (https://www.economist.com/business/2023/01/30/the-rac\ne-of-the-ai-labs-heats-up). The Economist. 30 January 2023. Archived (https://web.archive.o\nrg/web/20231117162947/https://www.economist.com/business/2023/01/30/the-race-of-the-a\ni-labs-heats-up) from the original on 17 November 2023. Retrieved 14 March 2023.\n180. Yang, June; Gokturk, Burak (14 March 2023). \"Google Cloud brings generative AI to\ndevelopers, businesses, and governments\" (https://cloud.google.com/blog/products/ai-mach\nine-learning/generative-ai-for-businesses-and-governments). Archived (https://web.archive.o\nrg/web/20231117160059/https://cloud.google.com/blog/products/ai-machine-learning/gener\native-ai-for-businesses-and-governments) from the original on 17 November 2023.\nRetrieved 15 March 2023.\n181. Simon, Felix M.; Altay, Sacha; Mercier, Hugo (18 October 2023). \"Misinformation reloaded?\nFears about the impact of generative AI on misinformation are overblown\" (https://cnrs.hal.s\ncience/hal-04282032/file/simon_generative_AI_fears_20231018.pdf) (PDF). Harvard\nKennedy School Misinformation Review. doi:10.37016/mr-2020-127 (https://doi.org/10.3701\n6%2Fmr-2020-127). S2CID 264113883 (https://api.semanticscholar.org/CorpusID:26411388\n3). Retrieved 16 November 2023.\n182. Hendrix, Justin (16 May 2023). \"Transcript: Senate Judiciary Subcommittee Hearing on\nOversight of AI\" (https://techpolicy.press/transcript-senate-judiciary-subcommittee-hearing-o\nn-oversight-of-ai/). techpolicy.press. Archived (https://web.archive.org/web/2023111716000\n4/https://techpolicy.press/transcript-senate-judiciary-subcommittee-hearing-on-oversight-of-a\ni/) from the original on 17 November 2023. Retrieved 19 May 2023.\n183. \"New AI systems collide with copyright law\" (https://www.bbc.co.uk/news/business-6623126\n8). BBC News. 1 August 2023. Retrieved 28 September 2024.\n184. Poole, David; Mackworth, Alan (2023). Artificial Intelligence, Foundations of Computational\nAgents (https://doi.org/10.1017/9781009258227) (3rd ed.). Cambridge University Press.\ndoi:10.1017/9781009258227 (https://doi.org/10.1017%2F9781009258227). ISBN 978-1-\n0092-5819-7. Archived (https://web.archive.org/web/20241005165650/https://www.cambridg\ne.org/highereducation/books/artificial-intelligence/C113F6CE284AB00F5489EBA5A59B93B\n7#overview) from the original on 5 October 2024. Retrieved 5 October 2024.\n185. Russell, Stuart; Norvig, Peter (2020). Artificial Intelligence: A Modern Approach (4th ed.).\nPearson. ISBN 978-0-1346-1099-3.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:42>\n",
    "text": "186. \"Why agents are the next frontier of generative AI\" (https://www.mckinsey.com/capabilities/m\nckinsey-digital/our-insights/why-agents-are-the-next-frontier-of-generative-ai). McKinsey\nDigital. 24 July 2024. Archived (https://web.archive.org/web/20241003212335/https://www.m\nckinsey.com/capabilities/mckinsey-digital/our-insights/why-agents-are-the-next-frontier-of-ge\nnerative-ai) from the original on 3 October 2024. Retrieved 10 August 2024.\n187. Figueiredo, Mayara Costa; Ankrah, Elizabeth; Powell, Jacquelyn E.; Epstein, Daniel A.;\nChen, Yunan (12 January 2024). \"Powered by AI: Examining How AI Descriptions Influence\nPerceptions of Fertility Tracking Applications\" (https://dl.acm.org/doi/10.1145/3631414).\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol. 7 (4): 154:1\u2013154:24.\ndoi:10.1145/3631414 (https://doi.org/10.1145%2F3631414).\n188. Power, Jennifer; Pym, Tinonee; James, Alexandra; Waling, Andrea (5 July 2024). \"Smart\nSex Toys: A Narrative Review of Recent Research on Cultural, Health and Safety\nConsiderations\" (https://doi.org/10.1007%2Fs11930-024-00392-3). Current Sexual Health\nReports. 16 (3): 199\u2013215. doi:10.1007/s11930-024-00392-3 (https://doi.org/10.1007%2Fs11\n930-024-00392-3). ISSN 1548-3592 (https://search.worldcat.org/issn/1548-3592).\n189. Marcantonio, Tiffany L.; Avery, Gracie; Thrash, Anna; Leone, Ruschelle M. (10 September\n2024). \"Large Language Models in an App: Conducting a Qualitative Synthetic Data\nAnalysis of How Snapchat's \"My AI\" Responds to Questions About Sexual Consent, Sexual\nRefusals, Sexual Assault, and Sexting\" (https://www.tandfonline.com/doi/full/10.1080/00224\n499.2024.2396457). The Journal of Sex Research: 1\u201315.\ndoi:10.1080/00224499.2024.2396457 (https://doi.org/10.1080%2F00224499.2024.239645\n7). ISSN 0022-4499 (https://search.worldcat.org/issn/0022-4499). PMC 11891083.\nPMID 39254628 (https://pubmed.ncbi.nlm.nih.gov/39254628). Archived (https://web.archive.\norg/web/20241209185843/https://www.tandfonline.com/doi/full/10.1080/00224499.2024.239\n6457) from the original on 9 December 2024. Retrieved 9 December 2024.\n190. Hanson, Kenneth R.; Bolthouse, Hannah (2024). \"\"Replika Removing Erotic Role-Play Is\nLike Grand Theft Auto Removing Guns or Cars\": Reddit Discourse on Artificial Intelligence\nChatbots and Sexual Technologies\" (https://doi.org/10.1177%2F23780231241259627).\nSocius: Sociological Research for a Dynamic World. 10. doi:10.1177/23780231241259627\n(https://doi.org/10.1177%2F23780231241259627). ISSN 2378-0231 (https://search.worldca\nt.org/issn/2378-0231).\n191. Mania, Karolina (1 January 2024). \"Legal Protection of Revenge and Deepfake Porn Victims\nin the European Union: Findings From a Comparative Legal Study\" (https://journals.sagepu\nb.com/doi/abs/10.1177/15248380221143772?journalCode=tvaa). Trauma, Violence, &\nAbuse. 25 (1): 117\u2013129. doi:10.1177/15248380221143772 (https://doi.org/10.1177%2F1524\n8380221143772). ISSN 1524-8380 (https://search.worldcat.org/issn/1524-8380).\nPMID 36565267 (https://pubmed.ncbi.nlm.nih.gov/36565267).\n192. Singh, Suyesha; Nambiar, Vaishnavi (2024). \"Role of Artificial Intelligence in the Prevention\nof Online Child Sexual Abuse: A Systematic Review of Literature\" (https://www.tandfonline.c\nom/doi/full/10.1080/19361610.2024.2331885). Journal of Applied Security Research. 19 (4):\n586\u2013627. doi:10.1080/19361610.2024.2331885 (https://doi.org/10.1080%2F19361610.202\n4.2331885). ISSN 1936-1610 (https://search.worldcat.org/issn/1936-1610). Archived (https://\nweb.archive.org/web/20241209171923/https://www.tandfonline.com/doi/full/10.1080/193616\n10.2024.2331885) from the original on 9 December 2024. Retrieved 9 December 2024.\n193. Razi, Afsaneh; Kim, Seunghyun; Alsoubai, Ashwaq; Stringhini, Gianluca; Solorio, Thamar;\nDe Choudhury, Munmun; Wisniewski, Pamela J. (13 October 2021). \"A Human-Centered\nSystematic Literature Review of the Computational Approaches for Online Sexual Risk\nDetection\" (https://dl.acm.org/doi/10.1145/3479609). Proceedings of the ACM on Human-\nComputer Interaction. 5 (CSCW2): 1\u201338. doi:10.1145/3479609 (https://doi.org/10.1145%2F\n3479609). ISSN 2573-0142 (https://search.worldcat.org/issn/2573-0142). Archived (https://w\neb.archive.org/web/20241209171735/https://dl.acm.org/doi/10.1145/3479609) from the\noriginal on 9 December 2024. Retrieved 9 December 2024.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:43>\n",
    "text": "194. Ransbotham, Sam; Kiron, David; Gerbert, Philipp; Reeves, Martin (6 September 2017).\n\"Reshaping Business With Artificial Intelligence\" (https://sloanreview.mit.edu/projects/reshap\ning-business-with-artificial-intelligence). MIT Sloan Management Review. Archived (https://w\neb.archive.org/web/20240213070751/https://sloanreview.mit.edu/projects/reshaping-busines\ns-with-artificial-intelligence) from the original on 13 February 2024.\n195. Sun, Yuran; Zhao, Xilei; Lovreglio, Ruggiero; Kuligowski, Erica (1 January 2024), Naser, M.\nZ. (ed.), \"8 \u2013 AI for large-scale evacuation modeling: promises and challenges\" (https://www.\nsciencedirect.com/science/article/pii/B9780128240731000149), Interpretable Machine\nLearning for the Analysis, Design, Assessment, and Informed Decision Making for Civil\nInfrastructure, Woodhead Publishing Series in Civil and Structural Engineering, Woodhead\nPublishing, pp. 185\u2013204, ISBN 978-0-1282-4073-1, archived (https://web.archive.org/web/2\n0240519121547/https://www.sciencedirect.com/science/article/abs/pii/B9780128240731000\n149) from the original on 19 May 2024, retrieved 28 June 2024.\n196. Gomaa, Islam; Adelzadeh, Masoud; Gwynne, Steven; Spencer, Bruce; Ko, Yoon; B\u00e9nichou,\nNoureddine; Ma, Chunyun; Elsagan, Nour; Duong, Dana; Zalok, Ehab; Kinateder, Max (1\nNovember 2021). \"A Framework for Intelligent Fire Detection and Evacuation System\" (http\ns://doi.org/10.1007/s10694-021-01157-3). Fire Technology. 57 (6): 3179\u20133185.\ndoi:10.1007/s10694-021-01157-3 (https://doi.org/10.1007%2Fs10694-021-01157-3).\nISSN 1572-8099 (https://search.worldcat.org/issn/1572-8099). Archived (https://web.archive.\norg/web/20241005165650/https://link.springer.com/article/10.1007/s10694-021-01157-3)\nfrom the original on 5 October 2024. Retrieved 5 October 2024.\n197. Zhao, Xilei; Lovreglio, Ruggiero; Nilsson, Daniel (1 May 2020). \"Modelling and interpreting\npre-evacuation decision-making using machine learning\" (https://www.sciencedirect.com/sci\nence/article/pii/S0926580519313184). Automation in Construction. 113: 103140.\ndoi:10.1016/j.autcon.2020.103140 (https://doi.org/10.1016%2Fj.autcon.2020.103140).\nhdl:10179/17315 (https://hdl.handle.net/10179%2F17315). ISSN 0926-5805 (https://search.\nworldcat.org/issn/0926-5805). Archived (https://web.archive.org/web/20240519121548/http\ns://www.sciencedirect.com/science/article/abs/pii/S0926580519313184) from the original on\n19 May 2024. Retrieved 5 October 2024.\n198. \"India's latest election embraced AI technology. Here are some ways it was used\nconstructively\" (https://www.pbs.org/newshour/world/indias-latest-election-embraced-ai-tech\nnology-here-are-some-ways-it-was-used-constructively). PBS News. 12 June 2024.\nArchived (https://web.archive.org/web/20240917194950/https://www.pbs.org/newshour/worl\nd/indias-latest-election-embraced-ai-technology-here-are-some-ways-it-was-used-constructi\nvely) from the original on 17 September 2024. Retrieved 28 October 2024.\n199. M\u00fcller, Vincent C. (30 April 2020). \"Ethics of Artificial Intelligence and Robotics\" (https://plat\no.stanford.edu/archives/fall2023/entries/ethics-ai/). Stanford Encyclopedia of Philosophy\nArchive. Archived (https://web.archive.org/web/20241005165650/https://plato.stanford.edu/a\nrchives/fall2023/entries/ethics-ai/) from the original on 5 October 2024. Retrieved 5 October\n2024.\n200. Simonite (2016).\n201. Russell & Norvig (2021), p. 987.\n202. Laskowski (2023).\n203. GAO (2022).\n204. Valinsky (2019).\n205. Russell & Norvig (2021), p. 991.\n206. Russell & Norvig (2021), pp. 991\u2013992.\n207. Christian (2020), p. 63.\n208. Vincent (2022).\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:44>\n",
    "text": "209. Kopel, Matthew. \"Copyright Services: Fair Use\" (https://guides.library.cornell.edu/copyright/f\nair-use). Cornell University Library. Archived (https://web.archive.org/web/20240926194057/\nhttps://guides.library.cornell.edu/copyright/fair-use) from the original on 26 September 2024.\nRetrieved 26 April 2024.\n210. Burgess, Matt. \"How to Stop Your Data From Being Used to Train AI\" (https://www.wired.co\nm/story/how-to-stop-your-data-from-being-used-to-train-ai). Wired. ISSN 1059-1028 (https://\nsearch.worldcat.org/issn/1059-1028). Archived (https://web.archive.org/web/202410031801\n00/https://www.wired.com/story/how-to-stop-your-data-from-being-used-to-train-ai/) from the\noriginal on 3 October 2024. Retrieved 26 April 2024.\n211. Reisner (2023).\n212. Alter & Harris (2023).\n213. \"Getting the Innovation Ecosystem Ready for AI. An IP policy toolkit\" (https://www.wipo.int/e\ndocs/pubdocs/en/wipo-pub-2003-en-getting-the-innovation-ecosystem-ready-for-ai.pdf)\n(PDF). WIPO.\n214. Hammond, George (27 December 2023). \"Big Tech is spending more than VC firms on AI\nstartups\" (https://arstechnica.com/ai/2023/12/big-tech-is-spending-more-than-vc-firms-on-ai-\nstartups). Ars Technica. Archived (https://web.archive.org/web/20240110195706/https://arst\nechnica.com/ai/2023/12/big-tech-is-spending-more-than-vc-firms-on-ai-startups) from the\noriginal on 10 January 2024.\n215. Wong, Matteo (24 October 2023). \"The Future of AI Is GOMA\" (https://www.theatlantic.com/t\nechnology/archive/2023/10/big-ai-silicon-valley-dominance/675752). The Atlantic. Archived\n(https://web.archive.org/web/20240105020744/https://www.theatlantic.com/technology/archi\nve/2023/10/big-ai-silicon-valley-dominance/675752) from the original on 5 January 2024.\n216. \"Big tech and the pursuit of AI dominance\" (https://www.economist.com/business/2023/03/2\n6/big-tech-and-the-pursuit-of-ai-dominance). The Economist. 26 March 2023. Archived (http\ns://web.archive.org/web/20231229021351/https://www.economist.com/business/2023/03/26/\nbig-tech-and-the-pursuit-of-ai-dominance) from the original on 29 December 2023.\n217. Fung, Brian (19 December 2023). \"Where the battle to dominate AI may be won\" (https://ww\nw.cnn.com/2023/12/19/tech/cloud-competition-and-ai/index.html). CNN Business. Archived\n(https://web.archive.org/web/20240113053332/https://www.cnn.com/2023/12/19/tech/cloud-\ncompetition-and-ai/index.html) from the original on 13 January 2024.\n218. Metz, Cade (5 July 2023). \"In the Age of A.I., Tech's Little Guys Need Big Friends\" (https://w\nww.nytimes.com/2023/07/05/business/artificial-intelligence-power-data-centers.html). The\nNew York Times. Archived (https://web.archive.org/web/20240708214644/https://www.nytim\nes.com/2023/07/05/business/artificial-intelligence-power-data-centers.html) from the original\non 8 July 2024. Retrieved 5 October 2024.\n219. \"Electricity 2024 \u2013 Analysis\" (https://www.iea.org/reports/electricity-2024). IEA. 24 January\n2024. Retrieved 13 July 2024.\n220. Calvert, Brian (28 March 2024). \"AI already uses as much energy as a small country. It's\nonly the beginning\" (https://www.vox.com/climate/2024/3/28/24111721/ai-uses-a-lot-of-ener\ngy-experts-expect-it-to-double-in-just-a-few-years). Vox. New York, New York. Archived (http\ns://web.archive.org/web/20240703080555/https://www.vox.com/climate/2024/3/28/2411172\n1/ai-uses-a-lot-of-energy-experts-expect-it-to-double-in-just-a-few-years) from the original\non 3 July 2024. Retrieved 5 October 2024.\n221. Halper, Evan; O'Donovan, Caroline (21 June 2024). \"AI is exhausting the power grid. Tech\nfirms are seeking a miracle solution\" (https://www.washingtonpost.com/business/2024/06/2\n1/artificial-intelligence-nuclear-fusion-climate/?utm_campaign=wp_post_most&utm_medium\n=email&utm_source=newsletter&wpisrc=nl_most&carta-url=https%3A%2F%2Fs2.washingto\nnpost.com%2Fcar-ln-tr%2F3e0d678%2F6675a2d2c2c05472dd9ec0f4%2F596c09009bbc0f\n20865036e7%2F12%2F52%2F6675a2d2c2c05472dd9ec0f4). Washington Post.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:45>\n",
    "text": "222. Davenport, Carly. \"AI Data Centers and the Coming YS Power Demand Surge\" (https://web.\narchive.org/web/20240726080428/https://www.goldmansachs.com/intelligence/pages/gs-res\nearch/generational-growth-ai-data-centers-and-the-coming-us-power-surge/report.pdf)\n(PDF). Goldman Sachs. Archived from the original (https://www.goldmansachs.com/intellige\nnce/pages/gs-research/generational-growth-ai-data-centers-and-the-coming-us-power-surg\ne/report.pdf) (PDF) on 26 July 2024. Retrieved 5 October 2024.\n223. Ryan, Carol (12 April 2024). \"Energy-Guzzling AI Is Also the Future of Energy Savings\" (http\ns://www.wsj.com/business/energy-oil/ai-data-centers-energy-savings-d602296e). Wall Street\nJournal. Dow Jones.\n224. Hiller, Jennifer (1 July 2024). \"Tech Industry Wants to Lock Up Nuclear Power for AI\" (https://\nwww.wsj.com/business/energy-oil/tech-industry-wants-to-lock-up-nuclear-power-for-ai-6cb7\n5316?mod=djem10point). Wall Street Journal. Dow Jones. Archived (https://web.archive.or\ng/web/20241005165650/https://www.wsj.com/business/energy-oil/tech-industry-wants-to-loc\nk-up-nuclear-power-for-ai-6cb75316?mod=djem10point) from the original on 5 October\n2024. Retrieved 5 October 2024.\n225. Kendall, Tyler (28 September 2024). \"Nvidia's Huang Says Nuclear Power an Option to\nFeed Data Centers\" (https://www.bloomberg.com/news/articles/2024-09-27/nvidia-s-huang-s\nays-nuclear-power-an-option-to-feed-data-centers). Bloomberg.\n226. Halper, Evan (20 September 2024). \"Microsoft deal would reopen Three Mile Island nuclear\nplant to power AI\" (https://www.washingtonpost.com/business/2024/09/20/microsoft-three-mi\nle-island-nuclear-constellation). Washington Post.\n227. Hiller, Jennifer (20 September 2024). \"Three Mile Island's Nuclear Plant to Reopen, Help\nPower Microsoft's AI Centers\" (https://www.wsj.com/business/energy-oil/three-mile-islands-n\nuclear-plant-to-reopen-help-power-microsofts-ai-centers-aebfb3c8?mod=Searchresults_pos\n1&page=1). Wall Street Journal. Dow Jones. Archived (https://web.archive.org/web/2024100\n5170152/https://www.wsj.com/business/energy-oil/three-mile-islands-nuclear-plant-to-reope\nn-help-power-microsofts-ai-centers-aebfb3c8?mod=Searchresults_pos1&page=1) from the\noriginal on 5 October 2024. Retrieved 5 October 2024.\n228. Niva Yadav (19 August 2024). \"Taiwan to stop large data centers in the North, cites\ninsufficient power\" (https://www.datacenterdynamics.com/en/news/taiwan-to-stop-large-data\n-centers-in-the-north-cites-insufficient-power/). DatacenterDynamics. Archived (https://web.a\nrchive.org/web/20241108213650/https://www.datacenterdynamics.com/en/news/taiwan-to-st\nop-large-data-centers-in-the-north-cites-insufficient-power/) from the original on 8 November\n2024. Retrieved 7 November 2024.\n229. Mochizuki, Takashi; Oda, Shoko (18 October 2024). \"\u30a8\u30cc\u30d3\u30c7\u30a3\u30a2\u51fa\u8cc7\u306e\u2f47\u672c\u4f01\u696d\u3001\u539f\u767a\u8fd1\n\u304f\u3067\uff21\uff29\u30c7\u30fc\u30bf\u30bb\u30f3\u30bf\u30fc\u65b0\u8a2d\u691c\u8a0e\" (https://www.bloomberg.co.jp/news/articles/2024-10-18/S\nLHGKKT0AFB400). Bloomberg (in Japanese). Archived (https://web.archive.org/web/20241\n108213843/https://www.bloomberg.co.jp/news/articles/2024-10-18/SLHGKKT0AFB400)\nfrom the original on 8 November 2024. Retrieved 7 November 2024.\n230. Naureen S Malik and Will Wade (5 November 2024). \"Nuclear-Hungry AI Campuses Need\nNew Plan to Find Power Fast\" (https://www.bloomberg.com/news/articles/2024-11-04/nucle\nar-hungry-ai-campuses-need-new-strategy-to-find-power-fast). Bloomberg.\n231. \"Energy and AI Executive summary\" (https://www.iea.org/reports/energy-and-ai/executive-su\nmmary). International Energy Agency. Retrieved 10 April 2025.\n232. Nicas (2018).\n233. Rainie, Lee; Keeter, Scott; Perrin, Andrew (22 July 2019). \"Trust and Distrust in America\" (ht\ntps://www.pewresearch.org/politics/2019/07/22/trust-and-distrust-in-america). Pew Research\nCenter. Archived (https://web.archive.org/web/20240222000601/https://www.pewresearch.or\ng/politics/2019/07/22/trust-and-distrust-in-america) from the original on 22 February 2024.\n234. Kosoff, Maya (8 February 2018). \"YouTube Struggles to Contain Its Conspiracy Problem\" (ht\ntps://www.vanityfair.com/news/2018/02/youtube-conspiracy-problem). Vanity Fair. Retrieved\n10 April 2025.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:46>\n",
    "text": "235. Williams (2023).\n236. Olanipekun, Samson Olufemi (2025). \"Computational propaganda and misinformation: AI\ntechnologies as tools of media manipulation\" (https://journalwjarr.com/node/366). World\nJournal of Advanced Research and Reviews. 25 (1): 911\u2013923.\ndoi:10.30574/wjarr.2025.25.1.0131 (https://doi.org/10.30574%2Fwjarr.2025.25.1.0131).\nISSN 2581-9615 (https://search.worldcat.org/issn/2581-9615).\n237. Taylor & Hern (2023).\n238. Samuel, Sigal (19 April 2022). \"Why it's so damn hard to make AI fair and unbiased\" (https://\nwww.vox.com/future-perfect/22916602/ai-bias-fairness-tradeoffs-artificial-intelligence). Vox.\nArchived (https://web.archive.org/web/20241005170153/https://www.vox.com/future-perfect/\n22916602/ai-bias-fairness-tradeoffs-artificial-intelligence) from the original on 5 October\n2024. Retrieved 24 July 2024.\n239. Rose (2023).\n240. CNA (2019).\n241. Goffrey (2008), p. 17.\n242. Berdahl et al. (2023); Goffrey (2008, p. 17); Rose (2023); Russell & Norvig (2021, p. 995)\n243. Christian (2020), p. 25.\n244. Russell & Norvig (2021), p. 995.\n245. Grant & Hill (2023).\n246. Larson & Angwin (2016).\n247. Christian (2020), p. 67\u201370.\n248. Christian (2020, pp. 67\u201370); Russell & Norvig (2021, pp. 993\u2013994)\n249. Russell & Norvig (2021, p. 995); Lipartito (2011, p. 36); Goodman & Flaxman (2017, p. 6);\nChristian (2020, pp. 39\u201340, 65)\n250. Quoted in Christian (2020, p. 65).\n251. Russell & Norvig (2021, p. 994); Christian (2020, pp. 40, 80\u201381)\n252. Quoted in Christian (2020, p. 80)\n253. Dockrill (2022).\n254. Sample (2017).\n255. \"Black Box AI\" (https://www.techopedia.com/definition/34940/black-box-ai). 16 June 2023.\nArchived (https://web.archive.org/web/20240615100800/https://www.techopedia.com/definiti\non/34940/black-box-ai) from the original on 15 June 2024. Retrieved 5 October 2024.\n256. Christian (2020), p. 110.\n257. Christian (2020), pp. 88\u201391.\n258. Christian (2020, p. 83); Russell & Norvig (2021, p. 997)\n259. Christian (2020), p. 91.\n260. Christian (2020), p. 83.\n261. Verma (2021).\n262. Rothman (2020).\n263. Christian (2020), pp. 105\u2013108.\n264. Christian (2020), pp. 108\u2013112.\n265. Ropek, Lucas (21 May 2024). \"New Anthropic Research Sheds Light on AI's 'Black Box'\" (ht\ntps://gizmodo.com/new-anthropic-research-sheds-light-on-ais-black-box-1851491333).\nGizmodo. Archived (https://web.archive.org/web/20241005170309/https://gizmodo.com/new\n-anthropic-research-sheds-light-on-ais-black-box-1851491333) from the original on 5\nOctober 2024. Retrieved 23 May 2024.\n266. Russell & Norvig (2021), p. 989.\n267. Russell & Norvig (2021), pp. 987\u2013990.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:47>\n",
    "text": "268. Russell & Norvig (2021), p. 988.\n269. Robitzski (2018); Sainato (2015)\n270. Harari (2018).\n271. Buckley, Chris; Mozur, Paul (22 May 2019). \"How China Uses High-Tech Surveillance to\nSubdue Minorities\" (https://www.nytimes.com/2019/05/22/world/asia/china-surveillance-xinji\nang.html). The New York Times. Archived (https://web.archive.org/web/20191125180459/htt\nps://www.nytimes.com/2019/05/22/world/asia/china-surveillance-xinjiang.html) from the\noriginal on 25 November 2019. Retrieved 2 July 2019.\n272. \"Security lapse exposed a Chinese smart city surveillance system\" (https://techcrunch.com/\n2019/05/03/china-smart-city-exposed). 3 May 2019. Archived (https://web.archive.org/web/2\n0210307203740/https://consent.yahoo.com/v2/collectConsent?sessionId=3_cc-session_c85\n62b93-9863-4915-8523-6c7b930a3efc) from the original on 7 March 2021. Retrieved\n14 September 2020.\n273. Urbina et al. (2022).\n274. E. McGaughey, 'Will Robots Automate Your Job Away? Full Employment, Basic Income, and\nEconomic Democracy' (2022), 51(3) Industrial Law Journal 511\u2013559 (https://academic.oup.c\nom/ilj/article/51/3/511/6321008). Archived (https://web.archive.org/web/20230527163045/htt\nps://academic.oup.com/ilj/article/51/3/511/6321008) 27 May 2023 at the Wayback Machine.\n275. Ford & Colvin (2015);McGaughey (2022)\n276. IGM Chicago (2017).\n277. Arntz, Gregory & Zierahn (2016), p. 33.\n278. Lohr (2017); Frey & Osborne (2017); Arntz, Gregory & Zierahn (2016, p. 33)\n279. Zhou, Viola (11 April 2023). \"AI is already taking video game illustrators' jobs in China\" (http\ns://restofworld.org/2023/ai-image-china-video-game-layoffs). Rest of World. Archived (http\ns://web.archive.org/web/20240221131748/https://restofworld.org/2023/ai-image-china-video\n-game-layoffs/) from the original on 21 February 2024. Retrieved 17 August 2023.\n280. Carter, Justin (11 April 2023). \"China's game art industry reportedly decimated by growing AI\nuse\" (https://www.gamedeveloper.com/art/china-s-game-art-industry-reportedly-decimated-a\ni-art-use). Game Developer. Archived (https://web.archive.org/web/20230817010519/https://\nwww.gamedeveloper.com/art/china-s-game-art-industry-reportedly-decimated-ai-art-use)\nfrom the original on 17 August 2023. Retrieved 17 August 2023.\n281. Morgenstern (2015).\n282. Mahdawi (2017); Thompson (2014)\n283. Tarnoff, Ben (4 August 2023). \"Lessons from Eliza\". The Guardian Weekly. pp. 34\u201339.\n284. Cellan-Jones (2014).\n285. Russell & Norvig 2021, p. 1001.\n286. Bostrom (2014).\n287. Russell (2019).\n288. Bostrom (2014); M\u00fcller & Bostrom (2014); Bostrom (2015).\n289. Harari (2023).\n290. M\u00fcller & Bostrom (2014).\n291. Leaders' concerns about the existential risks of AI around 2015: Rawlinson (2015), Holley\n(2015), Gibbs (2014), Sainato (2015)\n292. \"\"Godfather of artificial intelligence\" talks impact and potential of new AI\" (https://www.cbsne\nws.com/video/godfather-of-artificial-intelligence-talks-impact-and-potential-of-new-ai). CBS\nNews. 25 March 2023. Archived (https://web.archive.org/web/20230328225221/https://www.\ncbsnews.com/video/godfather-of-artificial-intelligence-talks-impact-and-potential-of-new-ai)\nfrom the original on 28 March 2023. Retrieved 28 March 2023.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:48>\n",
    "text": "293. Pittis, Don (4 May 2023). \"Canadian artificial intelligence leader Geoffrey Hinton piles on\nfears of computer takeover\" (https://www.cbc.ca/news/business/ai-doom-column-don-pittis-\n1.6829302). CBC. Archived (https://web.archive.org/web/20240707032135/https://www.cbc.\nca/news/business/ai-doom-column-don-pittis-1.6829302) from the original on 7 July 2024.\nRetrieved 5 October 2024.\n294. \"'50\u201350 chance' that AI outsmarts humanity, Geoffrey Hinton says\" (https://www.bnnbloomb\nerg.ca/50-50-chance-that-ai-outsmarts-humanity-geoffrey-hinton-says-1.2085394).\nBloomberg BNN. 14 June 2024. Archived (https://web.archive.org/web/20240614144506/htt\nps://www.bnnbloomberg.ca/50-50-chance-that-ai-outsmarts-humanity-geoffrey-hinton-says-\n1.2085394) from the original on 14 June 2024. Retrieved 6 July 2024.\n295. Valance (2023).\n296. Taylor, Josh (7 May 2023). \"Rise of artificial intelligence is inevitable but should not be\nfeared, 'father of AI' says\" (https://www.theguardian.com/technology/2023/may/07/rise-of-arti\nficial-intelligence-is-inevitable-but-should-not-be-feared-father-of-ai-says). The Guardian.\nArchived (https://web.archive.org/web/20231023061228/https://www.theguardian.com/techn\nology/2023/may/07/rise-of-artificial-intelligence-is-inevitable-but-should-not-be-feared-father-\nof-ai-says) from the original on 23 October 2023. Retrieved 26 May 2023.\n297. Colton, Emma (7 May 2023). \"'Father of AI' says tech fears misplaced: 'You cannot stop it'\"\n(https://www.foxnews.com/tech/father-ai-jurgen-schmidhuber-says-tech-fears-misplaced-can\nnot-stop). Fox News. Archived (https://web.archive.org/web/20230526162642/https://www.fo\nxnews.com/tech/father-ai-jurgen-schmidhuber-says-tech-fears-misplaced-cannot-stop) from\nthe original on 26 May 2023. Retrieved 26 May 2023.\n298. Jones, Hessie (23 May 2023). \"Juergen Schmidhuber, Renowned 'Father Of Modern AI,'\nSays His Life's Work Won't Lead To Dystopia\" (https://www.forbes.com/sites/hessiejones/20\n23/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead-\nto-dystopia). Forbes. Archived (https://web.archive.org/web/20230526163102/https://www.fo\nrbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai\n-says-his-lifes-work-wont-lead-to-dystopia/) from the original on 26 May 2023. Retrieved\n26 May 2023.\n299. McMorrow, Ryan (19 December 2023). \"Andrew Ng: 'Do we think the world is better off with\nmore or less intelligence?'\" (https://www.ft.com/content/2dc07f9e-d2a9-4d98-b746-b051f93\n52be3). Financial Times. Archived (https://web.archive.org/web/20240125014121/https://ww\nw.ft.com/content/2dc07f9e-d2a9-4d98-b746-b051f9352be3) from the original on 25 January\n2024. Retrieved 30 December 2023.\n300. Levy, Steven (22 December 2023). \"How Not to Be Stupid About AI, With Yann LeCun\" (http\ns://www.wired.com/story/artificial-intelligence-meta-yann-lecun-interview). Wired. Archived (h\nttps://web.archive.org/web/20231228152443/https://www.wired.com/story/artificial-intelligenc\ne-meta-yann-lecun-interview/) from the original on 28 December 2023. Retrieved\n30 December 2023.\n301. Arguments that AI is not an imminent risk: Brooks (2014), Geist (2015), Madrigal (2015),\nLee (2014)\n302. Christian (2020), pp. 67, 73.\n303. Yudkowsky (2008).\n304. Anderson & Anderson (2011).\n305. AAAI (2014).\n306. Wallach (2010).\n307. Russell (2019), p. 173.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:49>\n",
    "text": "308. Stewart, Ashley; Melton, Monica. \"Hugging Face CEO says he's focused on building a\n'sustainable model' for the $4.5 billion open-source-AI startup\" (https://www.businessinsider.\ncom/hugging-face-open-source-ai-approach-2023-12). Business Insider. Archived (https://w\neb.archive.org/web/20240925013220/https://www.businessinsider.com/hugging-face-open-s\nource-ai-approach-2023-12) from the original on 25 September 2024. Retrieved 14 April\n2024.\n309. Wiggers, Kyle (9 April 2024). \"Google open sources tools to support AI model development\"\n(https://techcrunch.com/2024/04/09/google-open-sources-tools-to-support-ai-model-develop\nment). TechCrunch. Archived (https://web.archive.org/web/20240910112401/https://techcrun\nch.com/2024/04/09/google-open-sources-tools-to-support-ai-model-development/) from the\noriginal on 10 September 2024. Retrieved 14 April 2024.\n310. Heaven, Will Douglas (12 May 2023). \"The open-source AI boom is built on Big Tech's\nhandouts. How long will it last?\" (https://www.technologyreview.com/2023/05/12/1072950/op\nen-source-ai-google-openai-eleuther-meta). MIT Technology Review. Retrieved 14 April\n2024.\n311. Brodsky, Sascha (19 December 2023). \"Mistral AI's New Language Model Aims for Open\nSource Supremacy\" (https://aibusiness.com/nlp/mistral-ai-s-new-language-model-aims-for-o\npen-source-supremacy). AI Business. Archived (https://web.archive.org/web/202409052126\n07/https://aibusiness.com/nlp/mistral-ai-s-new-language-model-aims-for-open-source-supre\nmacy) from the original on 5 September 2024. Retrieved 5 October 2024.\n312. Edwards, Benj (22 February 2024). \"Stability announces Stable Diffusion 3, a next-gen AI\nimage generator\" (https://arstechnica.com/information-technology/2024/02/stability-announc\nes-stable-diffusion-3-a-next-gen-ai-image-generator). Ars Technica. Archived (https://web.ar\nchive.org/web/20241005170201/https://arstechnica.com/information-technology/2024/02/sta\nbility-announces-stable-diffusion-3-a-next-gen-ai-image-generator/) from the original on 5\nOctober 2024. Retrieved 14 April 2024.\n313. Marshall, Matt (29 January 2024). \"How enterprises are using open source LLMs: 16\nexamples\" (https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-exa\nmples). VentureBeat. Archived (https://web.archive.org/web/20240926171131/https://ventur\nebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples/) from the original\non 26 September 2024. Retrieved 5 October 2024.\n314. Piper, Kelsey (2 February 2024). \"Should we make our most powerful AI models open\nsource to all?\" (https://www.vox.com/future-perfect/2024/2/2/24058484/open-source-artificial\n-intelligence-ai-risk-meta-llama-2-chatgpt-openai-deepfake). Vox. Archived (https://web.archi\nve.org/web/20241005170204/https://www.vox.com/future-perfect/2024/2/2/24058484/open-s\nource-artificial-intelligence-ai-risk-meta-llama-2-chatgpt-openai-deepfake) from the original\non 5 October 2024. Retrieved 14 April 2024.\n315. Alan Turing Institute (2019). \"Understanding artificial intelligence ethics and safety\" (https://\nwww.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and\n_safety.pdf) (PDF). Archived (https://web.archive.org/web/20240911131935/https://www.turi\nng.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.\npdf) (PDF) from the original on 11 September 2024. Retrieved 5 October 2024.\n316. Alan Turing Institute (2023). \"AI Ethics and Governance in Practice\" (https://www.turing.ac.u\nk/sites/default/files/2023-12/aieg-ati-ai-ethics-an-intro_1.pdf) (PDF). Archived (https://web.ar\nchive.org/web/20240911125504/https://www.turing.ac.uk/sites/default/files/2023-12/aieg-ati-\nai-ethics-an-intro_1.pdf) (PDF) from the original on 11 September 2024. Retrieved\n5 October 2024.\n317. Floridi, Luciano; Cowls, Josh (23 June 2019). \"A Unified Framework of Five Principles for AI\nin Society\" (https://hdsr.mitpress.mit.edu/pub/l0jsh9d1). Harvard Data Science Review. 1\n(1). doi:10.1162/99608f92.8cd550d1 (https://doi.org/10.1162%2F99608f92.8cd550d1).\nS2CID 198775713 (https://api.semanticscholar.org/CorpusID:198775713). Archived (https://\narchive.today/20190807202909/https://hdsr.mitpress.mit.edu/pub/l0jsh9d1) from the original\non 7 August 2019. Retrieved 5 December 2023.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:50>\n",
    "text": "318. Buruk, Banu; Ekmekci, Perihan Elif; Arda, Berna (1 September 2020). \"A critical perspective\non guidelines for responsible and trustworthy artificial intelligence\" (https://doi.org/10.1007/s\n11019-020-09948-1). Medicine, Health Care and Philosophy. 23 (3): 387\u2013399.\ndoi:10.1007/s11019-020-09948-1 (https://doi.org/10.1007%2Fs11019-020-09948-1).\nISSN 1572-8633 (https://search.worldcat.org/issn/1572-8633). PMID 32236794 (https://pub\nmed.ncbi.nlm.nih.gov/32236794). S2CID 214766800 (https://api.semanticscholar.org/Corpu\nsID:214766800). Archived (https://web.archive.org/web/20241005170206/https://link.springe\nr.com/article/10.1007/s11019-020-09948-1) from the original on 5 October 2024. Retrieved\n5 October 2024.\n319. Kamila, Manoj Kumar; Jasrotia, Sahil Singh (1 January 2023). \"Ethical issues in the\ndevelopment of artificial intelligence: recognizing the risks\" (https://doi.org/10.1108/IJOES-0\n5-2023-0107). International Journal of Ethics and Systems. 41 (ahead-of-print): 45\u201363.\ndoi:10.1108/IJOES-05-2023-0107 (https://doi.org/10.1108%2FIJOES-05-2023-0107).\nISSN 2514-9369 (https://search.worldcat.org/issn/2514-9369). S2CID 259614124 (https://ap\ni.semanticscholar.org/CorpusID:259614124). Archived (https://web.archive.org/web/202410\n05170207/https://www.emerald.com/insight/content/doi/10.1108/IJOES-05-2023-0107/full/ht\nml) from the original on 5 October 2024. Retrieved 5 October 2024.\n320. \"AI Safety Institute releases new AI safety evaluations platform\" (https://www.gov.uk/govern\nment/news/ai-safety-institute-releases-new-ai-safety-evaluations-platform). UK Government.\n10 May 2024. Archived (https://web.archive.org/web/20241005170207/https://www.gov.uk/g\novernment/news/ai-safety-institute-releases-new-ai-safety-evaluations-platform) from the\noriginal on 5 October 2024. Retrieved 14 May 2024.\n321. Regulation of AI to mitigate risks: Berryhill et al. (2019), Barfield & Pagallo (2018), Iphofen &\nKritikos (2019), Wirtz, Weyerer & Geyer (2018), Buiten (2019)\n322. Law Library of Congress (U.S.). Global Legal Research Directorate (2019).\n323. Vincent (2023).\n324. Stanford University (2023).\n325. UNESCO (2021).\n326. Kissinger (2021).\n327. Altman, Brockman & Sutskever (2023).\n328. VOA News (25 October 2023). \"UN Announces Advisory Body on Artificial Intelligence\" (http\ns://www.voanews.com/a/un-announces-advisory-body-on-artificial-intelligence-/7328732.htm\nl). Archived (https://web.archive.org/web/20240918071530/https://www.voanews.com/a/un-a\nnnounces-advisory-body-on-artificial-intelligence-/7328732.html) from the original on 18\nSeptember 2024. Retrieved 5 October 2024.\n329. \"Council of Europe opens first ever global treaty on AI for signature\" (https://www.coe.int/en/\nweb/portal/-/council-of-europe-opens-first-ever-global-treaty-on-ai-for-signature). Council of\nEurope. 5 September 2024. Archived (https://web.archive.org/web/20240917001330/https://\nwww.coe.int/en/web/portal/-/council-of-europe-opens-first-ever-global-treaty-on-ai-for-signat\nure) from the original on 17 September 2024. Retrieved 17 September 2024.\n330. Edwards (2023).\n331. Kasperowicz (2023).\n332. Fox News (2023).\n333. Milmo, Dan (3 November 2023). \"Hope or Horror? The great AI debate dividing its pioneers\".\nThe Guardian Weekly. pp. 10\u201312.\n334. \"The Bletchley Declaration by Countries Attending the AI Safety Summit, 1\u20132 November\n2023\" (https://web.archive.org/web/20231101123904/https://www.gov.uk/government/public\nations/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countrie\ns-attending-the-ai-safety-summit-1-2-november-2023). GOV.UK. 1 November 2023.\nArchived from the original (https://www.gov.uk/government/publications/ai-safety-summit-20\n23-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-s\nummit-1-2-november-2023) on 1 November 2023. Retrieved 2 November 2023.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:51>\n",
    "text": "335. \"Countries agree to safe and responsible development of frontier AI in landmark Bletchley\nDeclaration\" (https://www.gov.uk/government/news/countries-agree-to-safe-and-responsible\n-development-of-frontier-ai-in-landmark-bletchley-declaration). GOV.UK (Press release).\nArchived (https://web.archive.org/web/20231101115016/https://www.gov.uk/government/ne\nws/countries-agree-to-safe-and-responsible-development-of-frontier-ai-in-landmark-bletchle\ny-declaration) from the original on 1 November 2023. Retrieved 1 November 2023.\n336. \"Second global AI summit secures safety commitments from companies\" (https://www.reuter\ns.com/technology/global-ai-summit-seoul-aims-forge-new-regulatory-agreements-2024-05-2\n1). Reuters. 21 May 2024. Retrieved 23 May 2024.\n337. \"Frontier AI Safety Commitments, AI Seoul Summit 2024\" (https://web.archive.org/web/2024\n0523201611/https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-\nseoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024). gov.uk. 21 May\n2024. Archived from the original (https://www.gov.uk/government/publications/frontier-ai-safe\nty-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-202\n4) on 23 May 2024. Retrieved 23 May 2024.\n338. Buntz, Brian (3 November 2024). \"Quality vs. quantity: US and China chart different paths in\nglobal AI patent race in 2024 / Geographical breakdown of AI patents in 2024\" (https://www.r\ndworldonline.com/quality-vs-quantity-us-and-china-chart-different-paths-in-global-ai-patent-r\nace-in-2024/). R&D World. Archived (https://web.archive.org/web/20241209072113/https://w\nww.rdworldonline.com/quality-vs-quantity-us-and-china-chart-different-paths-in-global-ai-pat\nent-race-in-2024/) from the original on 9 December 2024.\n339. Russell & Norvig 2021, p. 9.\n340. Copeland, J., ed. (2004). The Essential Turing: the ideas that gave birth to the computer\nage. Oxford, England: Clarendon Press. ISBN 0-1982-5079-7.\n341. \"Google books ngram\" (https://books.google.com/ngrams/graph?content=electronic+brain&\nyear_start=1930&year_end=2019&corpus=en-2019&smoothing=3). Archived (https://web.ar\nchive.org/web/20241005170209/https://books.google.com/ngrams/graph?content=electronic\n+brain&year_start=1930&year_end=2019&corpus=en-2019&smoothing=3) from the original\non 5 October 2024. Retrieved 5 October 2024.\n342. AI's immediate precursors: McCorduck (2004, pp. 51\u2013107), Crevier (1993, pp. 27\u201332),\nRussell & Norvig (2021, pp. 8\u201317), Moravec (1988, p. 3)\n343. Turing's original publication of the Turing test in \"Computing machinery and intelligence\":\nTuring (1950) Historical influence and philosophical implications: Haugeland (1985, pp. 6\u2013\n9), Crevier (1993, p. 24), McCorduck (2004, pp. 70\u201371), Russell & Norvig (2021, pp. 2, 984)\n344. Crevier (1993), pp. 47\u201349.\n345. Russell & Norvig (2003), p. 17.\n346. Russell & Norvig (2003), p. 18.\n347. Newquist (1994), pp. 86\u201386.\n348. Simon (1965, p. 96) quoted in Crevier (1993, p. 109)\n349. Minsky (1967, p. 2) quoted in Crevier (1993, p. 109)\n350. Russell & Norvig (2021), p. 21.\n351. Lighthill (1973).\n352. NRC 1999, pp. 212\u2013213.\n353. Russell & Norvig (2021), p. 22.\n354. Expert systems: Russell & Norvig (2021, pp. 23, 292), Luger & Stubblefield (2004, pp. 227\u2013\n331), Nilsson (1998, chpt. 17.4), McCorduck (2004, pp. 327\u2013335, 434\u2013435), Crevier (1993,\npp. 145\u2013162, 197\u2013203), Newquist (1994, pp. 155\u2013183)\n355. Russell & Norvig (2021), p. 24.\n356. Nilsson (1998), p. 7.\n357. McCorduck (2004), pp. 454\u2013462.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:52>\n",
    "text": "358. Moravec (1988).\n359. Brooks (1990).\n360. Developmental robotics: Weng et al. (2001), Lungarella et al. (2003), Asada et al. (2009),\nOudeyer (2010)\n361. Russell & Norvig (2021), p. 25.\n362. Crevier (1993, pp. 214\u2013215), Russell & Norvig (2021, pp. 24, 26)\n363. Russell & Norvig (2021), p. 26.\n364. Formal and narrow methods adopted in the 1990s: Russell & Norvig (2021, pp. 24\u201326),\nMcCorduck (2004, pp. 486\u2013487)\n365. AI widely used in the late 1990s: Kurzweil (2005, p. 265), NRC (1999, pp. 216\u2013222),\nNewquist (1994, pp. 189\u2013201)\n366. Wong (2023).\n367. Moore's Law and AI: Russell & Norvig (2021, pp. 14, 27)\n368. Clark (2015b).\n369. Big data: Russell & Norvig (2021, p. 26)\n370. Sagar, Ram (3 June 2020). \"OpenAI Releases GPT-3, The Largest Model So Far\" (https://a\nnalyticsindiamag.com/open-ai-gpt-3-language-model). Analytics India Magazine. Archived (h\nttps://web.archive.org/web/20200804173452/https://analyticsindiamag.com/open-ai-gpt-3-la\nnguage-model) from the original on 4 August 2020. Retrieved 15 March 2023.\n371. Milmo, Dan (2 February 2023). \"ChatGPT reaches 100 million users two months after\nlaunch\" (https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-op\nen-ai-fastest-growing-app). The Guardian. ISSN 0261-3077 (https://search.worldcat.org/iss\nn/0261-3077). Archived (https://web.archive.org/web/20230203051356/https://www.theguard\nian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app)\nfrom the original on 3 February 2023. Retrieved 31 December 2024.\n372. Gorichanaz, Tim (29 November 2023). \"ChatGPT turns 1: AI chatbot's success says as\nmuch about humans as technology\" (https://theconversation.com/chatgpt-turns-1-ai-chatbot\ns-success-says-as-much-about-humans-as-technology-218704). The Conversation.\nArchived (https://web.archive.org/web/20241231073513/https://theconversation.com/chatgpt\n-turns-1-ai-chatbots-success-says-as-much-about-humans-as-technology-218704) from the\noriginal on 31 December 2024. Retrieved 31 December 2024.\n373. DiFeliciantonio (2023).\n374. Goswami (2023).\n375. \"Nearly 1 in 4 new startups is an AI company\" (https://pitchbook.com/news/articles/nearly-1-i\nn-4-new-startups-is-an-ai-company). PitchBook. 24 December 2024. Retrieved 3 January\n2025.\n376. Grayling, Anthony; Ball, Brian (1 August 2024). \"Philosophy is crucial in the age of AI\" (http\ns://theconversation.com/philosophy-is-crucial-in-the-age-of-ai-235907). The Conversation.\nArchived (https://web.archive.org/web/20241005170243/https://theconversation.com/philoso\nphy-is-crucial-in-the-age-of-ai-235907) from the original on 5 October 2024. Retrieved\n4 October 2024.\n377. Jarow, Oshan (15 June 2024). \"Will AI ever become conscious? It depends on how you\nthink about biology\" (https://www.vox.com/future-perfect/351893/consciousness-ai-machines\n-neuroscience-mind). Vox. Archived (https://web.archive.org/web/20240921035218/https://w\nww.vox.com/future-perfect/351893/consciousness-ai-machines-neuroscience-mind) from\nthe original on 21 September 2024. Retrieved 4 October 2024.\n378. McCarthy, John. \"The Philosophy of AI and the AI of Philosophy\" (https://web.archive.org/we\nb/20181023181725/http://jmc.stanford.edu/articles/aiphil2.html). jmc.stanford.edu. Archived\nfrom the original (http://jmc.stanford.edu/articles/aiphil2.html) on 23 October 2018. Retrieved\n3 October 2024.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:53>\n",
    "text": "379. Turing (1950), p. 1.\n380. Turing (1950), Under \"The Argument from Consciousness\".\n381. Kirk-Giannini, Cameron Domenico; Goldstein, Simon (16 October 2023). \"AI is closer than\never to passing the Turing test for 'intelligence'. What happens when it does?\" (https://theco\nnversation.com/ai-is-closer-than-ever-to-passing-the-turing-test-for-intelligence-what-happe\nns-when-it-does-214721). The Conversation. Archived (https://web.archive.org/web/202409\n25040612/https://theconversation.com/ai-is-closer-than-ever-to-passing-the-turing-test-for-in\ntelligence-what-happens-when-it-does-214721) from the original on 25 September 2024.\nRetrieved 17 August 2024.\n382. Russell & Norvig (2021), p. 3.\n383. Maker (2006).\n384. McCarthy (1999).\n385. Minsky (1986).\n386. \"What Is Artificial Intelligence (AI)?\" (https://cloud.google.com/learn/what-is-artificial-intellige\nnce). Google Cloud Platform. Archived (https://web.archive.org/web/20230731114802/http\ns://cloud.google.com/learn/what-is-artificial-intelligence) from the original on 31 July 2023.\nRetrieved 16 October 2023.\n387. \"One of the Biggest Problems in Regulating AI Is Agreeing on a Definition\" (https://carnegiee\nndowment.org/posts/2022/10/one-of-the-biggest-problems-in-regulating-ai-is-agreeing-on-a-\ndefinition?lang=en). Carnegie Endowment for International Peace. Retrieved 31 July 2024.\n388. \"AI or BS? How to tell if a marketing tool really uses artificial intelligence\" (https://www.thedr\num.com/opinion/2023/03/30/ai-or-bs-how-tell-if-marketing-tool-really-uses-artificial-intelligen\nce). The Drum. Retrieved 31 July 2024.\n389. Nilsson (1983), p. 10.\n390. Haugeland (1985), pp. 112\u2013117.\n391. Physical symbol system hypothesis: Newell & Simon (1976, p. 116) Historical significance:\nMcCorduck (2004, p. 153), Russell & Norvig (2021, p. 19)\n392. Moravec's paradox: Moravec (1988, pp. 15\u201316), Minsky (1986, p. 29), Pinker (2007,\npp. 190\u2013191)\n393. Dreyfus' critique of AI: Dreyfus (1972), Dreyfus & Dreyfus (1986) Historical significance and\nphilosophical implications: Crevier (1993, pp. 120\u2013132), McCorduck (2004, pp. 211\u2013239),\nRussell & Norvig (2021, pp. 981\u2013982), Fearn (2007, chpt. 3)\n394. Crevier (1993), p. 125.\n395. Langley (2011).\n396. Katz (2012).\n397. Neats vs. scruffies, the historic debate: McCorduck (2004, pp. 421\u2013424, 486\u2013489), Crevier\n(1993, p. 168), Nilsson (1983, pp. 10\u201311), Russell & Norvig (2021, p. 24) A classic example\nof the \"scruffy\" approach to intelligence: Minsky (1986) A modern example of neat AI and its\naspirations in the 21st century: Domingos (2015)\n398. Pennachin & Goertzel (2007).\n399. Roberts (2016).\n400. Russell & Norvig (2021), p. 986.\n401. Chalmers (1995).\n402. Dennett (1991).\n403. Horst (2005).\n404. Searle (1999).\n405. Searle (1980), p. 1.\n406. Russell & Norvig (2021), p. 9817.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:54>\n",
    "text": "407. Searle's Chinese room argument: Searle (1980). Searle's original presentation of the\nthought experiment., Searle (1999). Discussion: Russell & Norvig (2021, pp. 985),\nMcCorduck (2004, pp. 443\u2013445), Crevier (1993, pp. 269\u2013271)\n408. Leith, Sam (7 July 2022). \"Nick Bostrom: How can we be certain a machine isn't\nconscious?\" (https://www.spectator.co.uk/article/nick-bostrom-how-can-we-be-certain-a-mac\nhine-isnt-conscious). The Spectator. Archived (https://web.archive.org/web/2024092615563\n9/https://www.spectator.co.uk/article/nick-bostrom-how-can-we-be-certain-a-machine-isnt-co\nnscious/) from the original on 26 September 2024. Retrieved 23 February 2024.\n409. Thomson, Jonny (31 October 2022). \"Why don't robots have rights?\" (https://bigthink.com/thi\nnking/why-dont-robots-have-rights). Big Think. Archived (https://web.archive.org/web/20240\n913055336/https://bigthink.com/thinking/why-dont-robots-have-rights/) from the original on\n13 September 2024. Retrieved 23 February 2024.\n410. Kateman, Brian (24 July 2023). \"AI Should Be Terrified of Humans\" (https://time.com/629623\n4/ai-should-be-terrified-of-humans). Time. Archived (https://web.archive.org/web/202409250\n41601/https://time.com/6296234/ai-should-be-terrified-of-humans/) from the original on 25\nSeptember 2024. Retrieved 23 February 2024.\n411. Wong, Jeff (10 July 2023). \"What leaders need to know about robot rights\" (https://www.fast\ncompany.com/90920769/what-leaders-need-to-know-about-robot-rights). Fast Company.\n412. Hern, Alex (12 January 2017). \"Give robots 'personhood' status, EU committee argues\" (http\ns://www.theguardian.com/technology/2017/jan/12/give-robots-personhood-status-eu-commit\ntee-argues). The Guardian. ISSN 0261-3077 (https://search.worldcat.org/issn/0261-3077).\nArchived (https://web.archive.org/web/20241005171222/https://www.theguardian.com/techn\nology/2017/jan/12/give-robots-personhood-status-eu-committee-argues) from the original on\n5 October 2024. Retrieved 23 February 2024.\n413. Dovey, Dana (14 April 2018). \"Experts Don't Think Robots Should Have Rights\" (https://ww\nw.newsweek.com/robots-human-rights-electronic-persons-humans-versus-machines-88607\n5). Newsweek. Archived (https://web.archive.org/web/20241005171333/https://www.newswe\nek.com/robots-human-rights-electronic-persons-humans-versus-machines-886075) from the\noriginal on 5 October 2024. Retrieved 23 February 2024.\n414. Cuddy, Alice (13 April 2018). \"Robot rights violate human rights, experts warn EU\" (https://w\nww.euronews.com/2018/04/13/robot-rights-violate-human-rights-experts-warn-eu).\neuronews. Archived (https://web.archive.org/web/20240919022327/https://www.euronews.c\nom/2018/04/13/robot-rights-violate-human-rights-experts-warn-eu) from the original on 19\nSeptember 2024. Retrieved 23 February 2024.\n415. The Intelligence explosion and technological singularity: Russell & Norvig (2021, pp. 1004\u2013\n1005), Omohundro (2008), Kurzweil (2005) I. J. Good's \"intelligence explosion\": Good\n(1965) Vernor Vinge's \"singularity\": Vinge (1993)\n416. Russell & Norvig (2021), p. 1005.\n417. Transhumanism: Moravec (1988), Kurzweil (2005), Russell & Norvig (2021, p. 1005)\n418. AI as evolution: Edward Fredkin is quoted in McCorduck (2004, p. 401), Butler (1863),\nDyson (1998)\n419. McQuillan, Dan (14 January 2025). \"a gift to the far right\" (https://www.computerweekly.com/\nopinion/Labours-AI-Action-Plan-a-gift-to-the-far-right). ComputerWeekly.com. Retrieved\n22 January 2025.\n420. AI in myth: McCorduck (2004, pp. 4\u20135)\n421. McCorduck (2004), pp. 340\u2013400.\n422. Buttazzo (2001).\n423. Anderson (2008).\n424. McCauley (2007).\n425. Galvan (1997).\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:55>\n",
    "text": "AI textbooks\nThe two most widely used textbooks in 2023 (see the Open Syllabus (https://explorer.opensyllabus.org/re\nsult/field?id=Computer+Science)):\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.).\nHoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474 (https://lccn.loc.gov/201904\n74).\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.).\nNew Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.\nThe four most widely used AI textbooks in 2008:\nLuger, George; Stubblefield, William (2004). Artificial Intelligence: Structures and Strategies for\nComplex Problem Solving (https://archive.org/details/artificialintell0000luge) (5th ed.).\nBenjamin/Cummings. ISBN 978-0-8053-4780-7. Archived (https://web.archive.org/web/2020\n0726220613/https://archive.org/details/artificialintell0000luge) from the original on 26 July\n2020. Retrieved 17 December 2019.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis (https://archive.org/details/artificialin\ntell0000nils). Morgan Kaufmann. ISBN 978-1-5586-0467-4. Archived (https://web.archive.or\ng/web/20200726131654/https://archive.org/details/artificialintell0000nils) from the original on\n26 July 2020. Retrieved 18 November 2019.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (http://aima.c\ns.berkeley.edu/) (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-\n790395-2.\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical\nApproach (https://archive.org/details/computationalint00pool). New York: Oxford University\nPress. ISBN 978-0-1951-0270-3. Archived (https://web.archive.org/web/20200726131436/ht\ntps://archive.org/details/computationalint00pool) from the original on 26 July 2020. Retrieved\n22 August 2020. Later edition: Poole, David; Mackworth, Alan (2017). Artificial Intelligence:\nFoundations of Computational Agents (http://artint.info/index.html) (2nd ed.). Cambridge\nUniversity Press. ISBN 978-1-1071-9539-4. Archived (https://web.archive.org/web/2017120\n7013855/http://artint.info/index.html) from the original on 7 December 2017. Retrieved\n6 December 2017.\nOther textbooks:\nErtel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-\n3195-8486-7.\nCiaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from\ndata analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.\nHistory of AI\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY:\nBasicBooks. ISBN 0-465-02997-3.\nMcCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, Massachusetts: A. K.\nPeters, ISBN 1-5688-1205-1\nNewquist, H. P. (1994). The Brain Makers: Genius, Ego, And Greed In The Quest For Machines\nThat Think. New York: Macmillan/SAMS. ISBN 978-0-6723-0412-5.\nHarmon, Paul; Sawyer, Brian (1990). Creating Expert Systems for Business and Industry. New\nYork: John Wiley & Sons. ISBN 0471614963.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:56>\n",
    "text": "Other sources\nAI & ML in Fusion (https://suli.pppl.gov/2023/course/Rea-PPPL-SULI2023.pdf)\nAI & ML in Fusion, video lecture (https://drive.google.com/file/d/1npCTrJ8XJn20ZGDA_DfMpAN\nuQZFMzKPh/view?usp=drive_link) Archived (https://web.archive.org/web/20230702164332/\nhttps://drive.google.com/file/d/1npCTrJ8XJn20ZGDA_DfMpANuQZFMzKPh/view?usp=drive\n_link) 2 July 2023 at the Wayback Machine\nAlter, Alexandra; Harris, Elizabeth A. (20 September 2023), \"Franzen, Grisham and Other\nProminent Authors Sue OpenAI\" (https://www.nytimes.com/2023/09/20/books/authors-open\nai-lawsuit-chatgpt-copyright.html?campaign_id=2&emc=edit_th_20230921&instance_id=103\n259&nl=todaysheadlines&regi_id=62816440&segment_id=145288&user_id=ad24f3545dae\n0ec44284a38bb4a88f1d), The New York Times, archived (https://web.archive.org/web/2024\n0914155020/https://www.nytimes.com/2023/09/20/books/authors-openai-lawsuit-chatgpt-co\npyright.html?campaign_id=2&emc=edit_th_20230921&instance_id=103259&nl=todaysheadl\nines&regi_id=62816440&segment_id=145288&user_id=ad24f3545dae0ec44284a38bb4a88\nf1d) from the original on 14 September 2024, retrieved 5 October 2024\nAltman, Sam; Brockman, Greg; Sutskever, Ilya (22 May 2023). \"Governance of\nSuperintelligence\" (https://openai.com/blog/governance-of-superintelligence). openai.com.\nArchived (https://web.archive.org/web/20230527061619/https://openai.com/blog/governanc\ne-of-superintelligence) from the original on 27 May 2023. Retrieved 27 May 2023.\nAnderson, Susan Leigh (2008). \"Asimov's \"three laws of robotics\" and machine metaethics\". AI\n& Society. 22 (4): 477\u2013493. doi:10.1007/s00146-007-0094-5 (https://doi.org/10.1007%2Fs0\n0146-007-0094-5). S2CID 1809459 (https://api.semanticscholar.org/CorpusID:1809459).\nAnderson, Michael; Anderson, Susan Leigh (2011). Machine Ethics. Cambridge University\nPress.\nArntz, Melanie; Gregory, Terry; Zierahn, Ulrich (2016), \"The risk of automation for jobs in OECD\ncountries: A comparative analysis\", OECD Social, Employment, and Migration Working\nPapers 189\nAsada, M.; Hosoda, K.; Kuniyoshi, Y.; Ishiguro, H.; Inui, T.; Yoshikawa, Y.; Ogino, M.; Yoshida,\nC. (2009). \"Cognitive developmental robotics: a survey\". IEEE Transactions on Autonomous\nMental Development. 1 (1): 12\u201334. doi:10.1109/tamd.2009.2021702 (https://doi.org/10.110\n9%2Ftamd.2009.2021702). S2CID 10168773 (https://api.semanticscholar.org/CorpusID:101\n68773).\n\"Ask the AI experts: What's driving today's progress in AI?\" (https://www.mckinsey.com/business\n-functions/mckinsey-analytics/our-insights/ask-the-ai-experts-whats-driving-todays-progress-\nin-ai). McKinsey & Company. Archived (https://web.archive.org/web/20180413190018/http\ns://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/ask-the-ai-expert\ns-whats-driving-todays-progress-in-ai) from the original on 13 April 2018. Retrieved 13 April\n2018.\nBarfield, Woodrow; Pagallo, Ugo (2018). Research handbook on the law of artificial intelligence.\nCheltenham, UK: Edward Elgar Publishing. ISBN 978-1-7864-3904-8. OCLC 1039480085\n(https://search.worldcat.org/oclc/1039480085).\nBeal, J.; Winston, Patrick (2009), \"The New Frontier of Human-Level Artificial Intelligence\",\nIEEE Intelligent Systems, vol. 24, pp. 21\u201324, doi:10.1109/MIS.2009.75 (https://doi.org/10.11\n09%2FMIS.2009.75), hdl:1721.1/52357 (https://hdl.handle.net/1721.1%2F52357),\nS2CID 32437713 (https://api.semanticscholar.org/CorpusID:32437713)\nBerdahl, Carl Thomas; Baker, Lawrence; Mann, Sean; Osoba, Osonde; Girosi, Federico (7\nFebruary 2023). \"Strategies to Improve the Impact of Artificial Intelligence on Health Equity:\nScoping Review\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11041459). JMIR AI. 2:\ne42936. doi:10.2196/42936 (https://doi.org/10.2196%2F42936). ISSN 2817-1705 (https://se\narch.worldcat.org/issn/2817-1705). PMC 11041459 (https://www.ncbi.nlm.nih.gov/pmc/articl\nes/PMC11041459). PMID 38875587 (https://pubmed.ncbi.nlm.nih.gov/38875587).\nS2CID 256681439 (https://api.semanticscholar.org/CorpusID:256681439).\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:57>\n",
    "text": "Berryhill, Jamie; Heang, K\u00e9vin Kok; Clogher, Rob; McBride, Keegan (2019). Hello, World:\nArtificial Intelligence and its Use in the Public Sector (https://oecd-opsi.org/wp-content/uploa\nds/2019/11/AI-Report-Online.pdf) (PDF). Paris: OECD Observatory of Public Sector\nInnovation. Archived (https://web.archive.org/web/20191220021331/https://oecd-opsi.org/wp\n-content/uploads/2019/11/AI-Report-Online.pdf) (PDF) from the original on 20 December\n2019. Retrieved 9 August 2020.\nBertini, M; Del Bimbo, A; Torniai, C (2006). \"Automatic annotation and semantic retrieval of\nvideo sequences using multimedia ontologies\". MM '06 Proceedings of the 14th ACM\ninternational conference on Multimedia. 14th ACM international conference on Multimedia.\nSanta Barbara: ACM. pp. 679\u2013682.\nBostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.\nBostrom, Nick (2015). \"What happens when our computers get smarter than we are?\" (https://w\nww.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smarter_than_w\ne_are/transcript). TED (conference). Archived (https://web.archive.org/web/2020072500571\n9/https://www.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smart\ner_than_we_are/transcript) from the original on 25 July 2020. Retrieved 30 January 2020.\nBrooks, Rodney (10 November 2014). \"artificial intelligence is a tool, not a threat\" (https://web.a\nrchive.org/web/20141112130954/http://www.rethinkrobotics.com/artificial-intelligence-tool-th\nreat). Archived from the original (http://www.rethinkrobotics.com/artificial-intelligence-tool-thr\neat) on 12 November 2014.\nBrooks, Rodney (1990). \"Elephants Don't Play Chess\" (http://people.csail.mit.edu/brooks/paper\ns/elephants.pdf) (PDF). Robotics and Autonomous Systems. 6 (1\u20132): 3\u201315.\nCiteSeerX 10.1.1.588.7539 (https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.588.\n7539). doi:10.1016/S0921-8890(05)80025-9 (https://doi.org/10.1016%2FS0921-8890%280\n5%2980025-9). Archived (https://web.archive.org/web/20070809020912/http://people.csail.\nmit.edu/brooks/papers/elephants.pdf) (PDF) from the original on 9 August 2007.\nBuiten, Miriam C (2019). \"Towards Intelligent Regulation of Artificial Intelligence\" (https://doi.org/\n10.1017%2Ferr.2019.8). European Journal of Risk Regulation. 10 (1): 41\u201359.\ndoi:10.1017/err.2019.8 (https://doi.org/10.1017%2Ferr.2019.8). ISSN 1867-299X (https://sea\nrch.worldcat.org/issn/1867-299X).\nBushwick, Sophie (16 March 2023), \"What the New GPT-4 AI Can Do\" (https://www.scientificam\nerican.com/article/what-the-new-gpt-4-ai-can-do/), Scientific American, archived (https://we\nb.archive.org/web/20230822233655/https://www.scientificamerican.com/article/what-the-ne\nw-gpt-4-ai-can-do/) from the original on 22 August 2023, retrieved 5 October 2024\nButler, Samuel (13 June 1863). \"Darwin among the Machines\" (https://nzetc.victoria.ac.nz/tm/sc\nholarly/tei-ButFir-t1-g1-t1-g1-t4-body.html). Letters to the Editor. The Press. Christchurch,\nNew Zealand. Archived (https://web.archive.org/web/20080919172551/http://www.nzetc.org/\ntm/scholarly/tei-ButFir-t1-g1-t1-g1-t4-body.html) from the original on 19 September 2008.\nRetrieved 16 October 2014 \u2013 via Victoria University of Wellington.\nButtazzo, G. (July 2001). \"Artificial consciousness: Utopia or real possibility?\". Computer. 34\n(7): 24\u201330. doi:10.1109/2.933500 (https://doi.org/10.1109%2F2.933500).\nCambria, Erik; White, Bebo (May 2014). \"Jumping NLP Curves: A Review of Natural Language\nProcessing Research [Review Article]\". IEEE Computational Intelligence Magazine. 9 (2):\n48\u201357. doi:10.1109/MCI.2014.2307227 (https://doi.org/10.1109%2FMCI.2014.2307227).\nS2CID 206451986 (https://api.semanticscholar.org/CorpusID:206451986).\nCellan-Jones, Rory (2 December 2014). \"Stephen Hawking warns artificial intelligence could\nend mankind\" (https://www.bbc.com/news/technology-30290540). BBC News. Archived (http\ns://web.archive.org/web/20151030054329/http://www.bbc.com/news/technology-30290540)\nfrom the original on 30 October 2015. Retrieved 30 October 2015.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:58>\n",
    "text": "Chalmers, David (1995). \"Facing up to the problem of consciousness\" (http://www.imprint.co.uk/\nchalmers.html). Journal of Consciousness Studies. 2 (3): 200\u2013219.\nCiteSeerX 10.1.1.103.8362 (https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.103.\n8362). Archived (https://web.archive.org/web/20050308163649/http://www.imprint.co.uk/chal\nmers.html) from the original on 8 March 2005. Retrieved 11 October 2018.\nChalla, Subhash; Moreland, Mark R.; Mu\u0161icki, Darko; Evans, Robin J. (2011). Fundamentals of\nObject Tracking. Cambridge University Press. doi:10.1017/CBO9780511975837 (https://doi.\norg/10.1017%2FCBO9780511975837). ISBN 978-0-5218-7628-5.\nChristian, Brian (2020). The Alignment Problem: Machine learning and human values. W. W.\nNorton & Company. ISBN 978-0-3938-6833-3. OCLC 1233266753 (https://search.worldcat.o\nrg/oclc/1233266753).\nCiresan, D.; Meier, U.; Schmidhuber, J. (2012). \"Multi-column deep neural networks for image\nclassification\". 2012 IEEE Conference on Computer Vision and Pattern Recognition.\npp. 3642\u20133649. arXiv:1202.2745 (https://arxiv.org/abs/1202.2745).\ndoi:10.1109/cvpr.2012.6248110 (https://doi.org/10.1109%2Fcvpr.2012.6248110). ISBN 978-\n1-4673-1228-8. S2CID 2161592 (https://api.semanticscholar.org/CorpusID:2161592).\nClark, Jack (2015b). \"Why 2015 Was a Breakthrough Year in Artificial Intelligence\" (https://www.\nbloomberg.com/news/articles/2015-12-08/why-2015-was-a-breakthrough-year-in-artificial-int\nelligence). Bloomberg.com. Archived (https://web.archive.org/web/20161123053855/https://\nwww.bloomberg.com/news/articles/2015-12-08/why-2015-was-a-breakthrough-year-in-artific\nial-intelligence) from the original on 23 November 2016. Retrieved 23 November 2016.\nCNA (12 January 2019). \"Commentary: Bad news. Artificial intelligence is biased\" (https://www.c\nhannelnewsasia.com/news/commentary/artificial-intelligence-big-data-bias-hiring-loans-key-\nchallenge-11097374). CNA. Archived (https://web.archive.org/web/20190112104421/https://\nwww.channelnewsasia.com/news/commentary/artificial-intelligence-big-data-bias-hiring-loan\ns-key-challenge-11097374) from the original on 12 January 2019. Retrieved 19 June 2020.\nCybenko, G. (1988). Continuous valued neural networks with two hidden layers are sufficient\n(Report). Department of Computer Science, Tufts University.\nDeng, L.; Yu, D. (2014). \"Deep Learning: Methods and Applications\" (http://research.microsoft.c\nom/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf) (PDF). Foundations and\nTrends in Signal Processing. 7 (3\u20134): 197\u2013387. doi:10.1561/2000000039 (https://doi.org/10.\n1561%2F2000000039). Archived (https://web.archive.org/web/20160314152112/http://resea\nrch.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf) (PDF)\nfrom the original on 14 March 2016. Retrieved 18 October 2014.\nDennett, Daniel (1991). Consciousness Explained. The Penguin Press. ISBN 978-0-7139-9037-\n9.\nDiFeliciantonio, Chase (3 April 2023). \"AI has already changed the world. This report shows\nhow\" (https://www.sfchronicle.com/tech/article/ai-artificial-intelligence-report-stanford-17869\n558.php). San Francisco Chronicle. Archived (https://web.archive.org/web/2023061901530\n9/https://www.sfchronicle.com/tech/article/ai-artificial-intelligence-report-stanford-17869558.\nphp) from the original on 19 June 2023. Retrieved 19 June 2023.\nDickson, Ben (2 May 2022). \"Machine learning: What is the transformer architecture?\" (https://b\ndtechtalks.com/2022/05/02/what-is-the-transformer). TechTalks. Archived (https://web.archiv\ne.org/web/20231122142948/https://bdtechtalks.com/2022/05/02/what-is-the-transformer/)\nfrom the original on 22 November 2023. Retrieved 22 November 2023.\nDockrill, Peter (27 June 2022), \"Robots With Flawed AI Make Sexist And Racist Decisions,\nExperiment Shows\" (https://web.archive.org/web/20220627225827/https://www.sciencealert.\ncom/robots-with-flawed-ai-make-sexist-racist-and-toxic-decisions-experiment-shows),\nScience Alert, archived from the original (https://www.sciencealert.com/robots-with-flawed-ai\n-make-sexist-racist-and-toxic-decisions-experiment-shows) on 27 June 2022\nDomingos, Pedro (2015). The Master Algorithm: How the Quest for the Ultimate Learning\nMachine Will Remake Our World. Basic Books. ISBN 978-0-4650-6570-7.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:59>\n",
    "text": "Dreyfus, Hubert (1972). What Computers Can't Do. New York: MIT Press. ISBN 978-0-0601-\n1082-6.\nDreyfus, Hubert; Dreyfus, Stuart (1986). Mind over Machine: The Power of Human Intuition and\nExpertise in the Era of the Computer (https://archive.org/details/mindovermachinep00drey).\nOxford: Blackwell. ISBN 978-0-0290-8060-3. Archived (https://web.archive.org/web/2020072\n6131414/https://archive.org/details/mindovermachinep00drey) from the original on 26 July\n2020. Retrieved 22 August 2020.\nDyson, George (1998). Darwin among the Machines (https://archive.org/details/darwinamongm\nachi00dyso). Allan Lane Science. ISBN 978-0-7382-0030-9. Archived (https://web.archive.or\ng/web/20200726131443/https://archive.org/details/darwinamongmachi00dyso) from the\noriginal on 26 July 2020. Retrieved 22 August 2020.\nEdelson, Edward (1991). The Nervous System (https://archive.org/details/nervoussystem0000e\ndel). New York: Chelsea House. ISBN 978-0-7910-0464-7. Archived (https://web.archive.or\ng/web/20200726131758/https://archive.org/details/nervoussystem0000edel) from the\noriginal on 26 July 2020. Retrieved 18 November 2019.\nEdwards, Benj (17 May 2023). \"Poll: AI poses risk to humanity, according to majority of\nAmericans\" (https://arstechnica.com/information-technology/2023/05/poll-61-of-americans-s\nay-ai-threatens-humanitys-future). Ars Technica. Archived (https://web.archive.org/web/2023\n0619013608/https://arstechnica.com/information-technology/2023/05/poll-61-of-americans-s\nay-ai-threatens-humanitys-future) from the original on 19 June 2023. Retrieved 19 June\n2023.\nFearn, Nicholas (2007). The Latest Answers to the Oldest Questions: A Philosophical Adventure\nwith the World's Greatest Thinkers. New York: Grove Press. ISBN 978-0-8021-1839-4.\nFord, Martin; Colvin, Geoff (6 September 2015). \"Will robots create more jobs than they\ndestroy?\" (https://www.theguardian.com/technology/2015/sep/06/will-robots-create-destroy-j\nobs). The Guardian. Archived (https://web.archive.org/web/20180616204119/https://www.th\neguardian.com/technology/2015/sep/06/will-robots-create-destroy-jobs) from the original on\n16 June 2018. Retrieved 13 January 2018.\nFox News (2023). \"Fox News Poll\" (https://static.foxnews.com/foxnews.com/content/uploads/20\n23/05/Fox_April-21-24-2023_Complete_National_Topline_May-1-Release.pdf) (PDF). Fox\nNews. Archived (https://web.archive.org/web/20230512082712/https://static.foxnews.com/fo\nxnews.com/content/uploads/2023/05/Fox_April-21-24-2023_Complete_National_Topline_M\nay-1-Release.pdf) (PDF) from the original on 12 May 2023. Retrieved 19 June 2023.\nFrey, Carl Benedikt; Osborne, Michael A (1 January 2017). \"The future of employment: How\nsusceptible are jobs to computerisation?\". Technological Forecasting and Social Change.\n114: 254\u2013280. CiteSeerX 10.1.1.395.416 (https://citeseerx.ist.psu.edu/viewdoc/summary?d\noi=10.1.1.395.416). doi:10.1016/j.techfore.2016.08.019 (https://doi.org/10.1016%2Fj.techfor\ne.2016.08.019). ISSN 0040-1625 (https://search.worldcat.org/issn/0040-1625).\n\"From not working to neural networking\" (https://www.economist.com/news/special-report/2170\n0756-artificial-intelligence-boom-based-old-idea-modern-twist-not). The Economist. 2016.\nArchived (https://web.archive.org/web/20161231203934/https://www.economist.com/news/s\npecial-report/21700756-artificial-intelligence-boom-based-old-idea-modern-twist-not) from\nthe original on 31 December 2016. Retrieved 26 April 2018.\nGalvan, Jill (1 January 1997). \"Entering the Posthuman Collective in Philip K. Dick's \"Do\nAndroids Dream of Electric Sheep?\"\". Science Fiction Studies. 24 (3): 413\u2013429.\ndoi:10.1525/sfs.24.3.0413 (https://doi.org/10.1525%2Fsfs.24.3.0413). JSTOR 4240644 (http\ns://www.jstor.org/stable/4240644).\nGeist, Edward Moore (9 August 2015). \"Is artificial intelligence really an existential threat to\nhumanity?\" (http://thebulletin.org/artificial-intelligence-really-existential-threat-humanity857\n7). Bulletin of the Atomic Scientists. Archived (https://web.archive.org/web/2015103005433\n0/http://thebulletin.org/artificial-intelligence-really-existential-threat-humanity8577) from the\noriginal on 30 October 2015. Retrieved 30 October 2015.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:60>\n",
    "text": "Gibbs, Samuel (27 October 2014). \"Elon Musk: artificial intelligence is our biggest existential\nthreat\" (https://www.theguardian.com/technology/2014/oct/27/elon-musk-artificial-intelligenc\ne-ai-biggest-existential-threat). The Guardian. Archived (https://web.archive.org/web/201510\n30054330/http://www.theguardian.com/technology/2014/oct/27/elon-musk-artificial-intelligen\nce-ai-biggest-existential-threat) from the original on 30 October 2015. Retrieved 30 October\n2015.\nGoffrey, Andrew (2008). \"Algorithm\". In Fuller, Matthew (ed.). Software studies: a lexicon (http\ns://archive.org/details/softwarestudiesl00full_007). Cambridge, Mass.: MIT Press. pp. 15 (htt\nps://archive.org/details/softwarestudiesl00full_007/page/n29)\u201320. ISBN 978-1-4356-4787-9.\nGoldman, Sharon (14 September 2022). \"10 years later, deep learning 'revolution' rages on, say\nAI pioneers Hinton, LeCun and Li\" (https://venturebeat.com/ai/10-years-on-ai-pioneers-hinto\nn-lecun-li-say-deep-learning-revolution-will-continue). VentureBeat. Archived (https://web.arc\nhive.org/web/20241005171338/https://venturebeat.com/ai/10-years-on-ai-pioneers-hinton-le\ncun-li-say-deep-learning-revolution-will-continue/) from the original on 5 October 2024.\nRetrieved 8 December 2023.\nGood, I. J. (1965), Speculations Concerning the First Ultraintelligent Machine (https://exhibits.st\nanford.edu/feigenbaum/catalog/gz727rg3869), archived (https://web.archive.org/web/20230\n710131733/https://exhibits.stanford.edu/feigenbaum/catalog/gz727rg3869) from the original\non 10 July 2023, retrieved 5 October 2024\nGoodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016), Deep Learning (https://web.archive.or\ng/web/20160416111010/http://www.deeplearningbook.org), MIT Press., archived from the\noriginal (http://www.deeplearningbook.org) on 16 April 2016, retrieved 12 November 2017\nGoodman, Bryce; Flaxman, Seth (2017). \"EU regulations on algorithmic decision-making and a\n'right to explanation'\". AI Magazine. 38 (3): 50. arXiv:1606.08813 (https://arxiv.org/abs/1606.\n08813). doi:10.1609/aimag.v38i3.2741 (https://doi.org/10.1609%2Faimag.v38i3.2741).\nS2CID 7373959 (https://api.semanticscholar.org/CorpusID:7373959).\nGovernment Accountability Office (13 September 2022). Consumer Data: Increasing Use Poses\nRisks to Privacy (https://www.gao.gov/products/gao-22-106096). gao.gov (Report). Archived\n(https://web.archive.org/web/20240913011410/https://www.gao.gov/products/gao-22-10609\n6) from the original on 13 September 2024. Retrieved 5 October 2024.\nGrant, Nico; Hill, Kashmir (22 May 2023). \"Google's Photo App Still Can't Find Gorillas. And\nNeither Can Apple's\" (https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-googl\ne-apple.html). The New York Times. Archived (https://web.archive.org/web/2024091415503\n2/https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html) from\nthe original on 14 September 2024. Retrieved 5 October 2024.\nGoswami, Rohan (5 April 2023). \"Here's where the A.I. jobs are\" (https://www.cnbc.com/2023/0\n4/05/ai-jobs-see-the-state-by-state-data-from-a-stanford-study.html). CNBC. Archived (http\ns://web.archive.org/web/20230619015309/https://www.cnbc.com/2023/04/05/ai-jobs-see-the\n-state-by-state-data-from-a-stanford-study.html) from the original on 19 June 2023.\nRetrieved 19 June 2023.\nHarari, Yuval Noah (October 2018). \"Why Technology Favors Tyranny\" (https://www.theatlantic.\ncom/magazine/archive/2018/10/yuval-noah-harari-technology-tyranny/568330). The Atlantic.\nArchived (https://web.archive.org/web/20210925221449/https://www.theatlantic.com/magazi\nne/archive/2018/10/yuval-noah-harari-technology-tyranny/568330) from the original on 25\nSeptember 2021. Retrieved 23 September 2021.\nHarari, Yuval Noah (2023). \"AI and the future of humanity\" (https://www.youtube.com/watch?v=\nLWiM-LuRe6w). YouTube. Archived (https://web.archive.org/web/20240930110823/https://w\nww.youtube.com/watch?v=LWiM-LuRe6w) from the original on 30 September 2024.\nRetrieved 5 October 2024.\nHaugeland, John (1985). Artificial Intelligence: The Very Idea. Cambridge, Mass.: MIT Press.\nISBN 978-0-2620-8153-5.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:61>\n",
    "text": "Hinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.;\nNguyen, P.; Sainath, T.; Kingsbury, B. (2012). \"Deep Neural Networks for Acoustic Modeling\nin Speech Recognition \u2013 The shared views of four research groups\". IEEE Signal\nProcessing Magazine. 29 (6): 82\u201397. Bibcode:2012ISPM...29...82H (https://ui.adsabs.harvar\nd.edu/abs/2012ISPM...29...82H). doi:10.1109/msp.2012.2205597 (https://doi.org/10.1109%\n2Fmsp.2012.2205597). S2CID 206485943 (https://api.semanticscholar.org/CorpusID:20648\n5943).\nHolley, Peter (28 January 2015). \"Bill Gates on dangers of artificial intelligence: 'I don't\nunderstand why some people are not concerned'\" (https://www.washingtonpost.com/news/t\nhe-switch/wp/2015/01/28/bill-gates-on-dangers-of-artificial-intelligence-dont-understand-why\n-some-people-are-not-concerned). The Washington Post. ISSN 0190-8286 (https://search.w\norldcat.org/issn/0190-8286). Archived (https://web.archive.org/web/20151030054330/https://\nwww.washingtonpost.com/news/the-switch/wp/2015/01/28/bill-gates-on-dangers-of-artificial-\nintelligence-dont-understand-why-some-people-are-not-concerned) from the original on 30\nOctober 2015. Retrieved 30 October 2015.\nHornik, Kurt; Stinchcombe, Maxwell; White, Halbert (1989). Multilayer Feedforward Networks\nare Universal Approximators (http://cognitivemedium.com/magic_paper/assets/Hornik.pdf)\n(PDF). Neural Networks. Vol. 2. Pergamon Press. pp. 359\u2013366. Archived (https://web.archiv\ne.org/web/20230421140436/https://cognitivemedium.com/magic_paper/assets/Hornik.pdf)\n(PDF) from the original on 21 April 2023. Retrieved 5 October 2024.\nHorst, Steven (2005). \"The Computational Theory of Mind\" (http://plato.stanford.edu/entries/com\nputational-mind). The Stanford Encyclopedia of Philosophy. Archived (https://web.archive.or\ng/web/20160306083748/http://plato.stanford.edu/entries/computational-mind) from the\noriginal on 6 March 2016. Retrieved 7 March 2016.\nHowe, J. (November 1994). \"Artificial Intelligence at Edinburgh University: a Perspective\" (http://\nwww.inf.ed.ac.uk/about/AIhistory.html). Archived (https://web.archive.org/web/20070515072\n641/http://www.inf.ed.ac.uk/about/AIhistory.html) from the original on 15 May 2007.\nRetrieved 30 August 2007.\nIGM Chicago (30 June 2017). \"Robots and Artificial Intelligence\" (http://www.igmchicago.org/sur\nveys/robots-and-artificial-intelligence). igmchicago.org. Archived (https://web.archive.org/we\nb/20190501114826/http://www.igmchicago.org/surveys/robots-and-artificial-intelligence)\nfrom the original on 1 May 2019. Retrieved 3 July 2019.\nIphofen, Ron; Kritikos, Mihalis (3 January 2019). \"Regulating artificial intelligence and robotics:\nethics by design in a digital society\". Contemporary Social Science. 16 (2): 170\u2013184.\ndoi:10.1080/21582041.2018.1563803 (https://doi.org/10.1080%2F21582041.2018.156380\n3). ISSN 2158-2041 (https://search.worldcat.org/issn/2158-2041). S2CID 59298502 (https://\napi.semanticscholar.org/CorpusID:59298502).\nJordan, M. I.; Mitchell, T. M. (16 July 2015). \"Machine learning: Trends, perspectives, and\nprospects\". Science. 349 (6245): 255\u2013260. Bibcode:2015Sci...349..255J (https://ui.adsabs.h\narvard.edu/abs/2015Sci...349..255J). doi:10.1126/science.aaa8415 (https://doi.org/10.112\n6%2Fscience.aaa8415). PMID 26185243 (https://pubmed.ncbi.nlm.nih.gov/26185243).\nS2CID 677218 (https://api.semanticscholar.org/CorpusID:677218).\nKahneman, Daniel (2011). Thinking, Fast and Slow (https://books.google.com/books?id=ZuKTv\nERuPG8C). Macmillan. ISBN 978-1-4299-6935-2. Archived (https://web.archive.org/web/20\n230315191803/https://books.google.com/books?id=ZuKTvERuPG8C) from the original on\n15 March 2023. Retrieved 8 April 2012.\nKahneman, Daniel; Slovic, D.; Tversky, Amos (1982). \"Judgment under uncertainty: Heuristics\nand biases\". Science. 185 (4157). New York: Cambridge University Press: 1124\u20131131.\nBibcode:1974Sci...185.1124T (https://ui.adsabs.harvard.edu/abs/1974Sci...185.1124T).\ndoi:10.1126/science.185.4157.1124 (https://doi.org/10.1126%2Fscience.185.4157.1124).\nISBN 978-0-5212-8414-1. PMID 17835457 (https://pubmed.ncbi.nlm.nih.gov/17835457).\nS2CID 143452957 (https://api.semanticscholar.org/CorpusID:143452957).\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:62>\n",
    "text": "Kasperowicz, Peter (1 May 2023). \"Regulate AI? GOP much more skeptical than Dems that\ngovernment can do it right: poll\" (https://www.foxnews.com/politics/regulate-ai-gop-much-mo\nre-skeptical-than-dems-that-the-government-can-do-it-right-poll). Fox News. Archived (http\ns://web.archive.org/web/20230619013616/https://www.foxnews.com/politics/regulate-ai-gop-\nmuch-more-skeptical-than-dems-that-the-government-can-do-it-right-poll) from the original\non 19 June 2023. Retrieved 19 June 2023.\nKatz, Yarden (1 November 2012). \"Noam Chomsky on Where Artificial Intelligence Went Wrong\"\n(https://www.theatlantic.com/technology/archive/2012/11/noam-chomsky-on-where-artificial-i\nntelligence-went-wrong/261637/?single_page=true). The Atlantic. Archived (https://web.arch\nive.org/web/20190228154403/https://www.theatlantic.com/technology/archive/2012/11/noa\nm-chomsky-on-where-artificial-intelligence-went-wrong/261637/?single_page=true) from the\noriginal on 28 February 2019. Retrieved 26 October 2014.\n\"Kismet\" (http://www.ai.mit.edu/projects/humanoid-robotics-group/kismet/kismet.html). MIT\nArtificial Intelligence Laboratory, Humanoid Robotics Group. Archived (https://web.archive.or\ng/web/20141017040432/http://www.ai.mit.edu/projects/humanoid-robotics-group/kismet/kis\nmet.html) from the original on 17 October 2014. Retrieved 25 October 2014.\nKissinger, Henry (1 November 2021). \"The Challenge of Being Human in the Age of AI\" (https://\nwww.wsj.com/articles/being-human-artifical-intelligence-ai-chess-antibiotic-philosophy-ethics\n-bill-of-rights-11635795271). The Wall Street Journal. Archived (https://web.archive.org/web/\n20211104012825/https://www.wsj.com/articles/being-human-artifical-intelligence-ai-chess-a\nntibiotic-philosophy-ethics-bill-of-rights-11635795271) from the original on 4 November\n2021. Retrieved 4 November 2021.\nKobielus, James (27 November 2019). \"GPUs Continue to Dominate the AI Accelerator Market\nfor Now\" (https://www.informationweek.com/ai-or-machine-learning/gpus-continue-to-domin\nate-the-ai-accelerator-market-for-now). InformationWeek. Archived (https://web.archive.org/\nweb/20211019031104/https://www.informationweek.com/ai-or-machine-learning/gpus-contin\nue-to-dominate-the-ai-accelerator-market-for-now) from the original on 19 October 2021.\nRetrieved 11 June 2020.\nKuperman, G. J.; Reichley, R. M.; Bailey, T. C. (1 July 2006). \"Using Commercial Knowledge\nBases for Clinical Decision Support: Opportunities, Hurdles, and Recommendations\" (http\ns://www.ncbi.nlm.nih.gov/pmc/articles/PMC1513681). Journal of the American Medical\nInformatics Association. 13 (4): 369\u2013371. doi:10.1197/jamia.M2055 (https://doi.org/10.119\n7%2Fjamia.M2055). PMC 1513681 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC151368\n1). PMID 16622160 (https://pubmed.ncbi.nlm.nih.gov/16622160).\nKurzweil, Ray (2005). The Singularity is Near. Penguin Books. ISBN 978-0-6700-3384-3.\nLangley, Pat (2011). \"The changing science of machine learning\" (https://doi.org/10.1007%2Fs1\n0994-011-5242-y). Machine Learning. 82 (3): 275\u2013279. doi:10.1007/s10994-011-5242-y (htt\nps://doi.org/10.1007%2Fs10994-011-5242-y).\nLarson, Jeff; Angwin, Julia (23 May 2016). \"How We Analyzed the COMPAS Recidivism\nAlgorithm\" (https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algor\nithm). ProPublica. Archived (https://web.archive.org/web/20190429190950/https://www.prop\nublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) from the original on 29\nApril 2019. Retrieved 19 June 2020.\nLaskowski, Nicole (November 2023). \"What is Artificial Intelligence and How Does AI Work?\nTechTarget\" (https://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligenc\ne). Enterprise AI. Archived (https://web.archive.org/web/20241005171229/https://www.techta\nrget.com/searchenterpriseai/definition/AI-Artificial-Intelligence) from the original on 5\nOctober 2024. Retrieved 30 October 2023.\nLaw Library of Congress (U.S.). Global Legal Research Directorate, issuing body. (2019).\nRegulation of artificial intelligence in selected jurisdictions. LCCN 2019668143 (https://lccn.l\noc.gov/2019668143). OCLC 1110727808 (https://search.worldcat.org/oclc/1110727808).\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:63>\n",
    "text": "Lee, Timothy B. (22 August 2014). \"Will artificial intelligence destroy humanity? Here are 5\nreasons not to worry\" (https://www.vox.com/2014/8/22/6043635/5-reasons-we-shouldnt-worr\ny-about-super-intelligent-computers-taking). Vox. Archived (https://web.archive.org/web/201\n51030092203/http://www.vox.com/2014/8/22/6043635/5-reasons-we-shouldnt-worry-about-s\nuper-intelligent-computers-taking) from the original on 30 October 2015. Retrieved\n30 October 2015.\nLenat, Douglas; Guha, R. V. (1989). Building Large Knowledge-Based Systems. Addison-\nWesley. ISBN 978-0-2015-1752-1.\nLighthill, James (1973). \"Artificial Intelligence: A General Survey\". Artificial Intelligence: a paper\nsymposium. Science Research Council.\nLipartito, Kenneth (6 January 2011), The Narrative and the Algorithm: Genres of Credit\nReporting from the Nineteenth Century to Today (https://mpra.ub.uni-muenchen.de/28142/1/\nMPRA_paper_28142.pdf) (PDF) (Unpublished manuscript), doi:10.2139/ssrn.1736283 (http\ns://doi.org/10.2139%2Fssrn.1736283), S2CID 166742927 (https://api.semanticscholar.org/C\norpusID:166742927), archived (https://ghostarchive.org/archive/20221009/https://mpra.ub.u\nni-muenchen.de/28142/1/MPRA_paper_28142.pdf) (PDF) from the original on 9 October\n2022\nLohr, Steve (2017). \"Robots Will Take Jobs, but Not as Fast as Some Fear, New Report Says\"\n(https://www.nytimes.com/2017/01/12/technology/robots-will-take-jobs-but-not-as-fast-as-so\nme-fear-new-report-says.html). The New York Times. Archived (https://web.archive.org/web/\n20180114073704/https://www.nytimes.com/2017/01/12/technology/robots-will-take-jobs-but-\nnot-as-fast-as-some-fear-new-report-says.html) from the original on 14 January 2018.\nRetrieved 13 January 2018.\nLungarella, M.; Metta, G.; Pfeifer, R.; Sandini, G. (2003). \"Developmental robotics: a survey\".\nConnection Science. 15 (4): 151\u2013190. CiteSeerX 10.1.1.83.7615 (https://citeseerx.ist.psu.ed\nu/viewdoc/summary?doi=10.1.1.83.7615). doi:10.1080/09540090310001655110 (https://doi.\norg/10.1080%2F09540090310001655110). S2CID 1452734 (https://api.semanticscholar.or\ng/CorpusID:1452734).\n\"Machine Ethics\" (https://web.archive.org/web/20141129044821/http://www.aaai.org/Library/Sy\nmposia/Fall/fs05-06). aaai.org. Archived from the original (http://www.aaai.org/Library/Symp\nosia/Fall/fs05-06) on 29 November 2014.\nMadrigal, Alexis C. (27 February 2015). \"The case against killer robots, from a guy actually\nworking on artificial intelligence\" (https://www.hrw.org/report/2012/11/19/losing-humanity/cas\ne-against-killer-robots). Fusion.net. Archived (https://web.archive.org/web/20160204175716/\nhttp://fusion.net/story/54583/the-case-against-killer-robots-from-a-guy-actually-building-ai)\nfrom the original on 4 February 2016. Retrieved 31 January 2016.\nMahdawi, Arwa (26 June 2017). \"What jobs will still be around in 20 years? Read this to prepare\nyour future\" (https://www.theguardian.com/us-news/2017/jun/26/jobs-future-automation-robo\nts-skills-creative-health). The Guardian. Archived (https://web.archive.org/web/20180114021\n804/https://www.theguardian.com/us-news/2017/jun/26/jobs-future-automation-robots-skills-\ncreative-health) from the original on 14 January 2018. Retrieved 13 January 2018.\nMaker, Meg Houston (2006), AI@50: AI Past, Present, Future (https://web.archive.org/web/200\n81008120238/http://www.engagingexperience.com/2006/07/ai50_ai_past_pr.html),\nDartmouth College, archived from the original (http://www.engagingexperience.com/2006/0\n7/ai50_ai_past_pr.html) on 8 October 2008, retrieved 16 October 2008\nMarmouyet, Fran\u00e7oise (15 December 2023). \"Google's Gemini: is the new AI model really better\nthan ChatGPT?\" (https://theconversation.com/googles-gemini-is-the-new-ai-model-really-bet\nter-than-chatgpt-219526). The Conversation. Archived (https://web.archive.org/web/202403\n04215625/https://theconversation.com/googles-gemini-is-the-new-ai-model-really-better-tha\nn-chatgpt-219526) from the original on 4 March 2024. Retrieved 25 December 2023.\nMinsky, Marvin (1986), The Society of Mind, Simon and Schuster\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:64>\n",
    "text": "McCarthy, John; Minsky, Marvin; Rochester, Nathan; Shannon, Claude (1955). \"A Proposal for\nthe Dartmouth Summer Research Project on Artificial Intelligence\" (https://web.archive.org/w\neb/20070826230310/http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html).\nArchived from the original (http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.\nhtml) on 26 August 2007. Retrieved 30 August 2007.\nMcCarthy, John (2007), \"From Here to Human-Level AI\", Artificial Intelligence, p. 171\nMcCarthy, John (1999), What is AI? (http://jmc.stanford.edu/artificial-intelligence/what-is-ai/inde\nx.html), archived (https://web.archive.org/web/20221204051737/http://jmc.stanford.edu/artifi\ncial-intelligence/what-is-ai/index.html) from the original on 4 December 2022, retrieved\n4 December 2022\nMcCauley, Lee (2007). \"AI armageddon and the three laws of robotics\". Ethics and Information\nTechnology. 9 (2): 153\u2013164. CiteSeerX 10.1.1.85.8904 (https://citeseerx.ist.psu.edu/viewdo\nc/summary?doi=10.1.1.85.8904). doi:10.1007/s10676-007-9138-2 (https://doi.org/10.1007%\n2Fs10676-007-9138-2). S2CID 37272949 (https://api.semanticscholar.org/CorpusID:372729\n49).\nMcGarry, Ken (1 December 2005). \"A survey of interestingness measures for knowledge\ndiscovery\". The Knowledge Engineering Review. 20 (1): 39\u201361.\ndoi:10.1017/S0269888905000408 (https://doi.org/10.1017%2FS0269888905000408).\nS2CID 14987656 (https://api.semanticscholar.org/CorpusID:14987656).\nMcGaughey, E (2022), Will Robots Automate Your Job Away? Full Employment, Basic Income,\nand Economic Democracy (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3044448),\np. 51(3) Industrial Law Journal 511\u2013559, doi:10.2139/ssrn.3044448 (https://doi.org/10.213\n9%2Fssrn.3044448), S2CID 219336439 (https://api.semanticscholar.org/CorpusID:2193364\n39), SSRN 3044448 (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3044448),\narchived (https://web.archive.org/web/20210131074722/https://papers.ssrn.com/sol3/paper\ns.cfm?abstract_id=3044448) from the original on 31 January 2021, retrieved 27 May 2023\nMerkle, Daniel; Middendorf, Martin (2013). \"Swarm Intelligence\". In Burke, Edmund K.; Kendall,\nGraham (eds.). Search Methodologies: Introductory Tutorials in Optimization and Decision\nSupport Techniques. Springer Science & Business Media. ISBN 978-1-4614-6940-7.\nMinsky, Marvin (1967), Computation: Finite and Infinite Machines, Englewood Cliffs, N.J.:\nPrentice-Hall\nMoravec, Hans (1988). Mind Children (https://archive.org/details/mindchildrenfutu00mora).\nHarvard University Press. ISBN 978-0-6745-7616-2. Archived (https://web.archive.org/web/2\n0200726131644/https://archive.org/details/mindchildrenfutu00mora) from the original on 26\nJuly 2020. Retrieved 18 November 2019.\nMorgenstern, Michael (9 May 2015). \"Automation and anxiety\" (https://www.economist.com/new\ns/special-report/21700758-will-smarter-machines-cause-mass-unemployment-automation-a\nnd-anxiety). The Economist. Archived (https://web.archive.org/web/20180112214621/https://\nwww.economist.com/news/special-report/21700758-will-smarter-machines-cause-mass-une\nmployment-automation-and-anxiety) from the original on 12 January 2018. Retrieved\n13 January 2018.\nM\u00fcller, Vincent C.; Bostrom, Nick (2014). \"Future Progress in Artificial Intelligence: A Poll Among\nExperts\" (http://www.sophia.de/pdf/2014_PT-AI_polls.pdf) (PDF). AI Matters. 1 (1): 9\u201311.\ndoi:10.1145/2639475.2639478 (https://doi.org/10.1145%2F2639475.2639478).\nS2CID 8510016 (https://api.semanticscholar.org/CorpusID:8510016). Archived (https://web.\narchive.org/web/20160115114604/http://www.sophia.de/pdf/2014_PT-AI_polls.pdf) (PDF)\nfrom the original on 15 January 2016.\nNeumann, Bernd; M\u00f6ller, Ralf (January 2008). \"On scene interpretation with description logics\".\nImage and Vision Computing. 26 (1): 82\u2013101. doi:10.1016/j.imavis.2007.08.013 (https://doi.\norg/10.1016%2Fj.imavis.2007.08.013). S2CID 10767011 (https://api.semanticscholar.org/Co\nrpusID:10767011).\nNilsson, Nils (1995), \"Eyes on the Prize\", AI Magazine, vol. 16, pp. 9\u201317\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:65>\n",
    "text": "Newell, Allen; Simon, H. A. (1976). \"Computer Science as Empirical Inquiry: Symbols and\nSearch\" (https://doi.org/10.1145%2F360018.360022). Communications of the ACM. 19 (3):\n113\u2013126. doi:10.1145/360018.360022 (https://doi.org/10.1145%2F360018.360022).\nNicas, Jack (7 February 2018). \"How YouTube Drives People to the Internet's Darkest Corners\"\n(https://www.wsj.com/articles/how-youtube-drives-viewers-to-the-internets-darkest-corners-1\n518020478). The Wall Street Journal. ISSN 0099-9660 (https://search.worldcat.org/issn/009\n9-9660). Archived (https://web.archive.org/web/20241005171230/https://www.wsj.com/articl\nes/how-youtube-drives-viewers-to-the-internets-darkest-corners-1518020478) from the\noriginal on 5 October 2024. Retrieved 16 June 2018.\nNilsson, Nils (1983). \"Artificial Intelligence Prepares for 2001\" (https://ai.stanford.edu/~nilsson/O\nnlinePubs-Nils/General%20Essays/AIMag04-04-002.pdf) (PDF). AI Magazine. 1 (1).\nArchived (https://web.archive.org/web/20200817194457/http://ai.stanford.edu/~nilsson/Onlin\nePubs-Nils/General%20Essays/AIMag04-04-002.pdf) (PDF) from the original on 17 August\n2020. Retrieved 22 August 2020. Presidential Address to the Association for the\nAdvancement of Artificial Intelligence.\nNRC (United States National Research Council) (1999). \"Developments in Artificial\nIntelligence\". Funding a Revolution: Government Support for Computing Research. National\nAcademy Press.\nOmohundro, Steve (2008). The Nature of Self-Improving Artificial Intelligence. presented and\ndistributed at the 2007 Singularity Summit, San Francisco, CA.\nOudeyer, P-Y. (2010). \"On the impact of robotics in behavioral and cognitive sciences: from\ninsect navigation to human cognitive development\" (http://www.pyoudeyer.com/IEEETAMD\nOudeyer10.pdf) (PDF). IEEE Transactions on Autonomous Mental Development. 2 (1): 2\u2013\n16. doi:10.1109/tamd.2009.2039057 (https://doi.org/10.1109%2Ftamd.2009.2039057).\nS2CID 6362217 (https://api.semanticscholar.org/CorpusID:6362217). Archived (https://web.\narchive.org/web/20181003202543/http://www.pyoudeyer.com/IEEETAMDOudeyer10.pdf)\n(PDF) from the original on 3 October 2018. Retrieved 4 June 2013.\nPennachin, C.; Goertzel, B. (2007). \"Contemporary Approaches to Artificial General\nIntelligence\". Artificial General Intelligence. Cognitive Technologies. Berlin, Heidelberg:\nSpringer. pp. 1\u201330. doi:10.1007/978-3-540-68677-4_1 (https://doi.org/10.1007%2F978-3-54\n0-68677-4_1). ISBN 978-3-5402-3733-4.\nPinker, Steven (2007) [1994], The Language Instinct, Perennial Modern Classics, Harper,\nISBN 978-0-0613-3646-1\nPoria, Soujanya; Cambria, Erik; Bajpai, Rajiv; Hussain, Amir (September 2017). \"A review of\naffective computing: From unimodal analysis to multimodal fusion\" (http://researchrepository.\nnapier.ac.uk/Output/1792429). Information Fusion. 37: 98\u2013125.\ndoi:10.1016/j.inffus.2017.02.003 (https://doi.org/10.1016%2Fj.inffus.2017.02.003).\nhdl:1893/25490 (https://hdl.handle.net/1893%2F25490). S2CID 205433041 (https://api.sem\nanticscholar.org/CorpusID:205433041). Archived (https://web.archive.org/web/20230323165\n407/https://www.napier.ac.uk/research-and-innovation/research-search/outputs/a-review-of-\naffective-computing-from-unimodal-analysis-to-multimodal-fusion) from the original on 23\nMarch 2023. Retrieved 27 April 2021.\nRawlinson, Kevin (29 January 2015). \"Microsoft's Bill Gates insists AI is a threat\" (https://www.b\nbc.co.uk/news/31047780). BBC News. Archived (https://web.archive.org/web/20150129183\n607/http://www.bbc.co.uk/news/31047780) from the original on 29 January 2015. Retrieved\n30 January 2015.\nReisner, Alex (19 August 2023), \"Revealed: The Authors Whose Pirated Books are Powering\nGenerative AI\" (https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-lla\nma-pirated-books/675063/), The Atlantic, archived (https://web.archive.org/web/2024100307\n1505/https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-\nbooks/675063/) from the original on 3 October 2024, retrieved 5 October 2024\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:66>\n",
    "text": "Roberts, Jacob (2016). \"Thinking Machines: The Search for Artificial Intelligence\" (https://web.ar\nchive.org/web/20180819152455/https://www.sciencehistory.org/distillations/magazine/thinkin\ng-machines-the-search-for-artificial-intelligence). Distillations. Vol. 2, no. 2. pp. 14\u201323.\nArchived from the original (https://www.sciencehistory.org/distillations/magazine/thinking-ma\nchines-the-search-for-artificial-intelligence) on 19 August 2018. Retrieved 20 March 2018.\nRobitzski, Dan (5 September 2018). \"Five experts share what scares them the most about AI\"\n(https://futurism.com/artificial-intelligence-experts-fear/amp). Archived (https://web.archive.or\ng/web/20191208094101/https://futurism.com/artificial-intelligence-experts-fear/amp) from\nthe original on 8 December 2019. Retrieved 8 December 2019.\nRose, Steve (11 July 2023). \"AI Utopia or dystopia?\". The Guardian Weekly. pp. 42\u201343.\nRussell, Stuart (2019). Human Compatible: Artificial Intelligence and the Problem of Control.\nUnited States: Viking. ISBN 978-0-5255-5861-3. OCLC 1083694322 (https://search.worldca\nt.org/oclc/1083694322).\nSainato, Michael (19 August 2015). \"Stephen Hawking, Elon Musk, and Bill Gates Warn About\nArtificial Intelligence\" (https://observer.com/2015/08/stephen-hawking-elon-musk-and-bill-gat\nes-warn-about-artificial-intelligence). Observer. Archived (https://web.archive.org/web/20151\n030053323/http://observer.com/2015/08/stephen-hawking-elon-musk-and-bill-gates-warn-ab\nout-artificial-intelligence) from the original on 30 October 2015. Retrieved 30 October 2015.\nSample, Ian (5 November 2017). \"Computer says no: why making AIs fair, accountable and\ntransparent is crucial\" (https://www.theguardian.com/science/2017/nov/05/computer-says-no\n-why-making-ais-fair-accountable-and-transparent-is-crucial). The Guardian. Archived (http\ns://web.archive.org/web/20221010134155/https://theguardian.com/science/2017/nov/05/co\nmputer-says-no-why-making-ais-fair-accountable-and-transparent-is-crucial) from the\noriginal on 10 October 2022. Retrieved 30 January 2018.\nRothman, Denis (7 October 2020). \"Exploring LIME Explanations and the Mathematics Behind\nIt\" (https://www.codemotion.com/magazine/ai-ml/lime-explainable-ai). Codemotion. Archived\n(https://web.archive.org/web/20231125045932/https://www.codemotion.com/magazine/ai-m\nl/lime-explainable-ai/) from the original on 25 November 2023. Retrieved 25 November\n2023.\nScassellati, Brian (2002). \"Theory of mind for a humanoid robot\". Autonomous Robots. 12 (1):\n13\u201324. doi:10.1023/A:1013298507114 (https://doi.org/10.1023%2FA%3A1013298507114).\nS2CID 1979315 (https://api.semanticscholar.org/CorpusID:1979315).\nSchmidhuber, J. (2015). \"Deep Learning in Neural Networks: An Overview\". Neural Networks.\n61: 85\u2013117. arXiv:1404.7828 (https://arxiv.org/abs/1404.7828).\ndoi:10.1016/j.neunet.2014.09.003 (https://doi.org/10.1016%2Fj.neunet.2014.09.003).\nPMID 25462637 (https://pubmed.ncbi.nlm.nih.gov/25462637). S2CID 11715509 (https://api.\nsemanticscholar.org/CorpusID:11715509).\nSchmidhuber, J\u00fcrgen (2022). \"Annotated History of Modern AI and Deep Learning\" (https://peop\nle.idsia.ch/~juergen/). Archived (https://web.archive.org/web/20230807173414/https://peopl\ne.idsia.ch/~juergen/) from the original on 7 August 2023. Retrieved 5 October 2024.\nSearle, John (1980). \"Minds, Brains and Programs\" (http://cogprints.org/7150/1/10.1.1.83.5248.\npdf) (PDF). Behavioral and Brain Sciences. 3 (3): 417\u2013457.\ndoi:10.1017/S0140525X00005756 (https://doi.org/10.1017%2FS0140525X00005756).\nS2CID 55303721 (https://api.semanticscholar.org/CorpusID:55303721). Archived (https://we\nb.archive.org/web/20190317230215/http://cogprints.org/7150/1/10.1.1.83.5248.pdf) (PDF)\nfrom the original on 17 March 2019. Retrieved 22 August 2020.\nSearle, John (1999). Mind, language and society (https://archive.org/details/mindlanguagesoci0\n0sear). New York: Basic Books. ISBN 978-0-4650-4521-1. OCLC 231867665 (https://searc\nh.worldcat.org/oclc/231867665). Archived (https://web.archive.org/web/20200726220615/htt\nps://archive.org/details/mindlanguagesoci00sear) from the original on 26 July 2020.\nRetrieved 22 August 2020.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:67>\n",
    "text": "Simon, H. A. (1965), The Shape of Automation for Men and Management, New York: Harper &\nRow\nSimonite, Tom (31 March 2016). \"How Google Plans to Solve Artificial Intelligence\" (https://www.\ntechnologyreview.com/2016/03/31/161234/how-google-plans-to-solve-artificial-intelligence).\nMIT Technology Review. Archived (https://web.archive.org/web/20240916003430/https://ww\nw.technologyreview.com/2016/03/31/161234/how-google-plans-to-solve-artificial-intelligenc\ne/) from the original on 16 September 2024. Retrieved 5 October 2024.\nSmith, Craig S. (15 March 2023). \"ChatGPT-4 Creator Ilya Sutskever on AI Hallucinations and\nAI Democracy\" (https://www.forbes.com/sites/craigsmith/2023/03/15/gpt-4-creator-ilya-sutsk\never-on-ai-hallucinations-and-ai-democracy). Forbes. Archived (https://web.archive.org/web/\n20240918141325/https://www.forbes.com/sites/craigsmith/2023/03/15/gpt-4-creator-ilya-sut\nskever-on-ai-hallucinations-and-ai-democracy/) from the original on 18 September 2024.\nRetrieved 25 December 2023.\nSmoliar, Stephen W.; Zhang, HongJiang (1994). \"Content based video indexing and retrieval\".\nIEEE MultiMedia. 1 (2): 62\u201372. doi:10.1109/93.311653 (https://doi.org/10.1109%2F93.3116\n53). S2CID 32710913 (https://api.semanticscholar.org/CorpusID:32710913).\nSolomonoff, Ray (1956). An Inductive Inference Machine (http://world.std.com/~rjs/indinf56.pdf)\n(PDF). Dartmouth Summer Research Conference on Artificial Intelligence. Archived (https://\nweb.archive.org/web/20110426161749/http://world.std.com/~rjs/indinf56.pdf) (PDF) from the\noriginal on 26 April 2011. Retrieved 22 March 2011 \u2013 via std.com, pdf scanned copy of the\noriginal. Later published as\nSolomonoff, Ray (1957). \"An Inductive Inference Machine\". IRE Convention Record.\nVol. Section on Information Theory, part 2. pp. 56\u201362.\nStanford University (2023). \"Artificial Intelligence Index Report 2023/Chapter 6: Policy and\nGovernance\" (https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report\n-2023_CHAPTER_6-1.pdf) (PDF). AI Index. Archived (https://web.archive.org/web/2023061\n9013609/https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report-202\n3_CHAPTER_6-1.pdf) (PDF) from the original on 19 June 2023. Retrieved 19 June 2023.\nTao, Jianhua; Tan, Tieniu (2005). Affective Computing and Intelligent Interaction. Affective\nComputing: A Review. Lecture Notes in Computer Science. Vol. 3784. Springer. pp. 981\u2013\n995. doi:10.1007/11573548 (https://doi.org/10.1007%2F11573548). ISBN 978-3-5402-9621-\n8.\nTaylor, Josh; Hern, Alex (2 May 2023). \"'Godfather of AI' Geoffrey Hinton quits Google and\nwarns over dangers of misinformation\" (https://www.theguardian.com/technology/2023/may/\n02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of-machine-learning). The\nGuardian. Archived (https://web.archive.org/web/20241005171343/https://www.theguardian.\ncom/technology/2023/may/02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of\n-machine-learning) from the original on 5 October 2024. Retrieved 5 October 2024.\nThompson, Derek (23 January 2014). \"What Jobs Will the Robots Take?\" (https://www.theatlanti\nc.com/business/archive/2014/01/what-jobs-will-the-robots-take/283239). The Atlantic.\nArchived (https://web.archive.org/web/20180424202435/https://www.theatlantic.com/busine\nss/archive/2014/01/what-jobs-will-the-robots-take/283239) from the original on 24 April\n2018. Retrieved 24 April 2018.\nThro, Ellen (1993). Robotics: The Marriage of Computers and Machines (https://archive.org/det\nails/isbn_9780816026289). New York: Facts on File. ISBN 978-0-8160-2628-9. Archived (htt\nps://web.archive.org/web/20200726131505/https://archive.org/details/isbn_9780816026289)\nfrom the original on 26 July 2020. Retrieved 22 August 2020.\nToews, Rob (3 September 2023). \"Transformers Revolutionized AI. What Will Replace Them?\"\n(https://www.forbes.com/sites/robtoews/2023/09/03/transformers-revolutionized-ai-what-will-\nreplace-them). Forbes. Archived (https://web.archive.org/web/20231208232145/https://www.\nforbes.com/sites/robtoews/2023/09/03/transformers-revolutionized-ai-what-will-replace-the\nm/) from the original on 8 December 2023. Retrieved 8 December 2023.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:68>\n",
    "text": "Turing, Alan (October 1950). \"Computing Machinery and Intelligence\" (https://academic.oup.co\nm/mind/article/LIX/236/433/986238). Mind. 59 (236): 433\u2013460.\ndoi:10.1093/mind/LIX.236.433 (https://doi.org/10.1093%2Fmind%2FLIX.236.433).\nISSN 1460-2113 (https://search.worldcat.org/issn/1460-2113). JSTOR 2251299 (https://ww\nw.jstor.org/stable/2251299). S2CID 14636783 (https://api.semanticscholar.org/CorpusID:146\n36783).\nUNESCO Science Report: the Race Against Time for Smarter Development (https://unesdoc.un\nesco.org/ark:/48223/pf0000377433/PDF/377433eng.pdf.multi). Paris: UNESCO. 2021.\nISBN 978-9-2310-0450-6. Archived (https://web.archive.org/web/20220618233752/https://un\nesdoc.unesco.org/ark:/48223/pf0000377433/PDF/377433eng.pdf.multi) from the original on\n18 June 2022. Retrieved 18 September 2021.\nUrbina, Fabio; Lentzos, Filippa; Invernizzi, C\u00e9dric; Ekins, Sean (7 March 2022). \"Dual use of\nartificial-intelligence-powered drug discovery\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PM\nC9544280). Nature Machine Intelligence. 4 (3): 189\u2013191. doi:10.1038/s42256-022-00465-9\n(https://doi.org/10.1038%2Fs42256-022-00465-9). PMC 9544280 (https://www.ncbi.nlm.nih.\ngov/pmc/articles/PMC9544280). PMID 36211133 (https://pubmed.ncbi.nlm.nih.gov/3621113\n3). S2CID 247302391 (https://api.semanticscholar.org/CorpusID:247302391).\nValance, Christ (30 May 2023). \"Artificial intelligence could lead to extinction, experts warn\" (htt\nps://www.bbc.com/news/uk-65746524). BBC News. Archived (https://web.archive.org/web/2\n0230617200355/https://www.bbc.com/news/uk-65746524) from the original on 17 June\n2023. Retrieved 18 June 2023.\nValinsky, Jordan (11 April 2019), \"Amazon reportedly employs thousands of people to listen to\nyour Alexa conversations\" (https://www.cnn.com/2019/04/11/tech/amazon-alexa-listening/in\ndex.html), CNN.com, archived (https://web.archive.org/web/20240126033535/https://www.c\nnn.com/2019/04/11/tech/amazon-alexa-listening/index.html) from the original on 26 January\n2024, retrieved 5 October 2024\nVerma, Yugesh (25 December 2021). \"A Complete Guide to SHAP \u2013 SHAPley Additive\nexPlanations for Practitioners\" (https://analyticsindiamag.com/a-complete-guide-to-shap-sha\npley-additive-explanations-for-practitioners). Analytics India Magazine. Archived (https://we\nb.archive.org/web/20231125045938/https://analyticsindiamag.com/a-complete-guide-to-sha\np-shapley-additive-explanations-for-practitioners/) from the original on 25 November 2023.\nRetrieved 25 November 2023.\nVincent, James (7 November 2019). \"OpenAI has published the text-generating AI it said was\ntoo dangerous to share\" (https://www.theverge.com/2019/11/7/20953040/openai-text-genera\ntion-ai-gpt-2-full-model-release-1-5b-parameters). The Verge. Archived (https://web.archive.\norg/web/20200611054114/https://www.theverge.com/2019/11/7/20953040/openai-text-gene\nration-ai-gpt-2-full-model-release-1-5b-parameters) from the original on 11 June 2020.\nRetrieved 11 June 2020.\nVincent, James (15 November 2022). \"The scary truth about AI copyright is nobody knows what\nwill happen next\" (https://www.theverge.com/23444685/generative-ai-copyright-infringement\n-legal-fair-use-training-data). The Verge. Archived (https://web.archive.org/web/2023061905\n5201/https://www.theverge.com/23444685/generative-ai-copyright-infringement-legal-fair-us\ne-training-data) from the original on 19 June 2023. Retrieved 19 June 2023.\nVincent, James (3 April 2023). \"AI is entering an era of corporate control\" (https://www.theverge.\ncom/23667752/ai-progress-2023-report-stanford-corporate-control). The Verge. Archived (ht\ntps://web.archive.org/web/20230619005803/https://www.theverge.com/23667752/ai-progres\ns-2023-report-stanford-corporate-control) from the original on 19 June 2023. Retrieved\n19 June 2023.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:69>\n",
    "text": "Vinge, Vernor (1993). \"The Coming Technological Singularity: How to Survive in the Post-\nHuman Era\" (https://web.archive.org/web/20070101133646/http://www-rohan.sdsu.edu/facul\nty/vinge/misc/singularity.html). Vision 21: Interdisciplinary Science and Engineering in the\nEra of Cyberspace: 11. Bibcode:1993vise.nasa...11V (https://ui.adsabs.harvard.edu/abs/199\n3vise.nasa...11V). Archived from the original (http://www-rohan.sdsu.edu/faculty/vinge/misc/\nsingularity.html) on 1 January 2007. Retrieved 14 November 2011.\nWaddell, Kaveh (2018). \"Chatbots Have Entered the Uncanny Valley\" (https://www.theatlantic.c\nom/technology/archive/2017/04/uncanny-valley-digital-assistants/523806). The Atlantic.\nArchived (https://web.archive.org/web/20180424202350/https://www.theatlantic.com/technol\nogy/archive/2017/04/uncanny-valley-digital-assistants/523806) from the original on 24 April\n2018. Retrieved 24 April 2018.\nWallach, Wendell (2010). Moral Machines. Oxford University Press.\nWason, P. C.; Shapiro, D. (1966). \"Reasoning\" (https://archive.org/details/newhorizonsinpsy000\n0foss). In Foss, B. M. (ed.). New horizons in psychology. Harmondsworth: Penguin.\nArchived (https://web.archive.org/web/20200726131518/https://archive.org/details/newhoriz\nonsinpsy0000foss) from the original on 26 July 2020. Retrieved 18 November 2019.\nWeng, J.; McClelland; Pentland, A.; Sporns, O.; Stockman, I.; Sur, M.; Thelen, E. (2001).\n\"Autonomous mental development by robots and animals\" (http://www.cse.msu.edu/dl/Scien\ncePaper.pdf) (PDF). Science. 291 (5504): 599\u2013600. doi:10.1126/science.291.5504.599 (http\ns://doi.org/10.1126%2Fscience.291.5504.599). PMID 11229402 (https://pubmed.ncbi.nlm.ni\nh.gov/11229402). S2CID 54131797 (https://api.semanticscholar.org/CorpusID:54131797).\nArchived (https://web.archive.org/web/20130904235242/http://www.cse.msu.edu/dl/Science\nPaper.pdf) (PDF) from the original on 4 September 2013. Retrieved 4 June 2013 \u2013 via\nmsu.edu.\n\"What is 'fuzzy logic'? Are there computers that are inherently fuzzy and do not apply the usual\nbinary logic?\" (https://www.scientificamerican.com/article/what-is-fuzzy-logic-are-t). Scientific\nAmerican. 21 October 1999. Archived (https://web.archive.org/web/20180506035133/https://\nwww.scientificamerican.com/article/what-is-fuzzy-logic-are-t) from the original on 6 May\n2018. Retrieved 5 May 2018.\nWilliams, Rhiannon (28 June 2023), \"Humans may be more likely to believe disinformation\ngenerated by AI\" (https://www.technologyreview.com/2023/06/28/1075683/humans-may-be-\nmore-likely-to-believe-disinformation-generated-by-ai/), MIT Technology Review, archived (h\nttps://web.archive.org/web/20240916014613/https://www.technologyreview.com/2023/06/28/\n1075683/humans-may-be-more-likely-to-believe-disinformation-generated-by-ai/) from the\noriginal on 16 September 2024, retrieved 5 October 2024\nWirtz, Bernd W.; Weyerer, Jan C.; Geyer, Carolin (24 July 2018). \"Artificial Intelligence and the\nPublic Sector \u2013 Applications and Challenges\" (https://zenodo.org/record/3569435).\nInternational Journal of Public Administration. 42 (7): 596\u2013615.\ndoi:10.1080/01900692.2018.1498103 (https://doi.org/10.1080%2F01900692.2018.149810\n3). ISSN 0190-0692 (https://search.worldcat.org/issn/0190-0692). S2CID 158829602 (http\ns://api.semanticscholar.org/CorpusID:158829602). Archived (https://web.archive.org/web/20\n200818131415/https://zenodo.org/record/3569435) from the original on 18 August 2020.\nRetrieved 22 August 2020.\nWong, Matteo (19 May 2023), \"ChatGPT Is Already Obsolete\" (https://www.theatlantic.com/tech\nnology/archive/2023/05/ai-advancements-multimodal-models/674113/), The Atlantic,\narchived (https://web.archive.org/web/20240918022529/https://www.theatlantic.com/technol\nogy/archive/2023/05/ai-advancements-multimodal-models/674113/) from the original on 18\nSeptember 2024, retrieved 5 October 2024\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:70>\n",
    "text": "Yudkowsky, E (2008), \"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" (h\nttp://intelligence.org/files/AIPosNegFactor.pdf) (PDF), Global Catastrophic Risks, Oxford\nUniversity Press, 2008, Bibcode:2008gcr..book..303Y (https://ui.adsabs.harvard.edu/abs/20\n08gcr..book..303Y), archived (https://web.archive.org/web/20131019182403/http://intelligenc\ne.org/files/AIPosNegFactor.pdf) (PDF) from the original on 19 October 2013, retrieved\n24 September 2021\nFurther reading\nAutor, David H., \"Why Are There Still So Many Jobs? The History and Future of Workplace\nAutomation\" (2015) 29(3) Journal of Economic Perspectives 3.\nBerlinski, David (2000). The Advent of the Algorithm (https://archive.org/details/adventofalgorith\n0000berl). Harcourt Books. ISBN 978-0-1560-1391-8. OCLC 46890682 (https://search.world\ncat.org/oclc/46890682). Archived (https://web.archive.org/web/20200726215744/https://arch\nive.org/details/adventofalgorith0000berl) from the original on 26 July 2020. Retrieved\n22 August 2020.\nBoyle, James, The Line: AI and the Future of Personhood (https://direct.mit.edu/books/book/585\n9/The-LineAI-and-the-Future-of-Personhood), MIT Press, 2024.\nCukier, Kenneth, \"Ready for Robots? How to Think about the Future of AI\", Foreign Affairs, vol.\n98, no. 4 (July/August 2019), pp. 192\u2013198. George Dyson, historian of computing, writes (in\nwhat might be called \"Dyson's Law\") that \"Any system simple enough to be understandable\nwill not be complicated enough to behave intelligently, while any system complicated\nenough to behave intelligently will be too complicated to understand.\" (p. 197.) Computer\nscientist Alex Pentland writes: \"Current AI machine-learning algorithms are, at their core,\ndead simple stupid. They work, but they work by brute force.\" (p. 198.)\nEvans, Woody (2015). \"Posthuman Rights: Dimensions of Transhuman Worlds\" (https://doi.org/\n10.5209%2Frev_TK.2015.v12.n2.49072). Teknokultura. 12 (2).\ndoi:10.5209/rev_TK.2015.v12.n2.49072 (https://doi.org/10.5209%2Frev_TK.2015.v12.n2.49\n072). S2CID 147612763 (https://api.semanticscholar.org/CorpusID:147612763).\nFrank, Michael (22 September 2023). \"US Leadership in Artificial Intelligence Can Shape the\n21st Century Global Order\" (https://thediplomat.com/2023/09/us-leadership-in-artificial-intelli\ngence-can-shape-the-21st-century-global-order). The Diplomat. Archived (https://web.archiv\ne.org/web/20240916014433/https://thediplomat.com/2023/09/us-leadership-in-artificial-intelli\ngence-can-shape-the-21st-century-global-order/) from the original on 16 September 2024.\nRetrieved 8 December 2023. \"Instead, the United States has developed a new area of\ndominance that the rest of the world views with a mixture of awe, envy, and resentment:\nartificial intelligence... From AI models and research to cloud computing and venture capital,\nU.S. companies, universities, and research labs \u2013 and their affiliates in allied countries \u2013\nappear to have an enormous lead in both developing cutting-edge AI and commercializing it.\nThe value of U.S. venture capital investments in AI start-ups exceeds that of the rest of the\nworld combined.\"\nGertner, Jon. (2023) \"Wikipedia's Moment of Truth: Can the online encyclopedia help teach A.I.\nchatbots to get their facts right \u2014 without destroying itself in the process?\" New York Times\nMagazine (July 18, 2023) online (https://www.nytimes.com/2023/07/18/magazine/wikipedia-\nai-chatgpt.html) Archived (https://web.archive.org/web/20230720125400/https://www.nytime\ns.com/2023/07/18/magazine/wikipedia-ai-chatgpt.html) 20 July 2023 at the Wayback\nMachine\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:71>\n",
    "text": "Gleick, James, \"The Fate of Free Will\" (review of Kevin J. Mitchell, Free Agents: How Evolution\nGave Us Free Will, Princeton University Press, 2023, 333 pp.), The New York Review of\nBooks, vol. LXXI, no. 1 (18 January 2024), pp. 27\u201328, 30. \"Agency is what distinguishes us\nfrom machines. For biological creatures, reason and purpose come from acting in the world\nand experiencing the consequences. Artificial intelligences \u2013 disembodied, strangers to\nblood, sweat, and tears \u2013 have no occasion for that.\" (p. 30.)\nHalpern, Sue, \"The Coming Tech Autocracy\" (review of Verity Harding, AI Needs You: How We\nCan Change AI's Future and Save Our Own, Princeton University Press, 274 pp.; Gary\nMarcus, Taming Silicon Valley: How We Can Ensure That AI Works for Us, MIT Press, 235\npp.; Daniela Rus and Gregory Mone, The Mind's Mirror: Risk and Reward in the Age of AI,\nNorton, 280 pp.; Madhumita Murgia, Code Dependent: Living in the Shadow of AI, Henry\nHolt, 311 pp.), The New York Review of Books, vol. LXXI, no. 17 (7 November 2024),\npp. 44\u201346. \"'We can't realistically expect that those who hope to get rich from AI are going to\nhave the interests of the rest of us close at heart,' ... writes [Gary Marcus]. 'We can't count\non governments driven by campaign finance contributions [from tech companies] to push\nback.'... Marcus details the demands that citizens should make of their governments and the\ntech companies. They include transparency on how AI systems work; compensation for\nindividuals if their data [are] used to train LLMs (large language model)s and the right to\nconsent to this use; and the ability to hold tech companies liable for the harms they cause\nby eliminating Section 230, imposing cash penalties, and passing stricter product liability\nlaws... Marcus also suggests... that a new, AI-specific federal agency, akin to the FDA, the\nFCC, or the FTC, might provide the most robust oversight.... [T]he Fordham law professor\nChinmayi Sharma... suggests... establish[ing] a professional licensing regime for engineers\nthat would function in a similar way to medical licenses, malpractice suits, and the\nHippocratic oath in medicine. 'What if, like doctors,' she asks..., 'AI engineers also vowed to\ndo no harm?'\" (p. 46.)\nHenderson, Mark (24 April 2007). \"Human rights for robots? We're getting carried away\" (https://\nwww.thetimes.com/uk/science/article/human-rights-for-robots-were-getting-carried-away-xfb\ndkpgwn0v). The Times Online. London. Archived (https://web.archive.org/web/20140531104\n850/http://www.thetimes.co.uk/tto/technology/article1966391.ece) from the original on 31\nMay 2014. Retrieved 31 May 2014.\nHughes-Castleberry, Kenna, \"A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone,\nwhich has stumped humans for decades, reveals the limitations of natural-language-\nprocessing algorithms\", Scientific American, vol. 329, no. 4 (November 2023), pp. 81\u201382.\n\"This murder mystery competition has revealed that although NLP (natural-language\nprocessing) models are capable of incredible feats, their abilities are very much limited by\nthe amount of context they receive. This [...] could cause [difficulties] for researchers who\nhope to use them to do things such as analyze ancient languages. In some cases, there are\nfew historical records on long-gone civilizations to serve as training data for such a\npurpose.\" (p. 82.)\nImmerwahr, Daniel, \"Your Lying Eyes: People now use A.I. to generate fake videos\nindistinguishable from real ones. How much does it matter?\", The New Yorker, 20\nNovember 2023, pp. 54\u201359. \"If by 'deepfakes' we mean realistic videos produced using\nartificial intelligence that actually deceive people, then they barely exist. The fakes aren't\ndeep, and the deeps aren't fake. [...] A.I.-generated videos are not, in general, operating in\nour media as counterfeited evidence. Their role better resembles that of cartoons, especially\nsmutty ones.\" (p. 59.)\nJohnston, John (2008) The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI,\nMIT Press.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:72>\n",
    "text": "Jumper, John; Evans, Richard; Pritzel, Alexander; et al. (26 August 2021). \"Highly accurate\nprotein structure prediction with AlphaFold\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC\n8371605). Nature. 596 (7873): 583\u2013589. Bibcode:2021Natur.596..583J (https://ui.adsabs.ha\nrvard.edu/abs/2021Natur.596..583J). doi:10.1038/s41586-021-03819-2 (https://doi.org/10.10\n38%2Fs41586-021-03819-2). PMC 8371605 (https://www.ncbi.nlm.nih.gov/pmc/articles/PM\nC8371605). PMID 34265844 (https://pubmed.ncbi.nlm.nih.gov/34265844).\nS2CID 235959867 (https://api.semanticscholar.org/CorpusID:235959867).\nLeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (28 May 2015). \"Deep learning\" (https://www.na\nture.com/articles/nature14539). Nature. 521 (7553): 436\u2013444. Bibcode:2015Natur.521..436L\n(https://ui.adsabs.harvard.edu/abs/2015Natur.521..436L). doi:10.1038/nature14539 (https://\ndoi.org/10.1038%2Fnature14539). PMID 26017442 (https://pubmed.ncbi.nlm.nih.gov/26017\n442). S2CID 3074096 (https://api.semanticscholar.org/CorpusID:3074096). Archived (https://\nweb.archive.org/web/20230605235832/https://www.nature.com/articles/nature14539) from\nthe original on 5 June 2023. Retrieved 19 June 2023.\nLeffer, Lauren, \"The Risks of Trusting AI: We must avoid humanizing machine-learning models\nused in scientific research\", Scientific American, vol. 330, no. 6 (June 2024), pp. 80\u201381.\nLepore, Jill, \"The Chit-Chatbot: Is talking with a machine a conversation?\", The New Yorker, 7\nOctober 2024, pp. 12\u201316.\nMaschafilm (2010). \"Content: Plug & Pray Film \u2013 Artificial Intelligence \u2013 Robots\" (http://www.plu\ngandpray-film.de/en/content.html). plugandpray-film.de. Archived (https://web.archive.org/w\neb/20160212040134/http://www.plugandpray-film.de/en/content.html) from the original on 12\nFebruary 2016.\nMarcus, Gary, \"Artificial Confidence: Even the newest, buzziest systems of artificial general\nintelligence are stymmied by the same old problems\", Scientific American, vol. 327, no. 4\n(October 2022), pp. 42\u201345.\nMitchell, Melanie (2019). Artificial intelligence: a guide for thinking humans. New York: Farrar,\nStraus and Giroux. ISBN 978-0-3742-5783-5.\nMnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; et al. (26 February 2015). \"Human-level\ncontrol through deep reinforcement learning\" (https://www.nature.com/articles/nature14236).\nNature. 518 (7540): 529\u2013533. Bibcode:2015Natur.518..529M (https://ui.adsabs.harvard.edu/\nabs/2015Natur.518..529M). doi:10.1038/nature14236 (https://doi.org/10.1038%2Fnature142\n36). PMID 25719670 (https://pubmed.ncbi.nlm.nih.gov/25719670). S2CID 205242740 (http\ns://api.semanticscholar.org/CorpusID:205242740). Archived (https://web.archive.org/web/20\n230619055525/https://www.nature.com/articles/nature14236) from the original on 19 June\n2023. Retrieved 19 June 2023. Introduced DQN, which produced human-level performance\non some Atari games.\nPress, Eyal, \"In Front of Their Faces: Does facial-recognition technology lead police to ignore\ncontradictory evidence?\", The New Yorker, 20 November 2023, pp. 20\u201326.\n\"Robots could demand legal rights\" (http://news.bbc.co.uk/2/hi/technology/6200005.stm). BBC\nNews. 21 December 2006. Archived (https://web.archive.org/web/20191015042628/http://ne\nws.bbc.co.uk/2/hi/technology/6200005.stm) from the original on 15 October 2019. Retrieved\n3 February 2011.\nRoivainen, Eka, \"AI's IQ: ChatGPT aced a [standard intelligence] test but showed that\nintelligence cannot be measured by IQ alone\", Scientific American, vol. 329, no. 1\n(July/August 2023), p. 7. \"Despite its high IQ, ChatGPT fails at tasks that require real\nhumanlike reasoning or an understanding of the physical and social world.... ChatGPT\nseemed unable to reason logically and tried to rely on its vast database of... facts derived\nfrom online texts.\"\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:73>\n",
    "text": "Scharre, Paul, \"Killer Apps: The Real Dangers of an AI Arms Race\", Foreign Affairs, vol. 98, no.\n3 (May/June 2019), pp. 135\u2013144. \"Today's AI technologies are powerful but unreliable.\nRules-based systems cannot deal with circumstances their programmers did not anticipate.\nLearning systems are limited by the data on which they were trained. AI failures have\nalready led to tragedy. Advanced autopilot features in cars, although they perform well in\nsome circumstances, have driven cars without warning into trucks, concrete barriers, and\nparked cars. In the wrong situation, AI systems go from supersmart to superdumb in an\ninstant. When an enemy is trying to manipulate and hack an AI system, the risks are even\ngreater.\" (p. 140.)\nSchulz, Hannes; Behnke, Sven (1 November 2012). \"Deep Learning\" (https://www.researchgat\ne.net/publication/230690795). KI \u2013 K\u00fcnstliche Intelligenz. 26 (4): 357\u2013363.\ndoi:10.1007/s13218-012-0198-z (https://doi.org/10.1007%2Fs13218-012-0198-z).\nISSN 1610-1987 (https://search.worldcat.org/issn/1610-1987). S2CID 220523562 (https://ap\ni.semanticscholar.org/CorpusID:220523562).\nSerenko, Alexander; Michael Dohan (2011). \"Comparing the expert survey and citation impact\njournal ranking methods: Example from the field of Artificial Intelligence\" (http://www.aserenk\no.com/papers/JOI_AI_Journal_Ranking_Serenko.pdf) (PDF). Journal of Informetrics. 5 (4):\n629\u2013649. doi:10.1016/j.joi.2011.06.002 (https://doi.org/10.1016%2Fj.joi.2011.06.002).\nArchived (https://web.archive.org/web/20131004212839/http://www.aserenko.com/papers/J\nOI_AI_Journal_Ranking_Serenko.pdf) (PDF) from the original on 4 October 2013. Retrieved\n12 September 2013.\nSilver, David; Huang, Aja; Maddison, Chris J.; et al. (28 January 2016). \"Mastering the game of\nGo with deep neural networks and tree search\" (https://www.nature.com/articles/nature1696\n1). Nature. 529 (7587): 484\u2013489. Bibcode:2016Natur.529..484S (https://ui.adsabs.harvard.e\ndu/abs/2016Natur.529..484S). doi:10.1038/nature16961 (https://doi.org/10.1038%2Fnature1\n6961). PMID 26819042 (https://pubmed.ncbi.nlm.nih.gov/26819042). S2CID 515925 (http\ns://api.semanticscholar.org/CorpusID:515925). Archived (https://web.archive.org/web/20230\n618213059/https://www.nature.com/articles/nature16961) from the original on 18 June 2023.\nRetrieved 19 June 2023.\nTarnoff, Ben, \"The Labor Theory of AI\" (review of Matteo Pasquinelli, The Eye of the Master: A\nSocial History of Artificial Intelligence, Verso, 2024, 264 pp.), The New York Review of\nBooks, vol. LXXII, no. 5 (27 March 2025), pp. 30\u201332. The reviewer, Ben Tarnoff, writes: \"The\nstrangeness at the heart of the generative AI boom is that nobody really knows how the\ntechnology works. We know how the large language models within ChatGPT and its\ncounterparts are trained, even if we don't always know which data they're being trained on:\nthey are asked to predict the next string of characters in a sequence. But exactly how they\narrive at any given prediction is a mystery. The computations that occur inside the model are\nsimply too intricate for any human to comprehend.\" (p. 32.)\nVaswani, Ashish, Noam Shazeer, Niki Parmar et al. \"Attention is all you need.\" Advances in\nneural information processing systems 30 (2017). Seminal paper on transformers.\nVincent, James, \"Horny Robot Baby Voice: James Vincent on AI chatbots\", London Review of\nBooks, vol. 46, no. 19 (10 October 2024), pp. 29\u201332. \"[AI chatbot] programs are made\npossible by new technologies but rely on the timelelss human tendency to\nanthropomorphise.\" (p. 29.)\nWhite Paper: On Artificial Intelligence \u2013 A European approach to excellence and trust (https://e\nc.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pd\nf) (PDF). Brussels: European Commission. 2020. Archived (https://web.archive.org/web/202\n00220173419/https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intell\nigence-feb2020_en.pdf) (PDF) from the original on 20 February 2020. Retrieved\n20 February 2020.\n",
    "type": "pdf\n"
  },
  {
    "filename": "Artificial_intelligence.pdf\n",
    "page": "<Page:74>\n",
    "text": "External links\n\"Artificial Intelligence\" (http://www.iep.utm.edu/art-inte). Internet Encyclopedia of Philosophy.\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=1286364868\"\n",
    "type": "pdf\n"
  }
]